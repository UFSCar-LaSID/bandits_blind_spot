<div align="center">
    
Language

English | [Portuguese (BR)](./README_PT-BR.md)
    
</div>

# Incremental Experiments

Incremental experiments involve training and generating recommendations using algorithms that support incremental training (i.e., they don't need to retrain "from scratch" to incorporate new data into their knowledge). For example, LinUCB and LinGreedy are considered incremental.

## Running the Experiments

The `main.py` code should be used to run a set of (or just one) incremental experiment. To execute it, use the command below from the project's root directory:

```
python src/scripts/incremental/main.py
```

Running it this way will prompt for four inputs:

1.  **Incremental Algorithm** (e.g., LinUCB, LinGreedy, LinTemporal)
2.  **Dataset**
3.  **Embedding Algorithm** (e.g., ALS and BPR)
4.  **Context Generation Function** (e.g., user embeddings, item mean, item concat, etc.)

Another way to select options is by running the command below:

```
python src/scripts/incremental/main.py --algorithms <algorithms> --datasets <datasets> --embeddings <embeddings> --contexts <contexts>
```

Replace `<algorithms>` with the names (or indices) of the incremental algorithms, separated by commas (","). The available algorithms to run are:

* \[1\]: Lin
* \[2\]: LinUCB
* \[3\]: LinGreedy
* \[4\]: LinTS
* all (will use all algorithms)

Replace `<datasets>` with the names (or indices) of the datasets, separated by commas (","). The datasets available for training/testing are:

* \[1\]: amazon-beauty
* \[2\]: amazon-books
* \[3\]: amazon-games
* \[4\]: bestbuy
* \[5\]: delicious2k
* \[6\]: delicious2k-urlPrincipal
* \[7\]: ml-100k
* \[8\]: ml-25m
* \[9\]: retailrocket
* all (will use all datasets)

Replace `<embeddings>` with the names (or indices) of the non-incremental algorithms, separated by commas (","). The embeddings generated by these algorithms will be used as part of the MAB context. Therefore, you need to generate the embeddings beforehand. The available embeddings are:

* \[1\]: als
* \[2\]: bpr
* all (will use all embeddings)

Replace `<contexts>` with the names (or indices) of the context generation strategies, separated by commas (","). The available strategies are:

* \[1\]: user
* \[2\]: item\_concat
* \[3\]: item\_mean
* \[4\]: item\_concat+user
* \[5\]: item\_mean+user
* \[6\]: item\_concat+item\_mean
* \[7\]: item\_concat+item\_mean+user
* all (will use all strategies)

If more than one option is selected for an item, all possible combinations will be executed.

## What Happens in Each Experiment?

Before the experiment even begins, the dataset is loaded, and all preprocessing and splits are performed. For more information on this step, [consult the documentation regarding databases](/src/scripts/preprocess/README.md).

With the dataset loaded, all hyperparameter combinations are evaluated. In this case, all possible combinations between the incremental algorithm's hyperparameters and the embeddings' hyperparameters will be tested. For this evaluation, the algorithm is trained on the **TRAINING** partition and then evaluated on the **VALIDATION** partition, calculating a metric (like NDCG or HR). For each hyperparameter combination, the recommendations on the **VALIDATION** partition are saved.

With the best hyperparameter combination (which achieved the best NDCG), the algorithm is trained and tested incrementally (similar to what's done in the **upper bound** of non-incremental algorithms). First, it's trained with the full training data, and recommendations for the first test window are made. Then, it's trained using the full training data + the first test window, and recommendations for the second test window are made. This process continues until recommendations are made for all windows. Once this is done, recommendations for all test windows will be saved.

This evaluation is exemplified below:

![incremental-protocol-example](/images/incremental_eval_eng.png)

At the end of the experiment, several results will be saved, which will be explained in the next section.

### Data Saved by an Experiment

The saved data follows this pattern:

```
<save_path>
├── metadata.json
├── logs.txt
├── final_recommendations.csv
└── <hiperparam_combination_id>
    └── recommendations.csv
```

Each file/directory is described below:

#### <save_path>

This is the folder where all experiment information will be saved.

It follows the pattern:

`<experiments-dir>/<dataset_name>/incremental/<incremental_algo_name>/<embeddings_algo_name>/<build_context_name>/<current_timestamp>`

#### metadata.json

```json
{
    "grid_search": {
        "<hiperparam_combination_id>": {          // Defined below. This is an integer that uniquely identifies each hyperparameter combination.
            "params": {                           // Object containing the tested hyperparameters
                "incremental_params": {           // Specific hyperparameters tested for the incremental algorithm
                    "<param_name>": "<param_value>"
                },
                "embeddings_params": {            // Specific hyperparameters tested for the embeddings
                    "<param_name>": "<param_value>"
                },
                "contexts_params": {              // Specific hyperparameters tested for context creation functions
                    "<param_name>": "<param_value>"
                }
            },
            "score": "<value>"                    // Score obtained (e.g., NDCG, HR) using these parameters on the validation set.
        }
    },
    "best_hiperparam_combination_id": "ID",       // ID of the hyperparameter combination that achieved the best score.
    "configs": {
        "<constant>": "<value>"                   // Constant values used, such as top-N size, split sizes, etc.
    }
}
```

#### logs.txt

Logs/prints saved by the [`Logger`](/src/scripts/utils/Logger.py) during an experiment's execution.

The information here can vary greatly from experiment to experiment, potentially including time taken for certain code steps, calculated metrics, etc.

#### final_recommendations.csv

This is a `.csv` file that will store the recommendations made for ALL test partitions by an incremental algorithm (with the **BEST hyperparameter combination**). It will store a list of recommendations and scores for **EACH INTERACTION** (even negative interactions).

This table has the following columns:

1.  **COLUMN_USER_ID**: ID of the user who made the interaction. The ID should be the original ID contained in the database.
2.  **COLUMN_ITEM_ID**: ID of the item that was consumed. The ID should be the original ID contained in the database.
3.  **COLUMN_RATING**: Rating / score stored in the database for this interaction.
4.  **COLUMN_WINDOW_NUMBER**: Number of the test partition this interaction is part of (from 0 to number of windows - 1).
5.  **COLUMN_DATETIME**: Time at which the interaction occurred, provided by the database itself.
6.  **COLUMN_RECOMMENDATIONS**: A top-N list containing the IDs of recommended items. It should be in the format of a Python list (e.g., `[1, 2, 3]`).
7.  **COLUMN_SCORES**: A list of the same size as the recommendations list, mapping each recommended item to a score (generated by the learning model). It should be in the format of a Python list (e.g., `[0.5, 0.4, 0.3]`).

#### <hiperparam_combination_id>

A value uniquely identifying a hyperparameter combination. The IDs chosen to represent the combinations are sequential positive integers (0, 1, 2, etc.).

#### recommendations.csv

This is a `.csv` file that will store recommendations made on the validation partition by an incremental algorithm (for a specific hyperparameter combination). It will store a list of recommendations and scores for **EACH INTERACTION** (even negative interactions).

This table has the following columns:

1.  **COLUMN_USER_ID**: ID of the user who made the interaction. The ID should be the original ID contained in the database.
2.  **COLUMN_ITEM_ID**: ID of the item that was consumed. The ID should be the original ID contained in the database.
3.  **COLUMN_RATING**: Rating / score stored in the database for this interaction.
4.  **COLUMN_DATETIME**: Time at which the interaction occurred, provided by the database itself.
5.  **COLUMN_RECOMMENDATIONS**: A top-N list containing the IDs of recommended items. It should be in the format of a Python list (e.g., `[1, 2, 3]`).
6.  **COLUMN_SCORES**: A list of the same size as the recommendations list, mapping each recommended item to a score (generated by the learning model). It should be in the format of a Python list (e.g., `[0.5, 0.4, 0.3]`).

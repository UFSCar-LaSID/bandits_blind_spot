{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a931d9cf",
   "metadata": {},
   "source": [
    "## Código original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a129603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "parent_path = '../..'\n",
    "sys.path.append(parent_path)\n",
    "\n",
    "import src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7331167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/fidelity/mabwiser/blob/master/mabwiser/linear.py\n",
    "# The difference is that the original code accept different types of arms (strings, integers, etc) and the modified code only accept sequential integers as arms. With this modification, the code can be optimized.\n",
    "\n",
    "from mabwiser.linear import _Linear\n",
    "from mabwiser.utils import Num, _BaseRNG\n",
    "from typing import List, Optional\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class LinearArmEncoded(_Linear):\n",
    "\n",
    "    def __init__(self, rng: _BaseRNG, num_arms: int, n_jobs: int, backend: Optional[str],\n",
    "                 alpha: Num, epsilon: Num, l2_lambda: Num, regression: str, scale: bool):\n",
    "        super().__init__(rng, np.arange(num_arms).tolist(), n_jobs, backend, alpha, epsilon, l2_lambda, regression, scale)\n",
    "        self.num_arms = num_arms\n",
    "\n",
    "    def _vectorized_predict_context(self, contexts: np.ndarray, is_predict: bool) -> List:\n",
    "\n",
    "        arms = np.arange(self.num_arms)\n",
    "\n",
    "        # Initializing array with expectations for each arm\n",
    "        num_contexts = contexts.shape[0]\n",
    "        arm_expectations = np.empty((num_contexts, self.num_arms), dtype=float)\n",
    "\n",
    "        # With epsilon probability, assign random flag to context\n",
    "        random_values = self.rng.rand(num_contexts)\n",
    "        random_mask = np.array(random_values < self.epsilon)\n",
    "        random_indices = random_mask.nonzero()[0]\n",
    "\n",
    "        # For random indices, generate random expectations\n",
    "        arm_expectations[random_indices] = self.rng.rand((random_indices.shape[0], self.num_arms))\n",
    "\n",
    "        # For non-random indices, get expectations for each arm\n",
    "        nonrandom_indices = np.where(~random_mask)[0]\n",
    "        nonrandom_context = contexts[nonrandom_indices]\n",
    "        start_time = time.time()\n",
    "        arm_expectations[nonrandom_indices] = np.array([self.arm_to_model[arm].predict(nonrandom_context)\n",
    "                                                        for arm in arms]).T\n",
    "        print(f'Gerar as predições demorou {time.time() - start_time} segundos')\n",
    "\n",
    "        return arm_expectations if len(arm_expectations) > 1 else arm_expectations[0]\n",
    "    \n",
    "    def fit(self, decisions: np.ndarray, rewards: np.ndarray, contexts: np.ndarray = None) -> None:\n",
    "\n",
    "        start_time = time.time()\n",
    "        # Initialize each model by arm\n",
    "        self.num_features = contexts.shape[1]\n",
    "        for arm in self.arms:\n",
    "            self.arm_to_model[arm].init(num_features=self.num_features)\n",
    "        print(f'init demorou {time.time() - start_time} segundos')\n",
    "\n",
    "        start_time = time.time()\n",
    "        # Reset warm started arms\n",
    "        # self._reset_arm_to_status()\n",
    "        print(f'reset_arm_to_status demorou {time.time() - start_time} segundos')\n",
    "\n",
    "        start_time = time.time()\n",
    "        # Perform parallel fit\n",
    "        self._parallel_fit(decisions, rewards, contexts)\n",
    "        print(f'parallel_fit demorou {time.time() - start_time} segundos')\n",
    "\n",
    "        # Update trained arms\n",
    "        start_time = time.time()\n",
    "        # Removi o código abaixo pois parece que ele não é usado para o nosso caso...\n",
    "        # Ele parece ser usado apenas no contexto de tentar fazer \"warm\" start\n",
    "        # Basicamente, copiando os mesmos parâmetros de um arm já treinado para um novo (cold) por proximidade de features...\n",
    "\n",
    "        # Otimizar essa função não parece ser algo tão trivial, já que teria que mudar a estrutura do arm_to_status, tendo que mudar vários outros códigos por causa disso\n",
    "        # self._set_arms_as_trained(decisions=decisions, is_partial=False)\n",
    "    \n",
    "    def partial_fit(self, decisions: np.ndarray, rewards: np.ndarray, contexts: np.ndarray = None) -> None:\n",
    "        # Perform parallel fit\n",
    "        self._parallel_fit(decisions, rewards, contexts)\n",
    "\n",
    "        # Update trained arms\n",
    "        # self._set_arms_as_trained(decisions=decisions, is_partial=True)\n",
    "    \n",
    "    def _parallel_fit(self, decisions: np.ndarray, rewards: np.ndarray,\n",
    "                      contexts: Optional[np.ndarray] = None):\n",
    "        \n",
    "        # Compute effective number of jobs\n",
    "        #n_jobs = self._effective_jobs(len(self.arms), self.n_jobs)\n",
    "        # Perform parallel fit\n",
    "        #Parallel(n_jobs=n_jobs, require='sharedmem')(\n",
    "        #                  delayed(self._fit_arm)(\n",
    "        #                      arm, decisions, rewards, contexts)\n",
    "        #                 for arm in self.arms)\n",
    "        \n",
    "        for arm in self.arms:\n",
    "            self._fit_arm(arm, decisions, rewards, contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84e032a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/fidelity/mabwiser/blob/master/mabwiser/mab.py\n",
    "# The difference is that the original code accept different types of arms (strings, integers, etc) and the modified code only accept sequential integers as arms. With this modification, the code can be optimized.\n",
    "\n",
    "from mabwiser.mab import MAB, LearningPolicyType, NeighborhoodPolicyType\n",
    "\n",
    "from mabwiser.utils import Constants, check_true, create_rng\n",
    "from mab2rec import LearningPolicy\n",
    "from mab2rec import NeighborhoodPolicy\n",
    "import numpy as np\n",
    "from mabwiser.neighbors import _KNearest, _Radius\n",
    "from mabwiser.treebandit import _TreeBandit\n",
    "from mabwiser.clusters import _Clusters\n",
    "from mabwiser.approximate import _LSHNearest\n",
    "\n",
    "class MABArmEncoded(MAB):\n",
    "    def __init__(self,\n",
    "                 num_arms: int,  # The list of arms\n",
    "                 learning_policy: LearningPolicyType,  # The learning policy\n",
    "                 neighborhood_policy: NeighborhoodPolicyType = None,  # The context policy, optional\n",
    "                 seed: int = Constants.default_seed,  # The random seed\n",
    "                 n_jobs: int = 1,  # Number of parallel jobs\n",
    "                 backend: str = None,  # Parallel backend implementation\n",
    "                 ):\n",
    "        \"\"\"Initializes a multi-armed bandit (MAB) with the given arguments.\n",
    "\n",
    "        Validates the arguments and raises exception in case there are violations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        arms : List[Union[int, float, str]]\n",
    "            The list of all the arms available for decisions.\n",
    "            Arms can be integers, strings, etc.\n",
    "        learning_policy : LearningPolicyType\n",
    "            The learning policy.\n",
    "        neighborhood_policy : NeighborhoodPolicyType, optional\n",
    "            The context policy. Default value is None.\n",
    "        seed : numbers.Rational, optional\n",
    "            The random seed to initialize the random number generator.\n",
    "            Default value is set to Constants.default_seed.value\n",
    "        n_jobs: int, optional\n",
    "            This is used to specify how many concurrent processes/threads should be used for parallelized routines.\n",
    "            Default value is set to 1.\n",
    "            If set to -1, all CPUs are used.\n",
    "            If set to -2, all CPUs but one are used, and so on.\n",
    "        backend: str, optional\n",
    "            Specify a parallelization backend implementation supported in the joblib library. Supported options are:\n",
    "            - “loky” used by default, can induce some communication and memory overhead when exchanging input and\n",
    "              output data with the worker Python processes.\n",
    "            - “multiprocessing” previous process-based backend based on multiprocessing.Pool. Less robust than loky.\n",
    "            - “threading” is a very low-overhead backend but it suffers from the Python Global Interpreter Lock if the\n",
    "              called function relies a lot on Python objects.\n",
    "            Default value is None. In this case the default backend selected by joblib will be used.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError:  Arms were not provided in a list.\n",
    "        TypeError:  Learning policy type mismatch.\n",
    "        TypeError:  Context policy type mismatch.\n",
    "        TypeError:  Seed is not an integer.\n",
    "        TypeError:  Number of parallel jobs is not an integer.\n",
    "        TypeError:  Parallel backend is not a string.\n",
    "        TypeError:  For EpsilonGreedy, epsilon must be integer or float.\n",
    "        TypeError:  For LinGreedy, epsilon must be an integer or float.\n",
    "        TypeError:  For LinGreedy, l2_lambda must be an integer or float.\n",
    "        TypeError:  For LinTS, alpha must be an integer or float.\n",
    "        TypeError:  For LinTS, l2_lambda must be an integer or float.\n",
    "        TypeError:  For LinUCB, alpha must be an integer or float.\n",
    "        TypeError:  For LinUCB, l2_lambda must be an integer or float.\n",
    "        TypeError:  For Softmax, tau must be an integer or float.\n",
    "        TypeError:  For ThompsonSampling, binarizer must be a callable function.\n",
    "        TypeError:  For UCB, alpha must be an integer or float.\n",
    "        TypeError:  For LSHNearest, n_dimensions must be an integer or float.\n",
    "        TypeError:  For LSHNearest, n_tables must be an integer or float.\n",
    "        TypeError:  For LSHNearest, no_nhood_prob_of_arm must be None or List that sums up to 1.0.\n",
    "        TypeError:  For Clusters, n_clusters must be an integer.\n",
    "        TypeError:  For Clusters, is_minibatch must be a boolean.\n",
    "        TypeError:  For Radius, radius must be an integer or float.\n",
    "        TypeError:  For Radius, no_nhood_prob_of_arm must be None or List that sums up to 1.0.\n",
    "        TypeError:  For KNearest, k must be an integer or float.\n",
    "\n",
    "        ValueError: Invalid number of arms.\n",
    "        ValueError: Invalid values (None, NaN, Inf) in arms.\n",
    "        ValueError: Duplicate values in arms.\n",
    "        ValueError: Number of parallel jobs is 0.\n",
    "        ValueError: For EpsilonGreedy, epsilon must be between 0 and 1.\n",
    "        ValueError: For LinGreedy, epsilon must be between 0 and 1.\n",
    "        ValueError: For LinGreedy, l2_lambda cannot be negative.\n",
    "        ValueError: For LinTS, alpha must be greater than zero.\n",
    "        ValueError: For LinTS, l2_lambda must be greater than zero.\n",
    "        ValueError: For LinUCB, alpha cannot be negative.\n",
    "        ValueError: For LinUCB, l2_lambda cannot be negative.\n",
    "        ValueError: For Softmax, tau must be greater than zero.\n",
    "        ValueError: For UCB, alpha must be greater than zero.\n",
    "        ValueError: For LSHNearest, n_dimensions must be gerater than zero.\n",
    "        ValueError: For LSHNearest, n_tables must be gerater than zero.\n",
    "        ValueError: For LSHNearest, if given, no_nhood_prob_of_arm list should sum up to 1.0.\n",
    "        ValueError: For Clusters, n_clusters cannot be less than 2.\n",
    "        ValueError: For Radius and KNearest, metric is not supported by scipy.spatial.distance.cdist.\n",
    "        ValueError: For Radius, radius must be greater than zero.\n",
    "        ValueError: For Radius, if given, no_nhood_prob_of_arm list should sum up to 1.0.\n",
    "        ValueError: For KNearest, k must be greater than zero.\n",
    "        \"\"\"\n",
    "\n",
    "        # Validate arguments\n",
    "        # MAB._validate_mab_args(arms, learning_policy, neighborhood_policy, seed, n_jobs, backend)\n",
    "\n",
    "        # Save the arguments\n",
    "        self.arms = np.arange(num_arms)\n",
    "        self.num_arms = num_arms\n",
    "        self.seed = seed\n",
    "        self.n_jobs = n_jobs\n",
    "        self.backend = backend\n",
    "\n",
    "        # Create the random number generator\n",
    "        self._rng = create_rng(self.seed)\n",
    "        self._is_initial_fit = False\n",
    "\n",
    "        # Create the learning policy implementor\n",
    "        lp = None\n",
    "        if isinstance(learning_policy, LearningPolicy.LinGreedy):\n",
    "            lp = LinearArmEncoded(self._rng, num_arms, self.n_jobs, self.backend, 0, learning_policy.epsilon,\n",
    "                         learning_policy.l2_lambda, \"ridge\", learning_policy.scale)\n",
    "        elif isinstance(learning_policy, LearningPolicy.LinTS):\n",
    "            lp = LinearArmEncoded(self._rng, num_arms, self.n_jobs, self.backend, learning_policy.alpha, 0,\n",
    "                         learning_policy.l2_lambda, \"ts\", learning_policy.scale)\n",
    "        elif isinstance(learning_policy, LearningPolicy.LinUCB):\n",
    "            lp = LinearArmEncoded(self._rng, num_arms, self.n_jobs, self.backend, learning_policy.alpha, 0,\n",
    "                         learning_policy.l2_lambda, \"ucb\", learning_policy.scale)\n",
    "        else:\n",
    "            check_true(False, ValueError(\"Undefined learning policy \" + str(learning_policy)))\n",
    "\n",
    "        if neighborhood_policy:\n",
    "            # Do not use parallel fit or predict for Learning Policy when contextual\n",
    "            lp.n_jobs = 1\n",
    "\n",
    "            if isinstance(neighborhood_policy, NeighborhoodPolicy.Clusters):\n",
    "                self._imp = _Clusters(self._rng, self.arms, self.n_jobs, self.backend, lp,\n",
    "                                      neighborhood_policy.n_clusters, neighborhood_policy.is_minibatch)\n",
    "            elif isinstance(neighborhood_policy, NeighborhoodPolicy.LSHNearest):\n",
    "                self._imp = _LSHNearest(self._rng, self.arms, self.n_jobs, self.backend, lp,\n",
    "                                        neighborhood_policy.n_dimensions, neighborhood_policy.n_tables,\n",
    "                                        neighborhood_policy.no_nhood_prob_of_arm)\n",
    "            elif isinstance(neighborhood_policy, NeighborhoodPolicy.KNearest):\n",
    "                self._imp = _KNearest(self._rng, self.arms, self.n_jobs, self.backend, lp,\n",
    "                                      neighborhood_policy.k, neighborhood_policy.metric)\n",
    "            elif isinstance(neighborhood_policy, NeighborhoodPolicy.Radius):\n",
    "                self._imp = _Radius(self._rng, self.arms, self.n_jobs, self.backend, lp,\n",
    "                                    neighborhood_policy.radius, neighborhood_policy.metric,\n",
    "                                    neighborhood_policy.no_nhood_prob_of_arm)\n",
    "            elif isinstance(neighborhood_policy, NeighborhoodPolicy.TreeBandit):\n",
    "                self._imp = _TreeBandit(self._rng, self.arms, self.n_jobs, self.backend, lp,\n",
    "                                        neighborhood_policy.tree_parameters)\n",
    "            else:\n",
    "                check_true(False, ValueError(\"Undefined context policy \" + str(neighborhood_policy)))\n",
    "        else:\n",
    "            self._imp = lp\n",
    "        \n",
    "        self.is_contextual = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18974794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/fidelity/mab2rec/blob/main/mab2rec/rec.py\n",
    "# The difference is that the original code accept different types of arms (strings, integers, etc) and the modified code only accept sequential integers as arms. With this modification, the code can be optimized.\n",
    "\n",
    "from typing import List, Tuple, Union\n",
    "from mabwiser.utils import Arm, Num\n",
    "from scipy.special import expit\n",
    "from mab2rec import BanditRecommender\n",
    "from mab2rec import LearningPolicy\n",
    "from mabwiser.utils import Arm\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "filter_matrix_original = None\n",
    "\n",
    "class BanditRecommenderArmEncoded(BanditRecommender):\n",
    "\n",
    "    def __init__(self, learning_policy: Union[LearningPolicy.LinGreedy,\n",
    "                                              LearningPolicy.LinTS,\n",
    "                                              LearningPolicy.LinUCB],\n",
    "                 neighborhood_policy: Union[None] = None,\n",
    "                 top_k: int = 10,\n",
    "                 seed: int = src.RANDOM_STATE,\n",
    "                 n_jobs: int = 1,\n",
    "                 backend: str = None):\n",
    "        \"\"\"Initializes bandit recommender with the given arguments.\n",
    "\n",
    "        Validates the arguments and raises exception in case there are violations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        learning_policy : LearningPolicy\n",
    "            The learning policy.\n",
    "        neighborhood_policy : NeighborhoodPolicy, default=None\n",
    "            The context policy.\n",
    "        top_k : int, default=10\n",
    "            The number of items to recommend.\n",
    "        seed : numbers.Rational, default=Constants.default_seed\n",
    "            The random seed to initialize the random number generator.\n",
    "            Default value is set to Constants.default_seed.value\n",
    "        top_k : int, default=10\n",
    "            The number of items to recommend.\n",
    "        n_jobs : int, default=1\n",
    "            This is used to specify how many concurrent processes/threads should be used for parallelized routines.\n",
    "            If set to -1, all CPUs are used.\n",
    "            If set to -2, all CPUs but one are used, and so on.\n",
    "        backend : str, default=None\n",
    "            Specify a parallelization backend implementation supported in the joblib library. Supported options are:\n",
    "            - “loky” used by default, can induce some communication and memory overhead when exchanging input and\n",
    "              output data with the worker Python processes.\n",
    "            - “multiprocessing” previous process-based backend based on multiprocessing.Pool. Less robust than loky.\n",
    "            - “threading” is a very low-overhead backend but it suffers from the Python Global Interpreter Lock if the\n",
    "              called function relies a lot on Python objects.\n",
    "            Default value is None. In this case the default backend selected by joblib will be used.\n",
    "        \"\"\"\n",
    "        super().__init__(learning_policy, neighborhood_policy, top_k, seed, n_jobs, backend)\n",
    "    \n",
    "    def _init(self, num_arms: int) -> None:\n",
    "        \"\"\"Initializes recommender with given list of arms.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        arms : List[Union[Arm]]\n",
    "            The list of all of the arms available for decisions.\n",
    "            Arms can be integers, strings, etc.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Returns nothing\n",
    "        \"\"\"\n",
    "        self.mab = MABArmEncoded(num_arms, self.learning_policy, self.neighborhood_policy, self.seed, self.n_jobs, self.backend)\n",
    "    \n",
    "    def fit(self, decisions: Union[List[Arm], np.ndarray, pd.Series],\n",
    "            rewards: Union[List[Num], np.ndarray, pd.Series],\n",
    "            contexts: Union[None, List[List[Num]], np.ndarray, pd.Series, pd.DataFrame] = None) -> None:\n",
    "        \"\"\"Fits the recommender the given *decisions*, their corresponding *rewards* and *contexts*, if any.\n",
    "        If the recommender arms has not been initialized using the `set_arms`, the recommender arms will be set\n",
    "        to the list of arms in *decisions*.\n",
    "\n",
    "        Validates arguments and raises exceptions in case there are violations.\n",
    "\n",
    "        This function makes the following assumptions:\n",
    "            - each decision corresponds to an arm of the bandit.\n",
    "            - there are no ``None``, ``Nan``, or ``Infinity`` values in the contexts.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "         decisions : Union[List[Arm], np.ndarray, pd.Series]\n",
    "            The decisions that are made.\n",
    "         rewards : Union[List[Num], np.ndarray, pd.Series]\n",
    "            The rewards that are received corresponding to the decisions.\n",
    "         contexts : Union[None, List[List[Num]], np.ndarray, pd.Series, pd.DataFrame], default=None\n",
    "            The context under which each decision is made.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Returns nothing.\n",
    "        \"\"\"\n",
    "        if self.mab is None:\n",
    "            self._init(np.unique(decisions).shape[0])\n",
    "        self.mab.fit(decisions, rewards, contexts)\n",
    "    \n",
    "    def recommend(self, contexts: Union[None, List[List[Num]], np.ndarray, pd.Series, pd.DataFrame] = None,\n",
    "                  excluded_arms: List[List[Arm]] = None, return_scores: bool = False, apply_sigmoid: bool = True) \\\n",
    "            -> Union[Union[List[Arm], Tuple[List[Arm], List[Num]],\n",
    "                     Union[List[List[Arm]], Tuple[List[List[Arm]], List[List[Num]]]]]]:\n",
    "        self._validate_mab(is_fit=True)\n",
    "        self._validate_get_rec(contexts, excluded_arms)\n",
    "\n",
    "        start_time = time.time()\n",
    "        # Get predicted expectations\n",
    "        num_contexts = len(contexts) if contexts is not None else 1\n",
    "        if num_contexts == 1:\n",
    "            expectations = np.array([self.mab.predict_expectations(contexts)])\n",
    "        else:\n",
    "            expectations = self.mab.predict_expectations(contexts)\n",
    "        \n",
    "        if not isinstance(expectations, np.ndarray):\n",
    "            expectations = np.array(expectations)\n",
    "\n",
    "        print(f'predict_expectations demorou {time.time() - start_time} segundos')\n",
    "\n",
    "        if apply_sigmoid:\n",
    "            expectations = expit(expectations)\n",
    "\n",
    "        # Create an exclusion mask, where exclusion_mask[context_ind][arm_ind] denotes if the arm with the\n",
    "        # index arm_ind was excluded for context with the index context_ind.\n",
    "        # The value will be True if it is excluded and those arms will not be returned as part of the results.\n",
    "        arm_to_index = {arm: arm_ind for arm_ind, arm in enumerate(self.mab.arms)}\n",
    "        exclude_mask = np.zeros((num_contexts, len(self.mab.arms)), dtype=bool)\n",
    "        if excluded_arms is not None:\n",
    "            for context_ind, excluded in enumerate(excluded_arms):\n",
    "                exclude_mask[context_ind][[arm_to_index[arm] for arm in excluded if arm in arm_to_index]] = True\n",
    "        global filter_matrix_original\n",
    "        filter_matrix_original = exclude_mask\n",
    "\n",
    "        # Set excluded item scores to -1, so they automatically get placed lower in best results\n",
    "        expectations[exclude_mask] = -1.\n",
    "\n",
    "        start_time = time.time()\n",
    "        # Get best `top_k` results by sorting the expectations\n",
    "        arm_inds = np.argpartition(-expectations, self.top_k - 1, axis=1)[:, :self.top_k]\n",
    "        arm_inds = arm_inds[np.arange(arm_inds.shape[0]).reshape(-1, 1), np.argsort(-expectations[np.arange(expectations.shape[0]).reshape(-1, 1), arm_inds], axis=1)]\n",
    "        print(f'Ordenação top-K demorou {time.time() - start_time} segundos')\n",
    "        \n",
    "\n",
    "        start_time = time.time()\n",
    "        # Get the list of top_k recommended items and corresponding expectations for each context\n",
    "        recommendations = [[]] * num_contexts\n",
    "        scores = [[]] * num_contexts\n",
    "        for context_ind in range(num_contexts):\n",
    "            recommendations[context_ind] = [self.mab.arms[arm_ind] for arm_ind in arm_inds[context_ind]]\n",
    "            if len(recommendations[context_ind]) != self.top_k:\n",
    "                print('Warning: the number of recommendations is less than the top_k value. ')\n",
    "            if return_scores:\n",
    "                scores[context_ind] = [expectations[context_ind, arm_ind] for arm_ind in arm_inds[context_ind]]\n",
    "        print(f'gerar lista de recomendações demorou {time.time() - start_time} segundos')\n",
    "        # Return recommendations and scores\n",
    "        if return_scores:\n",
    "            if num_contexts > 1:\n",
    "                return recommendations, scores\n",
    "            else:\n",
    "                return recommendations[0], scores[0]\n",
    "        else:\n",
    "            if num_contexts > 1:\n",
    "                return recommendations\n",
    "            else:\n",
    "                return recommendations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3137fdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "from mab2rec import LearningPolicy\n",
    "\n",
    "class Mab2RecRecommender(ABC):\n",
    "    '''\n",
    "    Classe base para os recomendadores da biblioteca [mab2rec](https://github.com/fidelity/mab2rec).\n",
    "\n",
    "    Essa classe é responsável por encapsular a lógica de inicialização e treinamento dos algoritmos da biblioteca mab2rec.\n",
    "\n",
    "    Essa classe não deve ser utilizada diretamente. Para utilizar um algoritmo da biblioteca mab2rec, utilize uma das classes filhas dessa classe.\n",
    "\n",
    "    Para implementar um novo algoritmo da biblioteca mab2rec, basta na classe filha implementar o método __init__ chamando o super().__init__ e inicializar o atributo self.recommender com o algoritmo da biblioteca mab2rec (pelo menos, a ideia inicial é ser simples assim).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, user_column: str=src.COLUMN_USER_ID, item_column: str=src.COLUMN_ITEM_ID, rating_column: str=src.COLUMN_RATING, hyperparameters: dict={}):\n",
    "        '''\n",
    "        Inicializa o recomendador.\n",
    "\n",
    "        params:\n",
    "            user_column: Nome da coluna que representa o usuário.\n",
    "            item_column: Nome da coluna que representa o item.\n",
    "            rating_column: Nome da coluna que representa a avaliação.\n",
    "        '''\n",
    "        self.user_column = user_column\n",
    "        self.item_column = item_column\n",
    "        self.rating_column = rating_column\n",
    "\n",
    "        self.interactions_by_user: pd.DataFrame = None\n",
    "        self.recommender: LearningPolicy = None\n",
    "\n",
    "    def train(self, interactions_df: pd.DataFrame, contexts):\n",
    "        '''\n",
    "        Treina \"do zero\" o recomendador com base nas interações passadas. Utilizar apenas na primeira chamada de treinamento, caso deseje treinar incrementalmente.\n",
    "\n",
    "        params:\n",
    "            interactions_df: DataFrame contendo as interações usuário-item.\n",
    "        '''\n",
    "        self.interactions_by_user = self.__group_interactions_by_user(interactions_df)\n",
    "        self.recommender.fit(\n",
    "            decisions=interactions_df[self.item_column],\n",
    "            rewards=interactions_df[self.rating_column],\n",
    "            contexts=contexts\n",
    "        )\n",
    "\n",
    "    def partial_train(self, interactions_df: pd.DataFrame, contexts):\n",
    "        '''\n",
    "        Treina o recomendador incrementalmente com base nas interações passadas. Deve ser utilizado após a primeira chamada de treinamento, o novo conhecimento será incorporado ao modelo, sem esquecer o conhecimento anterior.\n",
    "\n",
    "        params:\n",
    "            interactions_df: DataFrame contendo as interações usuário-item.\n",
    "        '''\n",
    "        self.interactions_by_user = self.__merge_interactions_by_user(self.interactions_by_user, self.__group_interactions_by_user(interactions_df))\n",
    "        self.recommender.partial_fit(\n",
    "            decisions=interactions_df[self.item_column],\n",
    "            rewards=interactions_df[self.rating_column],\n",
    "            contexts=contexts\n",
    "        )\n",
    "    \n",
    "    def recommend(self, users_ids: 'Union[list[int], np.ndarray]', contexts) -> 'tuple[list[int], list[float]]':\n",
    "        '''\n",
    "        Gera recomendações para uma lista de usuários.\n",
    "\n",
    "        params:\n",
    "            users_ids: Lista de IDs dos usuários para os quais deseja-se gerar recomendações.\n",
    "            topn: Número máximo de recomendações a serem geradas por `user_id`.\n",
    "\n",
    "        returns:\n",
    "            Tupla contendo dois arrays: o primeiro contém os IDs dos itens recomendados e o segundo contém a pontuação de cada item.\n",
    "        '''\n",
    "        filters = pd.DataFrame({self.user_column: users_ids})\\\n",
    "            .merge(self.interactions_by_user, how='left', on=self.user_column)[['interactions']].values.squeeze(axis=1)\n",
    "        # A variável filters mapeia o id de cada user nas interações de teste para uma lista de ids de itens já consumidos por ele.\n",
    "        # Isso é importante para evitar que sejam recomendados itens que o usuário já consumiu\n",
    "        return self.recommender.recommend(contexts, filters, apply_sigmoid=False, return_scores=True)\n",
    "\n",
    "\n",
    "    \n",
    "    def __group_interactions_by_user(self, interactions_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''\n",
    "        Agrupa as interações por usuário. Será gerado um DataFrame em que uma coluna é o ID do usuário e a outra coluna possui uma lista de interações\n",
    "        que cada usuário fez.\n",
    "\n",
    "        params:\n",
    "            interactions_df: DataFrame contendo as interações usuário-item.\n",
    "        \n",
    "            \n",
    "        returns:\n",
    "            Um DataFrame em que uma coluna é o ID do usuário (coluna self.user_column) e a outra coluna possui uma \n",
    "            lista de interações que cada usuário fez (coluna interactions).\n",
    "        '''\n",
    "        interactions_by_user = interactions_df\\\n",
    "                        .groupby(self.user_column)[[self.item_column]]\\\n",
    "                        .apply(lambda df_user: df_user[self.item_column].tolist())\\\n",
    "                        .reset_index(name='interactions')\n",
    "        interactions_by_user = interactions_by_user.reset_index(drop=True)\n",
    "        return interactions_by_user\n",
    "\n",
    "    def __merge_interactions_by_user(self, interactions_by_user_x: pd.DataFrame, interactions_by_user_y: pd.DataFrame) -> pd.DataFrame:\n",
    "        '''\n",
    "        Mescla dois DataFrames contendo interações por usuário. O resultado final será outro DataFrame em que uma \n",
    "        coluna é o ID do usuário e a outra coluna possui uma lista de interações que cada usuário fez, sendo que esta lista\n",
    "        terá as interações de ambos DataFrames usados na mesclagem. Por exemplo, se um usuário consumiu os itens 1 e 5 no\n",
    "        DataFrame X e os itens 2 e 9 no DataFrame Y, o resultado final para aquele usuário será uma lista contendo os itens\n",
    "        1, 2, 5 e 9.\n",
    "\n",
    "        params:\n",
    "            interactions_by_user_x: primeiro DataFrame contendo as interações agrupadas por usuário.\n",
    "            interactions_by_user_y: segundo DataFrame contendo as interações agrupadas por usuário.\n",
    "        \n",
    "        returns:\n",
    "            Uma mesclagem entre os dois DataFrames passados. O resultado será Um DataFrame em que uma coluna é o ID do usuário \n",
    "            (coluna self.user_column) e a outra coluna possui uma lista de interações que cada usuário fez (coluna interactions).\n",
    "        '''\n",
    "        def concat_user_interactions(row):\n",
    "            final_interactions = []\n",
    "            if isinstance(row['interactions_x'], list):\n",
    "                final_interactions += row['interactions_x']\n",
    "            if isinstance(row['interactions_y'], list):\n",
    "                final_interactions += row['interactions_y']\n",
    "            row['interactions'] = final_interactions\n",
    "            return row\n",
    "\n",
    "        merge_df = interactions_by_user_x.merge(interactions_by_user_y, how='outer', on=self.user_column).apply(concat_user_interactions, axis=1)\n",
    "        merge_df = merge_df.drop(['interactions_x', 'interactions_y'], axis=1)\n",
    "        return merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63c156a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mab2rec import LearningPolicy\n",
    "import src\n",
    "\n",
    "class Lin(Mab2RecRecommender):\n",
    "\n",
    "    def __init__(self, user_column: str=src.COLUMN_USER_ID, item_column: str=src.COLUMN_ITEM_ID, rating_column: str=src.COLUMN_RATING, hyperparameters: dict={}):\n",
    "        super().__init__(user_column, item_column, rating_column)\n",
    "\n",
    "        self.recommender = BanditRecommenderArmEncoded(\n",
    "            learning_policy=LearningPolicy.LinGreedy(epsilon=0),\n",
    "            top_k=src.TOP_N,\n",
    "            seed=src.RANDOM_STATE\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class LinGreedy(Mab2RecRecommender):\n",
    "\n",
    "    def __init__(self, user_column: str=src.COLUMN_USER_ID, item_column: str=src.COLUMN_ITEM_ID, rating_column: str=src.COLUMN_RATING, logger=None, hyperparameters: dict={}):\n",
    "        super().__init__(user_column, item_column, rating_column)\n",
    "\n",
    "        self.recommender = BanditRecommenderArmEncoded(\n",
    "            learning_policy=LearningPolicy.LinGreedy(**hyperparameters),\n",
    "            top_k=src.TOP_N,\n",
    "            seed=src.RANDOM_STATE\n",
    "        )\n",
    "\n",
    "class LinTS(Mab2RecRecommender):\n",
    "\n",
    "    def __init__(self, user_column: str=src.COLUMN_USER_ID, item_column: str=src.COLUMN_ITEM_ID, rating_column: str=src.COLUMN_RATING, logger=None, hyperparameters: dict={}):\n",
    "        super().__init__(user_column, item_column, rating_column, logger)\n",
    "\n",
    "        self.recommender = BanditRecommenderArmEncoded(\n",
    "            learning_policy=LearningPolicy.LinTS(**hyperparameters),\n",
    "            top_k=src.TOP_N,\n",
    "            seed=src.RANDOM_STATE,\n",
    "        )\n",
    "\n",
    "class LinUCB(Mab2RecRecommender):\n",
    "\n",
    "    def __init__(self, user_column: str=src.COLUMN_USER_ID, item_column: str=src.COLUMN_ITEM_ID, rating_column: str=src.COLUMN_RATING, logger=None, hyperparameters: dict={}):\n",
    "        super().__init__(user_column, item_column, rating_column, logger)\n",
    "\n",
    "        self.recommender = BanditRecommenderArmEncoded(\n",
    "            learning_policy=LearningPolicy.LinUCB(**hyperparameters),\n",
    "            top_k=src.TOP_N,\n",
    "            seed=src.RANDOM_STATE\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a31514",
   "metadata": {},
   "source": [
    "## Versão otimizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4eadc72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/fidelity/mabwiser/blob/master/mabwiser/linear.py\n",
    "# The difference is that the original code accept different types of arms (strings, integers, etc) and the modified code only accept sequential integers as arms. With this modification, the code can be optimized.\n",
    "\n",
    "ITEMS_PER_BATCH = 10_000\n",
    "INTERACTIONS_PER_BATCH_LINTS = 1_000\n",
    "\n",
    "from mabwiser.linear import _Linear\n",
    "from mabwiser.utils import Arm, Num, _BaseRNG\n",
    "from typing import Callable, Dict, List, Optional, Union\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "\n",
    "class _RidgeRegressionOptimized:\n",
    "\n",
    "    def __init__(self, rng: _BaseRNG, alpha: Num = 1.0, l2_lambda: Num = 1.0, scale: bool = False):\n",
    "\n",
    "        # Ridge Regression: https://onlinecourses.science.psu.edu/stat857/node/155/\n",
    "        self.rng = rng                      # random number generator\n",
    "        self.alpha = alpha                  # exploration parameter\n",
    "        self.l2_lambda = l2_lambda          # regularization parameter\n",
    "        self.scale = scale                  # scale contexts\n",
    "\n",
    "        self.beta = None                    # (XtX + l2_lambda * I_d)^-1 * Xty = A^-1 * Xty\n",
    "        self.A = None                       # (XtX + l2_lambda * I_d)\n",
    "        self.A_inv = None                   # (XtX + l2_lambda * I_d)^-1\n",
    "        self.Xty = None\n",
    "        self.scaler = None\n",
    "\n",
    "    def init(self, num_features: int, num_arms: int):\n",
    "        # By default, assume that\n",
    "        # A is the identity matrix and Xty is set to 0\n",
    "        start_time = time.time()\n",
    "        self.Xty = torch.zeros((num_arms, num_features), device='cuda', dtype=torch.double)\n",
    "        self.A = torch.eye(num_features, device='cuda', dtype=torch.double).unsqueeze(0).repeat(num_arms, 1, 1) * self.l2_lambda\n",
    "        #self.A_inv = self.A.clone()\n",
    "        self.beta = torch.zeros((num_arms, num_features), device='cuda', dtype=torch.double)\n",
    "        print(f'init demorou {time.time() - start_time} segundos')\n",
    "        #self.scaler = StandardScaler() if self.scale else None\n",
    "\n",
    "    def fit(self, decisions: np.ndarray, X: np.ndarray, y: np.ndarray):\n",
    "\n",
    "        # Scale\n",
    "        #if self.scaler is not None:\n",
    "        #    X = X.astype('float64')\n",
    "        #    if not hasattr(self.scaler, 'scale_'):\n",
    "        #        self.scaler.fit(X)\n",
    "        #    else:\n",
    "        #        self.scaler.partial_fit(X)\n",
    "        #    fix_small_variance(self.scaler)\n",
    "        #    X = self.scaler.transform(X)\n",
    "        start_time = time.time()\n",
    "        X_device = torch.tensor(X, device='cuda')\n",
    "        y_device = torch.tensor(y, device='cuda')\n",
    "        decisions_device = torch.tensor(decisions, device='cuda')\n",
    "        print(f'passar para cuda demorou {time.time() - start_time} segundos')\n",
    "        # Update A\n",
    "        #start_time = time.time()\n",
    "        #outer = torch.einsum('ni,nj->nij', X_device, X_device)  # (n, d, d)\n",
    "        #print(f'outer demorou {time.time() - start_time} segundos')\n",
    "\n",
    "        # Scatter add outer products into self.A based on decisions\n",
    "        start_time = time.time()\n",
    "        self.A.index_add_(0, decisions_device, torch.einsum('ni,nj->nij', X_device, X_device))\n",
    "        print(f'A add demorou {time.time() - start_time} segundos')\n",
    "\n",
    "        # Add X * y to Xty\n",
    "        start_time = time.time()\n",
    "        self.Xty.index_add_(0, decisions_device, X_device * y_device.view(-1, 1))\n",
    "        print(f'Xty demorou {time.time() - start_time} segundos')\n",
    "\n",
    "        # Invert each A matrix\n",
    "        for j in range(0, self.beta.shape[0], ITEMS_PER_BATCH):            \n",
    "            start_time = time.time()\n",
    "            self.beta[j:j+ITEMS_PER_BATCH] = torch.linalg.solve(\n",
    "                self.A[j:j+ITEMS_PER_BATCH],\n",
    "                self.Xty[j:j+ITEMS_PER_BATCH]\n",
    "            )\n",
    "            print(f'beta demorou {time.time() - start_time} segundos')\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "\n",
    "        # Calculate default expectation y = x * b\n",
    "        return torch.matmul(torch.tensor(x, device='cuda', dtype=torch.double), self.beta.T)\n",
    "\n",
    "\n",
    "class _LinTSOptimized2(_RidgeRegressionOptimized):\n",
    "\n",
    "    def __init__(self, rng: _BaseRNG, alpha: Num = 1.0, l2_lambda: Num = 1.0, scale: bool = False):\n",
    "        super().__init__(rng, alpha, l2_lambda, scale)\n",
    "        torch.manual_seed(src.RANDOM_STATE)\n",
    "    \n",
    "    def predict(self, x: np.ndarray):\n",
    "\n",
    "        # Randomly sample coefficients from multivariate normal distribution\n",
    "        # Covariance is enhanced with the exploration factor\n",
    "        # Generates  random samples for all contexts in one single go. type(beta_sampled): np.ndarray\n",
    "        # beta_sampled = self.rng.multivariate_normal(self.beta, np.square(self.alpha) * self.A_inv, size=x.shape[0])\n",
    "        mvn = torch.distributions.MultivariateNormal(loc=self.beta[0], covariance_matrix=((self.alpha ** 2) * torch.linalg.inv(self.A))[0])\n",
    "        beta_sampled = mvn.sample((x.shape[0],))\n",
    "        print('beta sampled torch')\n",
    "        print(beta_sampled)\n",
    "\n",
    "        beta_sampled2 = self.rng.multivariate_normal(self.beta.to('cpu').numpy()[0], ((self.alpha ** 2) * torch.linalg.inv(self.A)).to('cpu').numpy()[0], size=x.shape[0])\n",
    "        print('beta sample numpy')\n",
    "        print(beta_sampled2)\n",
    "        #scale_tril = torch.linalg.cholesky((self.alpha ** 2) * self.A_inv)  # [D, D]\n",
    "        #print(scale_tril.shape)\n",
    "\n",
    "        #z = torch.randn((x.shape[0], scale_tril.shape[0]), generator=self.rng, dtype=torch.float32, device='cuda')  # [N, D]\n",
    "        #print(z.shape)\n",
    "\n",
    "        #print(self.beta.shape)\n",
    "        #beta_sampled = self.beta + z @ scale_tril.T  # [N, D]\n",
    "        \n",
    "        # Calculate expectation y = x * beta_sampled\n",
    "        return torch.einsum('bji,bi->bj', beta_sampled, torch.tensor(x, device='cuda', dtype=torch.double))\n",
    "\n",
    "class _LinTSOptimized(_RidgeRegressionOptimized):\n",
    "\n",
    "    def __init__(self, rng: _BaseRNG, alpha: Num = 1.0, l2_lambda: Num = 1.0, scale: bool = False):\n",
    "        super().__init__(rng, alpha, l2_lambda, scale)\n",
    "        self.torch_rng = torch.Generator(device='cuda').manual_seed(src.RANDOM_STATE)\n",
    "    \n",
    "    def predict(self, x: np.ndarray):\n",
    "        x_torch = torch.tensor(x, device='cuda', dtype=torch.double)  # [B, D]\n",
    "\n",
    "        num_arms, num_features = self.beta.shape\n",
    "        num_contexts = x.shape[0]\n",
    "\n",
    "        scores = torch.zeros((num_contexts, num_arms), device='cuda', dtype=torch.double)\n",
    "\n",
    "        # z_all = torch.randn((num_arms, B, d), generator=self.torch_rng, device='cuda', dtype=torch.double)\n",
    "\n",
    "        for start in range(0, num_arms, ITEMS_PER_BATCH):\n",
    "            end = min(start + ITEMS_PER_BATCH, num_arms)\n",
    "            chunk_size = end - start\n",
    "\n",
    "            beta_chunk = self.beta[start:end]            # [chunk_size, D]\n",
    "            A_chunk = self.A[start:end]                  # [chunk_size, D, D]\n",
    "            A_inv_chunk = torch.linalg.inv(A_chunk)      # [chunk_size, D, D]\n",
    "\n",
    "            eps = torch.from_numpy(self.rng.standard_normal(size=(num_contexts, num_features))).cuda()  # [B, D]\n",
    "            L_chunk = torch.linalg.cholesky((self.alpha ** 2) * A_inv_chunk)  # [A, D, D]\n",
    "            beta_sampled = torch.einsum('bd,add->bad', eps, L_chunk) + beta_chunk  # [B, A, D]\n",
    "\n",
    "            scores[:, start:end] = torch.einsum('bd,bad->ba', x_torch, beta_sampled)\n",
    "\n",
    "        return scores  # shape: [B, M]\n",
    "\n",
    "\n",
    "\n",
    "class _LinUCBOptimized(_RidgeRegressionOptimized):\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "\n",
    "        x = torch.tensor(x, device='cuda')\n",
    "\n",
    "        scores = torch.matmul(x, self.beta.T)\n",
    "\n",
    "        for j in range(0, self.beta.shape[0], ITEMS_PER_BATCH):\n",
    "            x_A_inv = torch.matmul(x, torch.linalg.inv(self.A[j: j+ITEMS_PER_BATCH]))\n",
    "\n",
    "            # Upper confidence bound = alpha * sqrt(x A^-1 xt). Notice that, x = xt\n",
    "            # ucb values are claculated for all the contexts in one single go. type(ucb): np.ndarray\n",
    "            ucb = self.alpha * torch.sqrt(torch.sum(x_A_inv * x, axis=2))\n",
    "\n",
    "            # Calculate linucb expectation y = x * b + ucb\n",
    "            scores[:, j: j+ITEMS_PER_BATCH] += ucb.T\n",
    "        \n",
    "        return scores\n",
    "\n",
    "class LinearArmEncodedOptimized:\n",
    "    factory = {\n",
    "        \"ts\": _LinTSOptimized, \n",
    "        \"ucb\": _LinUCBOptimized, \n",
    "        \"ridge\": _RidgeRegressionOptimized\n",
    "    }\n",
    "\n",
    "    def __init__(self, rng: _BaseRNG, num_arms: int, num_features:int, n_jobs: int, backend: Optional[str],\n",
    "                 alpha: Num, epsilon: Num, l2_lambda: Num, regression: str, scale: bool):\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.regression = regression\n",
    "        self.scale = scale\n",
    "        self.n_jobs = n_jobs\n",
    "        self.backend = backend\n",
    "        self.rng = rng\n",
    "        self.num_arms = num_arms\n",
    "        self.num_features = num_features\n",
    "        self.model = self.factory[regression](rng, alpha, l2_lambda, scale)\n",
    "        self.model.init(self.num_features, self.num_arms)\n",
    "\n",
    "    def _vectorized_predict_context(self, contexts: np.ndarray, is_predict: bool) -> List:\n",
    "\n",
    "        # Initializing array with expectations for each arm\n",
    "        num_contexts = contexts.shape[0]\n",
    "        arm_expectations = self.model.predict(contexts)\n",
    "\n",
    "        # With epsilon probability, assign random flag to context\n",
    "        random_values = self.rng.rand(num_contexts)\n",
    "        random_mask = np.array(random_values < self.epsilon)\n",
    "        random_indices = random_mask.nonzero()[0]\n",
    "\n",
    "        # For random indices, generate random expectations\n",
    "        arm_expectations[random_indices] = torch.tensor(self.rng.rand((random_indices.shape[0], self.num_arms)), device='cuda')\n",
    "\n",
    "        # arm_expectations[nonrandom_indices] = np.array([self.arm_to_model[arm].predict(nonrandom_context)\n",
    "        #                                                for arm in arms]).T\n",
    "        # arm_expectations[nonrandom_indices] = self.model.predict(nonrandom_context)\n",
    "        # ARRUMAR ISSO, DESSA FORMA O LINGREEDY NAO IRA FUNCIONAR (ORIGINAL EMCIMA)\n",
    "        return arm_expectations if len(arm_expectations) > 1 else arm_expectations[0]\n",
    "    \n",
    "    def fit(self, decisions: np.ndarray, rewards: np.ndarray, contexts: np.ndarray = None) -> None:\n",
    "\n",
    "        #start_time = time.time()\n",
    "        # Initialize each model by arm\n",
    "        #self.num_features = contexts.shape[1]\n",
    "        #self.model.init(self.num_features, self.num_arms)\n",
    "        #print(f'init demorou {time.time() - start_time} segundos')\n",
    "\n",
    "        start_time = time.time()\n",
    "        # Perform parallel fit\n",
    "        self._fit(decisions, rewards, contexts)\n",
    "        print(f'parallel_fit demorou {time.time() - start_time} segundos')\n",
    "\n",
    "        # Update trained arms\n",
    "        # Removi o código abaixo pois parece que ele não é usado para o nosso caso...\n",
    "        # Ele parece ser usado apenas no contexto de tentar fazer \"warm\" start\n",
    "        # Basicamente, copiando os mesmos parâmetros de um arm já treinado para um novo (cold) por proximidade de features...\n",
    "\n",
    "        # Otimizar essa função não parece ser algo tão trivial, já que teria que mudar a estrutura do arm_to_status, tendo que mudar vários outros códigos por causa disso\n",
    "        # self._set_arms_as_trained(decisions=decisions, is_partial=False)\n",
    "    \n",
    "    def partial_fit(self, decisions: np.ndarray, rewards: np.ndarray, contexts: np.ndarray = None) -> None:\n",
    "        # Perform parallel fit\n",
    "        self._fit(decisions, rewards, contexts)\n",
    "\n",
    "        # Update trained arms\n",
    "        # self._set_arms_as_trained(decisions=decisions, is_partial=True)\n",
    "    \n",
    "    def _fit(self, decisions: np.ndarray, rewards: np.ndarray,\n",
    "                      contexts: Optional[np.ndarray] = None):\n",
    "        \n",
    "        # Compute effective number of jobs\n",
    "        #n_jobs = self._effective_jobs(len(self.arms), self.n_jobs)\n",
    "        # Perform parallel fit\n",
    "        #Parallel(n_jobs=n_jobs, require='sharedmem')(\n",
    "        #                  delayed(self._fit_arm)(\n",
    "        #                      arm, decisions, rewards, contexts)\n",
    "        #                 for arm in self.arms)\n",
    "        self.model.fit(decisions, contexts, rewards)\n",
    "    \n",
    "    def predict_expectations(self, contexts: np.ndarray = None) -> Union[Dict[Arm, Num], List[Dict[Arm, Num]]]:\n",
    "        # Return predict expectations for the given context\n",
    "        return self._vectorized_predict_context(contexts, is_predict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd352176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/fidelity/mabwiser/blob/master/mabwiser/mab.py\n",
    "# The difference is that the original code accept different types of arms (strings, integers, etc) and the modified code only accept sequential integers as arms. With this modification, the code can be optimized.\n",
    "\n",
    "from mabwiser.mab import MAB, LearningPolicyType, NeighborhoodPolicyType\n",
    "\n",
    "from mabwiser.utils import Constants, check_true, create_rng\n",
    "from mab2rec import LearningPolicy\n",
    "from mab2rec import NeighborhoodPolicy\n",
    "import numpy as np\n",
    "from mabwiser.neighbors import _KNearest, _Radius\n",
    "from mabwiser.treebandit import _TreeBandit\n",
    "from mabwiser.clusters import _Clusters\n",
    "from mabwiser.approximate import _LSHNearest\n",
    "\n",
    "from mabwiser.approximate import _LSHNearest\n",
    "from mabwiser.clusters import _Clusters\n",
    "from mabwiser.greedy import _EpsilonGreedy\n",
    "from mabwiser.linear import _Linear\n",
    "from mabwiser.neighbors import _KNearest, _Radius\n",
    "from mabwiser.popularity import _Popularity\n",
    "from mabwiser.rand import _Random\n",
    "from mabwiser.softmax import _Softmax\n",
    "from mabwiser.thompson import _ThompsonSampling\n",
    "from mabwiser.treebandit import _TreeBandit\n",
    "from mabwiser.ucb import _UCB1\n",
    "\n",
    "class MABArmEncodedOptimized(MAB):\n",
    "    def __init__(self,\n",
    "                 num_arms: int,  # The list of arms\n",
    "                 num_features: int,\n",
    "                 learning_policy: LearningPolicyType,  # The learning policy\n",
    "                 neighborhood_policy: NeighborhoodPolicyType = None,  # The context policy, optional\n",
    "                 seed: int = Constants.default_seed,  # The random seed\n",
    "                 n_jobs: int = 1,  # Number of parallel jobs\n",
    "                 backend: str = None,  # Parallel backend implementation\n",
    "                 ):\n",
    "        \"\"\"Initializes a multi-armed bandit (MAB) with the given arguments.\n",
    "\n",
    "        Validates the arguments and raises exception in case there are violations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        arms : List[Union[int, float, str]]\n",
    "            The list of all the arms available for decisions.\n",
    "            Arms can be integers, strings, etc.\n",
    "        learning_policy : LearningPolicyType\n",
    "            The learning policy.\n",
    "        neighborhood_policy : NeighborhoodPolicyType, optional\n",
    "            The context policy. Default value is None.\n",
    "        seed : numbers.Rational, optional\n",
    "            The random seed to initialize the random number generator.\n",
    "            Default value is set to Constants.default_seed.value\n",
    "        n_jobs: int, optional\n",
    "            This is used to specify how many concurrent processes/threads should be used for parallelized routines.\n",
    "            Default value is set to 1.\n",
    "            If set to -1, all CPUs are used.\n",
    "            If set to -2, all CPUs but one are used, and so on.\n",
    "        backend: str, optional\n",
    "            Specify a parallelization backend implementation supported in the joblib library. Supported options are:\n",
    "            - “loky” used by default, can induce some communication and memory overhead when exchanging input and\n",
    "              output data with the worker Python processes.\n",
    "            - “multiprocessing” previous process-based backend based on multiprocessing.Pool. Less robust than loky.\n",
    "            - “threading” is a very low-overhead backend but it suffers from the Python Global Interpreter Lock if the\n",
    "              called function relies a lot on Python objects.\n",
    "            Default value is None. In this case the default backend selected by joblib will be used.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError:  Arms were not provided in a list.\n",
    "        TypeError:  Learning policy type mismatch.\n",
    "        TypeError:  Context policy type mismatch.\n",
    "        TypeError:  Seed is not an integer.\n",
    "        TypeError:  Number of parallel jobs is not an integer.\n",
    "        TypeError:  Parallel backend is not a string.\n",
    "        TypeError:  For EpsilonGreedy, epsilon must be integer or float.\n",
    "        TypeError:  For LinGreedy, epsilon must be an integer or float.\n",
    "        TypeError:  For LinGreedy, l2_lambda must be an integer or float.\n",
    "        TypeError:  For LinTS, alpha must be an integer or float.\n",
    "        TypeError:  For LinTS, l2_lambda must be an integer or float.\n",
    "        TypeError:  For LinUCB, alpha must be an integer or float.\n",
    "        TypeError:  For LinUCB, l2_lambda must be an integer or float.\n",
    "        TypeError:  For Softmax, tau must be an integer or float.\n",
    "        TypeError:  For ThompsonSampling, binarizer must be a callable function.\n",
    "        TypeError:  For UCB, alpha must be an integer or float.\n",
    "        TypeError:  For LSHNearest, n_dimensions must be an integer or float.\n",
    "        TypeError:  For LSHNearest, n_tables must be an integer or float.\n",
    "        TypeError:  For LSHNearest, no_nhood_prob_of_arm must be None or List that sums up to 1.0.\n",
    "        TypeError:  For Clusters, n_clusters must be an integer.\n",
    "        TypeError:  For Clusters, is_minibatch must be a boolean.\n",
    "        TypeError:  For Radius, radius must be an integer or float.\n",
    "        TypeError:  For Radius, no_nhood_prob_of_arm must be None or List that sums up to 1.0.\n",
    "        TypeError:  For KNearest, k must be an integer or float.\n",
    "\n",
    "        ValueError: Invalid number of arms.\n",
    "        ValueError: Invalid values (None, NaN, Inf) in arms.\n",
    "        ValueError: Duplicate values in arms.\n",
    "        ValueError: Number of parallel jobs is 0.\n",
    "        ValueError: For EpsilonGreedy, epsilon must be between 0 and 1.\n",
    "        ValueError: For LinGreedy, epsilon must be between 0 and 1.\n",
    "        ValueError: For LinGreedy, l2_lambda cannot be negative.\n",
    "        ValueError: For LinTS, alpha must be greater than zero.\n",
    "        ValueError: For LinTS, l2_lambda must be greater than zero.\n",
    "        ValueError: For LinUCB, alpha cannot be negative.\n",
    "        ValueError: For LinUCB, l2_lambda cannot be negative.\n",
    "        ValueError: For Softmax, tau must be greater than zero.\n",
    "        ValueError: For UCB, alpha must be greater than zero.\n",
    "        ValueError: For LSHNearest, n_dimensions must be gerater than zero.\n",
    "        ValueError: For LSHNearest, n_tables must be gerater than zero.\n",
    "        ValueError: For LSHNearest, if given, no_nhood_prob_of_arm list should sum up to 1.0.\n",
    "        ValueError: For Clusters, n_clusters cannot be less than 2.\n",
    "        ValueError: For Radius and KNearest, metric is not supported by scipy.spatial.distance.cdist.\n",
    "        ValueError: For Radius, radius must be greater than zero.\n",
    "        ValueError: For Radius, if given, no_nhood_prob_of_arm list should sum up to 1.0.\n",
    "        ValueError: For KNearest, k must be greater than zero.\n",
    "        \"\"\"\n",
    "\n",
    "        # Validate arguments\n",
    "        # MAB._validate_mab_args(arms, learning_policy, neighborhood_policy, seed, n_jobs, backend)\n",
    "\n",
    "        # Save the arguments\n",
    "        self.arms = np.arange(num_arms)\n",
    "        self.num_arms = num_arms\n",
    "        self.num_features = num_features\n",
    "        self.seed = seed\n",
    "        self.n_jobs = n_jobs\n",
    "        self.backend = backend\n",
    "\n",
    "        # Create the random number generator\n",
    "        self._rng = create_rng(self.seed)\n",
    "        self._is_initial_fit = False\n",
    "\n",
    "        # Create the learning policy implementor\n",
    "        lp = None\n",
    "        if isinstance(learning_policy, LearningPolicy.LinGreedy):\n",
    "            lp = LinearArmEncodedOptimized(self._rng, num_arms, self.num_features, self.n_jobs, self.backend, 0, learning_policy.epsilon,\n",
    "                         learning_policy.l2_lambda, \"ridge\", learning_policy.scale)\n",
    "        elif isinstance(learning_policy, LearningPolicy.LinTS):\n",
    "            lp = LinearArmEncodedOptimized(self._rng, num_arms, self.num_features, self.n_jobs, self.backend, learning_policy.alpha, 0,\n",
    "                         learning_policy.l2_lambda, \"ts\", learning_policy.scale)\n",
    "        elif isinstance(learning_policy, LearningPolicy.LinUCB):\n",
    "            lp = LinearArmEncodedOptimized(self._rng, num_arms, self.num_features, self.n_jobs, self.backend, learning_policy.alpha, 0,\n",
    "                         learning_policy.l2_lambda, \"ucb\", learning_policy.scale)\n",
    "        else:\n",
    "            check_true(False, ValueError(\"Undefined learning policy \" + str(learning_policy)))\n",
    "\n",
    "        if neighborhood_policy:\n",
    "            # Do not use parallel fit or predict for Learning Policy when contextual\n",
    "            lp.n_jobs = 1\n",
    "\n",
    "            if isinstance(neighborhood_policy, NeighborhoodPolicy.Clusters):\n",
    "                self._imp = _Clusters(self._rng, self.arms, self.n_jobs, self.backend, lp,\n",
    "                                      neighborhood_policy.n_clusters, neighborhood_policy.is_minibatch)\n",
    "            elif isinstance(neighborhood_policy, NeighborhoodPolicy.LSHNearest):\n",
    "                self._imp = _LSHNearest(self._rng, self.arms, self.n_jobs, self.backend, lp,\n",
    "                                        neighborhood_policy.n_dimensions, neighborhood_policy.n_tables,\n",
    "                                        neighborhood_policy.no_nhood_prob_of_arm)\n",
    "            elif isinstance(neighborhood_policy, NeighborhoodPolicy.KNearest):\n",
    "                self._imp = _KNearest(self._rng, self.arms, self.n_jobs, self.backend, lp,\n",
    "                                      neighborhood_policy.k, neighborhood_policy.metric)\n",
    "            elif isinstance(neighborhood_policy, NeighborhoodPolicy.Radius):\n",
    "                self._imp = _Radius(self._rng, self.arms, self.n_jobs, self.backend, lp,\n",
    "                                    neighborhood_policy.radius, neighborhood_policy.metric,\n",
    "                                    neighborhood_policy.no_nhood_prob_of_arm)\n",
    "            elif isinstance(neighborhood_policy, NeighborhoodPolicy.TreeBandit):\n",
    "                self._imp = _TreeBandit(self._rng, self.arms, self.n_jobs, self.backend, lp,\n",
    "                                        neighborhood_policy.tree_parameters)\n",
    "            else:\n",
    "                check_true(False, ValueError(\"Undefined context policy \" + str(neighborhood_policy)))\n",
    "        else:\n",
    "            self._imp = lp\n",
    "        \n",
    "        self.is_contextual = True\n",
    "    \n",
    "    @property\n",
    "    def learning_policy(self):\n",
    "        \"\"\"\n",
    "        Creates named tuple of the learning policy based on the implementor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The learning policy.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        NotImplementedError: MAB learning_policy property not implemented for this learning policy.\n",
    "\n",
    "        \"\"\"\n",
    "        if isinstance(self._imp, (_LSHNearest, _KNearest, _Radius, _TreeBandit)):\n",
    "            lp = self._imp.lp\n",
    "        elif isinstance(self._imp, _Clusters):\n",
    "            lp = self._imp.lp_list[0]\n",
    "        else:\n",
    "            lp = self._imp\n",
    "\n",
    "        if isinstance(lp, _EpsilonGreedy):\n",
    "            if issubclass(type(lp), _Popularity):\n",
    "                return LearningPolicy.Popularity()\n",
    "            else:\n",
    "                return LearningPolicy.EpsilonGreedy(lp.epsilon)\n",
    "        elif isinstance(lp, _Linear):\n",
    "            if lp.regression == 'ridge':\n",
    "                return LearningPolicy.LinGreedy(lp.epsilon, lp.l2_lambda, lp.scale)\n",
    "            elif lp.regression == 'ts':\n",
    "                return LearningPolicy.LinTS(lp.alpha, lp.l2_lambda, lp.scale)\n",
    "            elif lp.regression == 'ucb':\n",
    "                return LearningPolicy.LinUCB(lp.alpha, lp.l2_lambda, lp.scale)\n",
    "            else:\n",
    "                check_true(False, ValueError(\"Undefined regression \" + str(lp.regression)))\n",
    "        elif isinstance(lp, LinearArmEncodedOptimized):\n",
    "            if lp.regression == 'ridge':\n",
    "                return _RidgeRegressionOptimized(lp.rng, lp.alpha, lp.l2_lambda, lp.scale)\n",
    "            elif lp.regression == 'ts':\n",
    "                return _LinTSOptimized(lp.alpha, lp.l2_lambda, lp.scale)\n",
    "            elif lp.regression == 'ucb':\n",
    "                return _LinUCBOptimized(lp.alpha, lp.l2_lambda, lp.scale)\n",
    "            else:\n",
    "                check_true(False, ValueError(\"Undefined regression \" + str(lp.regression)))\n",
    "        elif isinstance(lp, _Random):\n",
    "            return LearningPolicy.Random()\n",
    "        elif isinstance(lp, _Softmax):\n",
    "            return LearningPolicy.Softmax(lp.tau)\n",
    "        elif isinstance(lp, _ThompsonSampling):\n",
    "            return LearningPolicy.ThompsonSampling(lp.binarizer)\n",
    "        elif isinstance(lp, _UCB1):\n",
    "            return LearningPolicy.UCB1(lp.alpha)\n",
    "        else:\n",
    "            raise NotImplementedError(\"MAB learning_policy property not implemented for this learning policy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd3fbaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/fidelity/mab2rec/blob/main/mab2rec/rec.py\n",
    "# The difference is that the original code accept different types of arms (strings, integers, etc) and the modified code only accept sequential integers as arms. With this modification, the code can be optimized.\n",
    "\n",
    "from typing import List, Tuple, Union\n",
    "from mabwiser.utils import Arm, Num\n",
    "from scipy.special import expit\n",
    "from mab2rec import BanditRecommender\n",
    "from mab2rec import LearningPolicy\n",
    "from mabwiser.utils import Arm\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "class BanditRecommenderArmEncodedOptimized(BanditRecommender):\n",
    "\n",
    "    def __init__(self, learning_policy: Union[LearningPolicy.LinGreedy,\n",
    "                                              LearningPolicy.LinTS,\n",
    "                                              LearningPolicy.LinUCB],\n",
    "                 num_arms: int,\n",
    "                 num_features: int,\n",
    "                 neighborhood_policy: Union[None] = None,\n",
    "                 top_k: int = 10,\n",
    "                 seed: int = src.RANDOM_STATE,\n",
    "                 n_jobs: int = 1,\n",
    "                 backend: str = None):\n",
    "        \"\"\"Initializes bandit recommender with the given arguments.\n",
    "\n",
    "        Validates the arguments and raises exception in case there are violations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        learning_policy : LearningPolicy\n",
    "            The learning policy.\n",
    "        neighborhood_policy : NeighborhoodPolicy, default=None\n",
    "            The context policy.\n",
    "        top_k : int, default=10\n",
    "            The number of items to recommend.\n",
    "        seed : numbers.Rational, default=Constants.default_seed\n",
    "            The random seed to initialize the random number generator.\n",
    "            Default value is set to Constants.default_seed.value\n",
    "        top_k : int, default=10\n",
    "            The number of items to recommend.\n",
    "        n_jobs : int, default=1\n",
    "            This is used to specify how many concurrent processes/threads should be used for parallelized routines.\n",
    "            If set to -1, all CPUs are used.\n",
    "            If set to -2, all CPUs but one are used, and so on.\n",
    "        backend : str, default=None\n",
    "            Specify a parallelization backend implementation supported in the joblib library. Supported options are:\n",
    "            - “loky” used by default, can induce some communication and memory overhead when exchanging input and\n",
    "              output data with the worker Python processes.\n",
    "            - “multiprocessing” previous process-based backend based on multiprocessing.Pool. Less robust than loky.\n",
    "            - “threading” is a very low-overhead backend but it suffers from the Python Global Interpreter Lock if the\n",
    "              called function relies a lot on Python objects.\n",
    "            Default value is None. In this case the default backend selected by joblib will be used.\n",
    "        \"\"\"\n",
    "        self.num_arms = num_arms\n",
    "        self.num_features = num_features\n",
    "        super().__init__(learning_policy, neighborhood_policy, top_k, seed, n_jobs, backend)\n",
    "    \n",
    "    def _init(self) -> None:\n",
    "        \"\"\"Initializes recommender with given list of arms.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        arms : List[Union[Arm]]\n",
    "            The list of all of the arms available for decisions.\n",
    "            Arms can be integers, strings, etc.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Returns nothing\n",
    "        \"\"\"\n",
    "        self.mab = MABArmEncodedOptimized(self.num_arms, self.num_features, self.learning_policy, self.neighborhood_policy, self.seed, self.n_jobs, self.backend)\n",
    "    \n",
    "    def fit(self, decisions: Union[List[Arm], np.ndarray, pd.Series],\n",
    "            rewards: Union[List[Num], np.ndarray, pd.Series],\n",
    "            contexts: Union[None, List[List[Num]], np.ndarray, pd.Series, pd.DataFrame] = None) -> None:\n",
    "        \"\"\"Fits the recommender the given *decisions*, their corresponding *rewards* and *contexts*, if any.\n",
    "        If the recommender arms has not been initialized using the `set_arms`, the recommender arms will be set\n",
    "        to the list of arms in *decisions*.\n",
    "\n",
    "        Validates arguments and raises exceptions in case there are violations.\n",
    "\n",
    "        This function makes the following assumptions:\n",
    "            - each decision corresponds to an arm of the bandit.\n",
    "            - there are no ``None``, ``Nan``, or ``Infinity`` values in the contexts.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "         decisions : Union[List[Arm], np.ndarray, pd.Series]\n",
    "            The decisions that are made.\n",
    "         rewards : Union[List[Num], np.ndarray, pd.Series]\n",
    "            The rewards that are received corresponding to the decisions.\n",
    "         contexts : Union[None, List[List[Num]], np.ndarray, pd.Series, pd.DataFrame], default=None\n",
    "            The context under which each decision is made.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Returns nothing.\n",
    "        \"\"\"\n",
    "        if self.mab is None:\n",
    "            self._init()\n",
    "        self.mab.fit(decisions, rewards, contexts)\n",
    "    \n",
    "    def recommend(self, contexts: Union[None, List[List[Num]], np.ndarray, pd.Series, pd.DataFrame] = None,\n",
    "                  excluded_arms: List[List[Arm]] = None, return_scores: bool = False, apply_sigmoid: bool = True) \\\n",
    "            -> Union[Union[List[Arm], Tuple[List[Arm], List[Num]],\n",
    "                     Union[List[List[Arm]], Tuple[List[List[Arm]], List[List[Num]]]]]]:\n",
    "        #self._validate_mab(is_fit=True)\n",
    "        #self._validate_get_rec(contexts, excluded_arms)\n",
    "\n",
    "        start_time = time.time()\n",
    "        # Get predicted expectations\n",
    "        num_contexts = len(contexts) if contexts is not None else 1\n",
    "        if num_contexts == 1:\n",
    "            expectations = np.array([self.mab.predict_expectations(contexts)])\n",
    "        else:\n",
    "            expectations = self.mab.predict_expectations(contexts)\n",
    "        \n",
    "        #if not isinstance(expectations, np.ndarray):\n",
    "        #    expectations = np.array(expectations)\n",
    "\n",
    "        print(f'predict_expectations demorou {time.time() - start_time} segundos')\n",
    "\n",
    "        if apply_sigmoid:\n",
    "            expectations = expit(expectations)\n",
    "\n",
    "        # Create an exclusion mask, where exclusion_mask[context_ind][arm_ind] denotes if the arm with the\n",
    "        # index arm_ind was excluded for context with the index context_ind.\n",
    "        # The value will be True if it is excluded and those arms will not be returned as part of the results.\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Set excluded item scores to -1, so they automatically get placed lower in best results\n",
    "        excluded_rows = torch.from_numpy(excluded_arms[0]).to(expectations.device)\n",
    "        excluded_cols = torch.from_numpy(excluded_arms[1]).to(expectations.device)\n",
    "        expectations[excluded_rows, excluded_cols] = -1.\n",
    "        print(f'Exclude mask demorou {time.time() - start_time} segundos')\n",
    "\n",
    "        start_time = time.time()\n",
    "        # Get best `top_k` results by sorting the expectations\n",
    "        #expectations = torch.tensor(expectations, device='cuda')\n",
    "        topk_sorted_expectations = torch.topk(expectations, self.top_k, dim=1)\n",
    "        recommendations = topk_sorted_expectations.indices.cpu().numpy()\n",
    "        scores = topk_sorted_expectations.values.cpu().numpy()\n",
    "        #arm_inds = np.argpartition(-expectations, self.top_k - 1, axis=1)[:, :self.top_k]\n",
    "        #arm_inds = arm_inds[np.arange(arm_inds.shape[0]).reshape(-1, 1), np.argsort(-expectations[np.arange(expectations.shape[0]).reshape(-1, 1), arm_inds], axis=1)]\n",
    "        print(f'Ordenação top-K demorou {time.time() - start_time} segundos')\n",
    "        \n",
    "\n",
    "        #start_time = time.time()\n",
    "        # Get the list of top_k recommended items and corresponding expectations for each context\n",
    "        #recommendations = arm_inds\n",
    "        #scores = [[]] * num_contexts\n",
    "        #for context_ind in range(num_contexts):\n",
    "        #    recommendations[context_ind] = [self.mab.arms[arm_ind] for arm_ind in arm_inds[context_ind]]\n",
    "        #    if len(recommendations[context_ind]) != self.top_k:\n",
    "        #        print('Warning: the number of recommendations is less than the top_k value. ')\n",
    "        #    if return_scores:\n",
    "        #        scores[context_ind] = [expectations[context_ind, arm_ind] for arm_ind in arm_inds[context_ind]]\n",
    "        #print(f'gerar lista de recomendações demorou {time.time() - start_time} segundos')\n",
    "        # Return recommendations and scores\n",
    "        if return_scores:\n",
    "            if num_contexts > 1:\n",
    "                return recommendations, scores\n",
    "            else:\n",
    "                return recommendations[0], scores[0]\n",
    "        else:\n",
    "            if num_contexts > 1:\n",
    "                return recommendations\n",
    "            else:\n",
    "                return recommendations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccce633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "from mab2rec import LearningPolicy\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "class Mab2RecRecommenderOptimized(ABC):\n",
    "    '''\n",
    "    Classe base para os recomendadores da biblioteca [mab2rec](https://github.com/fidelity/mab2rec).\n",
    "\n",
    "    Essa classe é responsável por encapsular a lógica de inicialização e treinamento dos algoritmos da biblioteca mab2rec.\n",
    "\n",
    "    Essa classe não deve ser utilizada diretamente. Para utilizar um algoritmo da biblioteca mab2rec, utilize uma das classes filhas dessa classe.\n",
    "\n",
    "    Para implementar um novo algoritmo da biblioteca mab2rec, basta na classe filha implementar o método __init__ chamando o super().__init__ e inicializar o atributo self.recommender com o algoritmo da biblioteca mab2rec (pelo menos, a ideia inicial é ser simples assim).\n",
    "    '''\n",
    "\n",
    "    def __init__(self, num_users: int, num_items: int, num_features: int, user_column: str=src.COLUMN_USER_ID, item_column: str=src.COLUMN_ITEM_ID, rating_column: str=src.COLUMN_RATING, hyperparameters: dict={}):\n",
    "        '''\n",
    "        Inicializa o recomendador.\n",
    "\n",
    "        params:\n",
    "            user_column: Nome da coluna que representa o usuário.\n",
    "            item_column: Nome da coluna que representa o item.\n",
    "            rating_column: Nome da coluna que representa a avaliação.\n",
    "        '''\n",
    "        self.user_column = user_column\n",
    "        self.item_column = item_column\n",
    "        self.rating_column = rating_column\n",
    "        self.num_items = num_items\n",
    "        self.num_users = num_users\n",
    "        self.num_features = num_features\n",
    "\n",
    "        self.interactions_by_user: pd.DataFrame = None\n",
    "        self.recommender: LearningPolicy = None\n",
    "\n",
    "        self.exclude_mask = csr_matrix(([], ([], [])), shape=(self.num_users, self.num_items), dtype=bool).tolil()\n",
    "\n",
    "    def train(self, interactions_df: pd.DataFrame, contexts):\n",
    "        '''\n",
    "        Treina \"do zero\" o recomendador com base nas interações passadas. Utilizar apenas na primeira chamada de treinamento, caso deseje treinar incrementalmente.\n",
    "\n",
    "        params:\n",
    "            interactions_df: DataFrame contendo as interações usuário-item.\n",
    "        '''\n",
    "\n",
    "        self.exclude_mask[interactions_df[self.user_column], interactions_df[self.item_column]] = True\n",
    "        self.recommender.fit(\n",
    "            decisions=interactions_df[self.item_column],\n",
    "            rewards=interactions_df[self.rating_column],\n",
    "            contexts=contexts\n",
    "        )\n",
    "\n",
    "    def partial_train(self, interactions_df: pd.DataFrame, contexts):\n",
    "        '''\n",
    "        Treina o recomendador incrementalmente com base nas interações passadas. Deve ser utilizado após a primeira chamada de treinamento, o novo conhecimento será incorporado ao modelo, sem esquecer o conhecimento anterior.\n",
    "\n",
    "        params:\n",
    "            interactions_df: DataFrame contendo as interações usuário-item.\n",
    "        '''\n",
    "        self.exclude_mask[interactions_df[self.user_column], interactions_df[self.item_column]] = True\n",
    "        self.recommender.partial_fit(\n",
    "            decisions=interactions_df[self.item_column],\n",
    "            rewards=interactions_df[self.rating_column],\n",
    "            contexts=contexts\n",
    "        )\n",
    "    \n",
    "    def recommend(self, users_ids: 'Union[list[int], np.ndarray]', contexts) -> 'tuple[list[int], list[float]]':\n",
    "        '''\n",
    "        Gera recomendações para uma lista de usuários.\n",
    "\n",
    "        params:\n",
    "            users_ids: Lista de IDs dos usuários para os quais deseja-se gerar recomendações.\n",
    "            topn: Número máximo de recomendações a serem geradas por `user_id`.\n",
    "\n",
    "        returns:\n",
    "            Tupla contendo dois arrays: o primeiro contém os IDs dos itens recomendados e o segundo contém a pontuação de cada item.\n",
    "        '''\n",
    "        return self.recommender.recommend(contexts, self.exclude_mask[users_ids].nonzero(), apply_sigmoid=False, return_scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f0d0457",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from mab2rec import LearningPolicy\n",
    "import src\n",
    "\n",
    "class LinOptimized(Mab2RecRecommenderOptimized):\n",
    "\n",
    "    def __init__(self, num_users: int, num_items: int, num_features: int, user_column: str=src.COLUMN_USER_ID, item_column: str=src.COLUMN_ITEM_ID, rating_column: str=src.COLUMN_RATING, hyperparameters: dict={}):\n",
    "        super().__init__(num_users, num_items, num_features, user_column, item_column, rating_column)\n",
    "\n",
    "        self.recommender = BanditRecommenderArmEncodedOptimized(\n",
    "            num_arms=self.num_items,\n",
    "            num_features=self.num_features,\n",
    "            learning_policy=LearningPolicy.LinGreedy(epsilon=0),\n",
    "            top_k=src.TOP_N,\n",
    "            seed=src.RANDOM_STATE\n",
    "        )\n",
    "\n",
    "class LinGreedyOptimized(Mab2RecRecommenderOptimized):\n",
    "\n",
    "    def __init__(self, num_users: int, num_arms: int, num_features: int, user_column: str=src.COLUMN_USER_ID, item_column: str=src.COLUMN_ITEM_ID, rating_column: str=src.COLUMN_RATING, hyperparameters: dict={}):\n",
    "        super().__init__(num_users, num_arms, num_features, user_column, item_column, rating_column)\n",
    "\n",
    "        self.recommender = BanditRecommenderArmEncodedOptimized(\n",
    "            num_arms=self.num_arms,\n",
    "            num_features=self.num_features,\n",
    "            learning_policy=LearningPolicy.LinGreedy(**hyperparameters),\n",
    "            top_k=src.TOP_N,\n",
    "            seed=src.RANDOM_STATE\n",
    "        )\n",
    "\n",
    "class LinUCBOptimized(Mab2RecRecommenderOptimized):\n",
    "\n",
    "    def __init__(self, num_users: int, num_items: int, num_features: int, user_column: str=src.COLUMN_USER_ID, item_column: str=src.COLUMN_ITEM_ID, rating_column: str=src.COLUMN_RATING, hyperparameters: dict={}):\n",
    "        super().__init__(num_users, num_items, num_features, user_column, item_column, rating_column)\n",
    "\n",
    "        self.recommender = BanditRecommenderArmEncodedOptimized(\n",
    "            num_arms=self.num_items,\n",
    "            num_features=self.num_features,\n",
    "            learning_policy=LearningPolicy.LinUCB(**hyperparameters),\n",
    "            top_k=src.TOP_N,\n",
    "            seed=src.RANDOM_STATE\n",
    "        )\n",
    "\n",
    "class LinTSOptimized(Mab2RecRecommenderOptimized):\n",
    "\n",
    "    def __init__(self, num_users: int, num_items: int, num_features: int, user_column: str=src.COLUMN_USER_ID, item_column: str=src.COLUMN_ITEM_ID, rating_column: str=src.COLUMN_RATING, hyperparameters: dict={}):\n",
    "        super().__init__(num_users, num_items, num_features, user_column, item_column, rating_column)\n",
    "\n",
    "        self.recommender = BanditRecommenderArmEncodedOptimized(\n",
    "            num_arms=self.num_items,\n",
    "            num_features=self.num_features,\n",
    "            learning_policy=LearningPolicy.LinTS(**hyperparameters),\n",
    "            top_k=src.TOP_N,\n",
    "            seed=src.RANDOM_STATE\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf027e3",
   "metadata": {},
   "source": [
    "## Geração de dados toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9caee523",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(src.RANDOM_STATE)\n",
    "\n",
    "def generate_toy_dataset(num_users: int, num_items: int, num_interactions: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Gera um dataset de interações aleatórias entre usuários e itens.\n",
    "\n",
    "    :param num_users: Número de usuários.\n",
    "    :param num_items: Número de itens.\n",
    "    :param num_interactions: Número total de interações a serem geradas.\n",
    "    :return: DataFrame contendo as interações.\n",
    "    \"\"\"\n",
    "    first_users = np.arange(num_users)\n",
    "    users = np.random.randint(0, num_users, size=num_interactions - len(first_users))\n",
    "\n",
    "    first_items = np.arange(num_items)\n",
    "    items = np.random.randint(0, num_items, size=num_interactions - len(first_items))\n",
    "\n",
    "    ratings = np.random.randint(0, 2, size=num_interactions)  # Avaliações entre 0 e 1 (binárias)\n",
    "\n",
    "    full_dataset = pd.DataFrame({\n",
    "        src.COLUMN_USER_ID: np.concatenate([first_users, users]),\n",
    "        src.COLUMN_ITEM_ID: np.concatenate([first_items, items]),\n",
    "        src.COLUMN_RATING: ratings\n",
    "    })\n",
    "\n",
    "    return full_dataset[:num_interactions//2], full_dataset[num_interactions//2:]\n",
    "\n",
    "train_100_100_1k, test_100_100_1k = generate_toy_dataset(100, 100, 1000)\n",
    "train_1k_1k_10k, test_1k_1k_10k = generate_toy_dataset(1_000, 1_000, 10_000)\n",
    "train_10k_10k_100k, test_10k_10k_100k = generate_toy_dataset(10_000, 10_000, 100_000)\n",
    "train_20k_20k_250k, test_20k_20k_250k = generate_toy_dataset(20_000, 20_000, 250_000)\n",
    "\n",
    "train_10k_50k_250k, test_10k_50k_250k = generate_toy_dataset(10_000, 50_000, 250_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63306677",
   "metadata": {},
   "source": [
    "## Testes lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a801cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate current memory usage\n",
    "import psutil\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_info = process.memory_info()\n",
    "    return memory_info.rss / (1024 * 1024)  # Convert bytes to MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8794766d",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo1_contexts = np.random.rand(train_100_100_1k.shape[0], 10)\n",
    "algo1_contexts_rec = np.random.rand(test_100_100_1k.shape[0], 10)\n",
    "\n",
    "algo2_contexts = np.random.rand(train_1k_1k_10k.shape[0], 10)\n",
    "algo2_contexts_rec = np.random.rand(test_1k_1k_10k.shape[0], 10)\n",
    "\n",
    "algo3_contexts = np.random.rand(train_10k_10k_100k.shape[0], 10)\n",
    "algo3_contexts_rec = np.random.rand(test_10k_10k_100k.shape[0], 10)\n",
    "\n",
    "algo4_contexts = np.random.rand(train_20k_20k_250k.shape[0], 10)\n",
    "algo4_contexts_rec = np.random.rand(test_20k_20k_250k.shape[0], 10)\n",
    "\n",
    "algo5_contexts = np.random.rand(train_10k_50k_250k.shape[0], 128)\n",
    "algo5_contexts_rec = np.random.rand(test_10k_50k_250k.shape[0], 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ded17c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.0015604496002197266 segundos\n",
      "reset_arm_to_status demorou 9.5367431640625e-07 segundos\n",
      "parallel_fit demorou 0.026643037796020508 segundos\n",
      "Gerar as predições demorou 0.0013499259948730469 segundos\n",
      "predict_expectations demorou 0.0015196800231933594 segundos\n",
      "Ordenação top-K demorou 0.0010988712310791016 segundos\n",
      "gerar lista de recomendações demorou 0.006888389587402344 segundos\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Time taken by 100 users, 100 items and 1k interactions: 0.06 seconds\n",
      "Memory used by 100 users, 100 items and 1k interactions: 1.92 MB\n"
     ]
    }
   ],
   "source": [
    "start_memory_usage = get_memory_usage()\n",
    "algo1 = Lin()\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "algo1.train(train_100_100_1k, contexts=algo1_contexts)\n",
    "memory_used_algo1 = get_memory_usage() - start_memory_usage\n",
    "results_algo1 = algo1.recommend(users_ids=test_100_100_1k[src.COLUMN_USER_ID], contexts=algo1_contexts_rec)\n",
    "\n",
    "total_time_algo1 = time.time() - start_time\n",
    "\n",
    "print('\\n\\n\\n-----------------------------------------------------------\\n')\n",
    "print(f\"Time taken by 100 users, 100 items and 1k interactions: {total_time_algo1:.2f} seconds\")\n",
    "print(f\"Memory used by 100 users, 100 items and 1k interactions: {memory_used_algo1:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71b4b666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.014403104782104492 segundos\n",
      "reset_arm_to_status demorou 4.76837158203125e-07 segundos\n",
      "parallel_fit demorou 0.2588481903076172 segundos\n",
      "Gerar as predições demorou 0.14911866188049316 segundos\n",
      "predict_expectations demorou 0.14976954460144043 segundos\n",
      "Ordenação top-K demorou 0.09089255332946777 segundos\n",
      "gerar lista de recomendações demorou 0.06270337104797363 segundos\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Time taken by 1k users, 1k items and 10k interactions: 0.73 seconds\n",
      "Memory used by 1k users, 1k items and 10k interactions: 3.88 MB\n"
     ]
    }
   ],
   "source": [
    "start_memory_usage = get_memory_usage()\n",
    "algo2 = Lin()\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "algo2.train(train_1k_1k_10k, contexts=algo2_contexts)\n",
    "memory_used_algo2 = get_memory_usage() - start_memory_usage\n",
    "results_algo2 = algo2.recommend(users_ids=test_1k_1k_10k[src.COLUMN_USER_ID], contexts=algo2_contexts_rec)\n",
    "\n",
    "total_time_algo2 = time.time() - start_time\n",
    "\n",
    "print('\\n\\n\\n-----------------------------------------------------------\\n')\n",
    "print(f\"Time taken by 1k users, 1k items and 10k interactions: {total_time_algo2:.2f} seconds\")\n",
    "print(f\"Memory used by 1k users, 1k items and 10k interactions: {memory_used_algo2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "431560f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.09664511680603027 segundos\n",
      "reset_arm_to_status demorou 7.152557373046875e-07 segundos\n",
      "parallel_fit demorou 2.4247357845306396 segundos\n",
      "Gerar as predições demorou 11.9911527633667 segundos\n",
      "predict_expectations demorou 11.996206998825073 segundos\n",
      "Ordenação top-K demorou 7.821106672286987 segundos\n",
      "gerar lista de recomendações demorou 0.8709344863891602 segundos\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Time taken by 10k users, 10k items and 100k interactions: 24.25 seconds\n",
      "Memory used by 10k users, 10k items and 100k interactions: 22.92 MB\n"
     ]
    }
   ],
   "source": [
    "start_memory_usage = get_memory_usage()\n",
    "algo3 = Lin()\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "algo3.train(train_10k_10k_100k, contexts=algo3_contexts)\n",
    "memory_used_algo3 = get_memory_usage() - start_memory_usage\n",
    "results_algo3 = algo3.recommend(users_ids=test_10k_10k_100k[src.COLUMN_USER_ID], contexts=algo3_contexts_rec)\n",
    "\n",
    "total_time_algo3 = time.time() - start_time\n",
    "\n",
    "print('\\n\\n\\n-----------------------------------------------------------\\n')\n",
    "print(f\"Time taken by 10k users, 10k items and 100k interactions: {total_time_algo3:.2f} seconds\")\n",
    "print(f\"Memory used by 10k users, 10k items and 100k interactions: {memory_used_algo3:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2714cc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.2299494743347168 segundos\n",
      "reset_arm_to_status demorou 7.152557373046875e-07 segundos\n",
      "parallel_fit demorou 6.000527858734131 segundos\n",
      "Gerar as predições demorou 55.42299270629883 segundos\n",
      "predict_expectations demorou 55.42787313461304 segundos\n",
      "Ordenação top-K demorou 45.45101046562195 segundos\n",
      "gerar lista de recomendações demorou 2.0519351959228516 segundos\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Time taken by 20k users, 20k items and 250k interactions: 112.63 seconds\n",
      "Memory used by 20k users, 20k items and 250k interactions: 40.95 MB\n"
     ]
    }
   ],
   "source": [
    "start_memory_usage = get_memory_usage()\n",
    "algo4 = Lin()\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "algo4.train(train_20k_20k_250k, contexts=algo4_contexts)\n",
    "memory_used_algo4 = get_memory_usage() - start_memory_usage\n",
    "results_algo4 = algo4.recommend(users_ids=test_20k_20k_250k[src.COLUMN_USER_ID], contexts=algo4_contexts_rec)\n",
    "\n",
    "total_time_algo4 = time.time() - start_time\n",
    "\n",
    "print('\\n\\n\\n-----------------------------------------------------------\\n')\n",
    "print(f\"Time taken by 20k users, 20k items and 250k interactions: {total_time_algo4:.2f} seconds\")\n",
    "print(f\"Memory used by 20k users, 20k items and 250k interactions: {memory_used_algo4:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a88296a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.5228850841522217 segundos\n",
      "passar para cuda demorou 0.0006272792816162109 segundos\n",
      "A add demorou 0.08681988716125488 segundos\n",
      "Xty demorou 0.0003108978271484375 segundos\n",
      "beta demorou 0.03995251655578613 segundos\n",
      "beta demorou 0.0002601146697998047 segundos\n",
      "beta demorou 0.00021791458129882812 segundos\n",
      "beta demorou 0.00021123886108398438 segundos\n",
      "beta demorou 0.0002090930938720703 segundos\n",
      "beta demorou 0.00020837783813476562 segundos\n",
      "beta demorou 0.00020813941955566406 segundos\n",
      "beta demorou 0.00020694732666015625 segundos\n",
      "beta demorou 0.00020766258239746094 segundos\n",
      "beta demorou 0.00020694732666015625 segundos\n",
      "beta demorou 0.00020742416381835938 segundos\n",
      "beta demorou 0.003587484359741211 segundos\n",
      "beta demorou 0.0003743171691894531 segundos\n",
      "beta demorou 0.0002224445343017578 segundos\n",
      "beta demorou 0.00021266937255859375 segundos\n",
      "beta demorou 0.00020694732666015625 segundos\n",
      "beta demorou 0.0002086162567138672 segundos\n",
      "beta demorou 0.0002086162567138672 segundos\n",
      "beta demorou 0.00022602081298828125 segundos\n",
      "beta demorou 0.00020360946655273438 segundos\n",
      "beta demorou 0.00020599365234375 segundos\n",
      "beta demorou 0.00020265579223632812 segundos\n",
      "beta demorou 0.0002028942108154297 segundos\n",
      "beta demorou 0.00020170211791992188 segundos\n",
      "beta demorou 0.00020122528076171875 segundos\n",
      "beta demorou 0.00020241737365722656 segundos\n",
      "beta demorou 0.0002033710479736328 segundos\n",
      "beta demorou 0.00020170211791992188 segundos\n",
      "beta demorou 0.000202178955078125 segundos\n",
      "beta demorou 0.00020503997802734375 segundos\n",
      "beta demorou 0.0002071857452392578 segundos\n",
      "beta demorou 0.00021147727966308594 segundos\n",
      "beta demorou 0.00020194053649902344 segundos\n",
      "beta demorou 0.00020241737365722656 segundos\n",
      "beta demorou 0.00020194053649902344 segundos\n",
      "beta demorou 0.0002014636993408203 segundos\n",
      "beta demorou 0.00020170211791992188 segundos\n",
      "beta demorou 0.0002009868621826172 segundos\n",
      "beta demorou 0.000202178955078125 segundos\n",
      "beta demorou 0.0002052783966064453 segundos\n",
      "beta demorou 0.00020194053649902344 segundos\n",
      "beta demorou 0.00020265579223632812 segundos\n",
      "beta demorou 0.00020194053649902344 segundos\n",
      "beta demorou 0.00020456314086914062 segundos\n",
      "beta demorou 0.00020265579223632812 segundos\n",
      "beta demorou 0.00020122528076171875 segundos\n",
      "beta demorou 0.000202178955078125 segundos\n",
      "beta demorou 0.00020194053649902344 segundos\n",
      "beta demorou 0.00020265579223632812 segundos\n",
      "beta demorou 0.00021195411682128906 segundos\n",
      "beta demorou 0.0002033710479736328 segundos\n",
      "beta demorou 0.00020575523376464844 segundos\n",
      "beta demorou 0.0002014636993408203 segundos\n",
      "beta demorou 0.00020170211791992188 segundos\n",
      "beta demorou 0.00020170211791992188 segundos\n",
      "beta demorou 0.0002033710479736328 segundos\n",
      "beta demorou 0.00020074844360351562 segundos\n",
      "beta demorou 0.0002014636993408203 segundos\n",
      "beta demorou 0.00020241737365722656 segundos\n",
      "beta demorou 0.000202178955078125 segundos\n",
      "beta demorou 0.00020265579223632812 segundos\n",
      "beta demorou 0.0002028942108154297 segundos\n",
      "beta demorou 0.00020051002502441406 segundos\n",
      "beta demorou 0.000202178955078125 segundos\n",
      "beta demorou 0.00020074844360351562 segundos\n",
      "beta demorou 0.00020051002502441406 segundos\n",
      "beta demorou 0.0002009868621826172 segundos\n",
      "beta demorou 0.00025272369384765625 segundos\n",
      "beta demorou 0.00026106834411621094 segundos\n",
      "beta demorou 0.00020503997802734375 segundos\n",
      "beta demorou 0.00020122528076171875 segundos\n",
      "beta demorou 0.000202178955078125 segundos\n",
      "beta demorou 0.000202178955078125 segundos\n",
      "beta demorou 0.00020170211791992188 segundos\n",
      "beta demorou 0.00020122528076171875 segundos\n",
      "beta demorou 0.00020194053649902344 segundos\n",
      "beta demorou 0.0002009868621826172 segundos\n",
      "beta demorou 0.00020313262939453125 segundos\n",
      "beta demorou 0.0002014636993408203 segundos\n",
      "beta demorou 0.0002009868621826172 segundos\n",
      "beta demorou 0.00020122528076171875 segundos\n",
      "beta demorou 0.000202178955078125 segundos\n",
      "beta demorou 0.00020313262939453125 segundos\n",
      "beta demorou 0.00020241737365722656 segundos\n",
      "beta demorou 0.0002014636993408203 segundos\n",
      "beta demorou 0.0002129077911376953 segundos\n",
      "beta demorou 0.000202178955078125 segundos\n",
      "beta demorou 0.00020194053649902344 segundos\n",
      "beta demorou 0.00020051002502441406 segundos\n",
      "beta demorou 0.00020194053649902344 segundos\n",
      "beta demorou 0.00020170211791992188 segundos\n",
      "beta demorou 0.00020122528076171875 segundos\n",
      "beta demorou 0.000202178955078125 segundos\n",
      "beta demorou 0.0002014636993408203 segundos\n",
      "beta demorou 0.000202178955078125 segundos\n",
      "beta demorou 0.00020074844360351562 segundos\n",
      "beta demorou 0.00020313262939453125 segundos\n",
      "beta demorou 0.00020360946655273438 segundos\n",
      "beta demorou 0.0002014636993408203 segundos\n",
      "beta demorou 0.00020170211791992188 segundos\n",
      "parallel_fit demorou 0.15364575386047363 segundos\n",
      "predict_expectations demorou 0.034491539001464844 segundos\n",
      "Exclude mask demorou 0.015543937683105469 segundos\n",
      "Ordenação top-K demorou 0.0329594612121582 segundos\n",
      "recs iguais ?\n",
      "True\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "Time taken by 100 users, 100 items and 1k interactions: 0.76 seconds (0.09x mais rápido)\n",
      "Memory used by 100 users, 100 items and 1k interactions: 258.01 MB (0.01x menos memória)\n"
     ]
    }
   ],
   "source": [
    "ITEMS_PER_BATCH = 1\n",
    "start_memory_usage = get_memory_usage()\n",
    "algo1_optimized = LinOptimized(train_100_100_1k[src.COLUMN_USER_ID].nunique(), train_100_100_1k[src.COLUMN_ITEM_ID].nunique(), num_features=algo1_contexts.shape[1])\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "algo1_optimized.train(train_100_100_1k, contexts=algo1_contexts)\n",
    "memory_used_algo1_optimized = get_memory_usage() - start_memory_usage\n",
    "results_algo1_optimized = algo1_optimized.recommend(users_ids=test_100_100_1k[src.COLUMN_USER_ID], contexts=algo1_contexts_rec)\n",
    "\n",
    "total_time_algo1_optimized = time.time() - start_time\n",
    "\n",
    "print('recs iguais ?')\n",
    "print(np.array_equal(results_algo1[0], results_algo1_optimized[0]))\n",
    "print(np.allclose(results_algo1[1], results_algo1_optimized[1]))\n",
    "print('\\n\\n\\n-----------------------------------\\n')\n",
    "\n",
    "print(f\"Time taken by 100 users, 100 items and 1k interactions: {total_time_algo1_optimized:.2f} seconds ({total_time_algo1 / total_time_algo1_optimized:.2f}x mais rápido)\")\n",
    "print(f\"Memory used by 100 users, 100 items and 1k interactions: {memory_used_algo1_optimized:.2f} MB ({memory_used_algo1 / (memory_used_algo1_optimized + 0.00001):.2f}x menos memória)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea7e67e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.022382020950317383 segundos\n",
      "passar para cuda demorou 0.0006356239318847656 segundos\n",
      "A add demorou 0.0002415180206298828 segundos\n",
      "Xty demorou 0.00012087821960449219 segundos\n",
      "beta demorou 0.0010783672332763672 segundos\n",
      "parallel_fit demorou 0.002260446548461914 segundos\n",
      "predict_expectations demorou 0.0007431507110595703 segundos\n",
      "Exclude mask demorou 0.0003573894500732422 segundos\n",
      "Ordenação top-K demorou 0.0013833045959472656 segundos\n",
      "recs iguais ?\n",
      "True\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Time taken by 1k users, 1k items and 10k interactions: 0.04 seconds (17.19x mais rápido)\n",
      "Memory used by 1k users, 1k items and 10k interactions: 0.00 MB (388281.25x menos memória)\n"
     ]
    }
   ],
   "source": [
    "ITEMS_PER_BATCH = 1_000\n",
    "start_memory_usage = get_memory_usage()\n",
    "algo2_optimized = LinOptimized(train_1k_1k_10k[src.COLUMN_USER_ID].nunique(), train_1k_1k_10k[src.COLUMN_ITEM_ID].nunique(), num_features=algo2_contexts.shape[1])\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "algo2_optimized.train(train_1k_1k_10k, contexts=algo2_contexts)\n",
    "memory_used_algo2_optimized = get_memory_usage() - start_memory_usage\n",
    "results_algo2_optimized = algo2_optimized.recommend(users_ids=test_1k_1k_10k[src.COLUMN_USER_ID], contexts=algo2_contexts_rec)\n",
    "\n",
    "total_time_algo2_optimized = time.time() - start_time\n",
    "\n",
    "print('recs iguais ?')\n",
    "print(np.array_equal(results_algo2[0], results_algo2_optimized[0]))\n",
    "print(np.allclose(results_algo2[1], results_algo2_optimized[1]))\n",
    "\n",
    "print('\\n\\n\\n-----------------------------------------------------------\\n')\n",
    "print(f\"Time taken by 1k users, 1k items and 10k interactions: {total_time_algo2_optimized:.2f} seconds ({total_time_algo2 / total_time_algo2_optimized:.2f}x mais rápido)\")\n",
    "print(f\"Memory used by 1k users, 1k items and 10k interactions: {memory_used_algo2_optimized:.2f} MB ({memory_used_algo2 / (memory_used_algo2_optimized + 0.00001):.2f}x menos memória)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae17dc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.0004601478576660156 segundos\n",
      "passar para cuda demorou 0.0010037422180175781 segundos\n",
      "A add demorou 0.00018596649169921875 segundos\n",
      "Xty demorou 7.700920104980469e-05 segundos\n",
      "beta demorou 0.0019032955169677734 segundos\n",
      "parallel_fit demorou 0.0032927989959716797 segundos\n",
      "predict_expectations demorou 0.0013937950134277344 segundos\n",
      "Exclude mask demorou 0.03150129318237305 segundos\n",
      "Ordenação top-K demorou 0.046089887619018555 segundos\n",
      "recs iguais ?\n",
      "True\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Time taken by 10k users, 10k items and 100k interactions: 0.36 seconds (67.45x mais rápido)\n",
      "Memory used by 10k users, 10k items and 100k interactions: -0.75 MB (-30.40x menos memória)\n"
     ]
    }
   ],
   "source": [
    "ITEMS_PER_BATCH = 10_000\n",
    "start_memory_usage = get_memory_usage()\n",
    "algo3_optimized = LinOptimized(train_10k_10k_100k[src.COLUMN_USER_ID].nunique(), train_10k_10k_100k[src.COLUMN_ITEM_ID].nunique(), algo3_contexts.shape[1])\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "algo3_optimized.train(train_10k_10k_100k, contexts=algo3_contexts)\n",
    "memory_used_algo3_optimized = get_memory_usage() - start_memory_usage\n",
    "results_algo3_optimized = algo3_optimized.recommend(users_ids=test_10k_10k_100k[src.COLUMN_USER_ID], contexts=algo3_contexts_rec)\n",
    "\n",
    "total_time_algo3_optimized = time.time() - start_time\n",
    "\n",
    "print('recs iguais ?')\n",
    "print(np.array_equal(results_algo3[0], results_algo3_optimized[0]))\n",
    "print(np.allclose(results_algo3[1], results_algo3_optimized[1]))\n",
    "\n",
    "print('\\n\\n\\n-----------------------------------------------------------\\n')\n",
    "print(f\"Time taken by 10k users, 10k items and 100k interactions: {total_time_algo3_optimized:.2f} seconds ({total_time_algo3 / total_time_algo3_optimized:.2f}x mais rápido)\")\n",
    "print(f\"Memory used by 10k users, 10k items and 100k interactions: {memory_used_algo3_optimized:.2f} MB ({memory_used_algo3 / (memory_used_algo3_optimized + 0.0001):.2f}x menos memória)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d64823d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel_fit demorou 0.11179614067077637 segundos\n",
      "predict_expectations demorou 0.006095409393310547 segundos\n",
      "Exclude mask demorou 0.15730619430541992 segundos\n",
      "Ordenação top-K demorou 0.24439501762390137 segundos\n",
      "0.4246556758880615\n",
      "recs iguais ?\n",
      "True\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Time taken by 20k users, 20k items and 250k interactions: 0.59 seconds (190.22x mais rápido)\n",
      "Memory used by 20k users, 20k items and 250k interactions: 0.87 MB (47.00x menos memória)\n"
     ]
    }
   ],
   "source": [
    "start_memory_usage = get_memory_usage()\n",
    "algo4_optimized = LinOptimized(train_20k_20k_250k[src.COLUMN_ITEM_ID].nunique(), algo4_contexts.shape[1])\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "algo4_optimized.train(train_20k_20k_250k, contexts=algo4_contexts)\n",
    "memory_used_algo4_optimized = get_memory_usage() - start_memory_usage\n",
    "start_time_2 = time.time()\n",
    "results_algo4_optimized = algo4_optimized.recommend(users_ids=test_20k_20k_250k[src.COLUMN_USER_ID], contexts=algo4_contexts_rec)\n",
    "print(time.time() - start_time_2)\n",
    "\n",
    "total_time_algo4_optimized = time.time() - start_time\n",
    "\n",
    "print('recs iguais ?')\n",
    "print(np.array_equal(results_algo4[0], results_algo4_optimized[0]))\n",
    "print(np.allclose(results_algo4[1], results_algo4_optimized[1]))\n",
    "\n",
    "print('\\n\\n\\n-----------------------------------------------------------\\n')\n",
    "print(f\"Time taken by 20k users, 20k items and 250k interactions: {total_time_algo4_optimized:.2f} seconds ({total_time_algo4 / total_time_algo4_optimized:.2f}x mais rápido)\")\n",
    "print(f\"Memory used by 20k users, 20k items and 250k interactions: {memory_used_algo4_optimized:.2f} MB ({memory_used_algo4 / (memory_used_algo4_optimized + 0.0001):.2f}x menos memória)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0a9a9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 18.531693935394287 segundos\n",
      "reset_arm_to_status demorou 3.814697265625e-06 segundos\n",
      "parallel_fit demorou 99.5164840221405 segundos\n",
      "Gerar as predições demorou 75.51276659965515 segundos\n",
      "predict_expectations demorou 75.55166602134705 segundos\n",
      "Ordenação top-K demorou 32.233283281326294 segundos\n",
      "gerar lista de recomendações demorou 0.9358096122741699 segundos\n",
      "Gerar as predições demorou 87.14873933792114 segundos\n",
      "predict_expectations demorou 87.16777181625366 segundos\n",
      "Ordenação top-K demorou 59.73408222198486 segundos\n",
      "gerar lista de recomendações demorou 0.7291140556335449 segundos\n",
      "Gerar as predições demorou 30.094655752182007 segundos\n",
      "predict_expectations demorou 30.234052181243896 segundos\n",
      "Ordenação top-K demorou 12.389704942703247 segundos\n",
      "gerar lista de recomendações demorou 0.6723012924194336 segundos\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Time taken by 10k users, 50k items and 250k interactions: 423.04 seconds\n",
      "Memory used by 10k users, 50k items and 250k interactions: 12722.73 MB\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 50_000\n",
    "start_memory_usage = get_memory_usage()\n",
    "algo5 = Lin()\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "algo5.train(train_10k_50k_250k, contexts=algo5_contexts)\n",
    "memory_used_algo5 = get_memory_usage() - start_memory_usage\n",
    "\n",
    "recommendations_ids5 = np.empty((test_10k_50k_250k.shape[0], src.TOP_N), dtype=int)\n",
    "recommendations_scores5 = np.empty((test_10k_50k_250k.shape[0], src.TOP_N), dtype=float)\n",
    "\n",
    "for j in range(0, len(test_10k_50k_250k), BATCH_SIZE):\n",
    "    batch_df = test_10k_50k_250k.iloc[j:j + BATCH_SIZE]\n",
    "    batch_contexts = algo5_contexts_rec[j:j + BATCH_SIZE]\n",
    "    results_algo5 = algo5.recommend(users_ids=batch_df[src.COLUMN_USER_ID], contexts=batch_contexts)\n",
    "    recommendations_ids5[j:j + BATCH_SIZE] = results_algo5[0]\n",
    "    recommendations_scores5[j:j + BATCH_SIZE] = results_algo5[1]\n",
    "\n",
    "total_time_algo5 = time.time() - start_time\n",
    "\n",
    "print('\\n\\n\\n-----------------------------------------------------------\\n')\n",
    "print(f\"Time taken by 10k users, 50k items and 250k interactions: {total_time_algo5:.2f} seconds\")\n",
    "print(f\"Memory used by 10k users, 50k items and 250k interactions: {memory_used_algo5:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "142e6659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.021104812622070312 segundos\n",
      "passar para cuda demorou 0.04938936233520508 segundos\n",
      "A add demorou 0.0002582073211669922 segundos\n",
      "Xty demorou 8.535385131835938e-05 segundos\n",
      "beta demorou 0.2589085102081299 segundos\n",
      "beta demorou 0.2113180160522461 segundos\n",
      "parallel_fit demorou 0.5202620029449463 segundos\n",
      "passar para cuda demorou 0.00793910026550293 segundos\n",
      "A add demorou 0.0002491474151611328 segundos\n",
      "Xty demorou 8.177757263183594e-05 segundos\n",
      "beta demorou 0.2349529266357422 segundos\n",
      "beta demorou 0.20102715492248535 segundos\n",
      "parallel_fit demorou 0.4446594715118408 segundos\n",
      "passar para cuda demorou 0.0037581920623779297 segundos\n",
      "A add demorou 0.00017595291137695312 segundos\n",
      "Xty demorou 6.151199340820312e-05 segundos\n",
      "beta demorou 0.21686267852783203 segundos\n",
      "beta demorou 0.20102429389953613 segundos\n",
      "parallel_fit demorou 0.42223429679870605 segundos\n",
      "predict_expectations demorou 0.004850625991821289 segundos\n",
      "Exclude mask demorou 0.6706581115722656 segundos\n",
      "Ordenação top-K demorou 0.12278127670288086 segundos\n",
      "predict_expectations demorou 0.010348081588745117 segundos\n",
      "Exclude mask demorou 0.6702821254730225 segundos\n",
      "Ordenação top-K demorou 0.1216742992401123 segundos\n",
      "predict_expectations demorou 0.004862546920776367 segundos\n",
      "Exclude mask demorou 0.6756558418273926 segundos\n",
      "Ordenação top-K demorou 0.1423811912536621 segundos\n",
      "predict_expectations demorou 0.007534980773925781 segundos\n",
      "Exclude mask demorou 0.6761658191680908 segundos\n",
      "Ordenação top-K demorou 0.1208353042602539 segundos\n",
      "predict_expectations demorou 0.005903482437133789 segundos\n",
      "Exclude mask demorou 0.31566882133483887 segundos\n",
      "Ordenação top-K demorou 0.05610322952270508 segundos\n",
      "recs iguais ?\n",
      "True\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Time taken by 10k users, 50k items and 250k interactions: 5.53 seconds (76.56x mais rápido)\n",
      "Memory used by 10k users, 50k items and 250k interactions: -1.84 MB\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 50_000\n",
    "RECOMMEND_BATCH_SIZE = 28_000\n",
    "ITEMS_PER_BATCH = 25_000\n",
    "start_memory_usage = get_memory_usage()\n",
    "algo5_optimized = LinOptimized(train_10k_50k_250k[src.COLUMN_USER_ID].nunique(), train_10k_50k_250k[src.COLUMN_ITEM_ID].nunique(), algo5_contexts.shape[1])\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "for j in range(0, len(train_10k_50k_250k), TRAIN_BATCH_SIZE):\n",
    "    batch_df = train_10k_50k_250k.iloc[j:j + TRAIN_BATCH_SIZE]\n",
    "    batch_contexts = algo5_contexts[j:j + TRAIN_BATCH_SIZE]\n",
    "    algo5_optimized.train(batch_df, contexts=batch_contexts)\n",
    "#algo5_optimized.train(train_10k_50k_250k, contexts=algo5_contexts)\n",
    "memory_used_algo5_optimized = get_memory_usage() - start_memory_usage\n",
    "\n",
    "recommendations_ids5_optimized = np.empty((test_10k_50k_250k.shape[0], src.TOP_N), dtype=int)\n",
    "recommendations_scores5_optimized = np.empty((test_10k_50k_250k.shape[0], src.TOP_N), dtype=float)\n",
    "\n",
    "for j in range(0, len(test_10k_50k_250k), RECOMMEND_BATCH_SIZE):\n",
    "    batch_df = test_10k_50k_250k.iloc[j:j + RECOMMEND_BATCH_SIZE]\n",
    "    batch_contexts = algo5_contexts_rec[j:j + RECOMMEND_BATCH_SIZE]\n",
    "    results_algo5_optimized = algo5_optimized.recommend(users_ids=batch_df[src.COLUMN_USER_ID], contexts=batch_contexts)\n",
    "    recommendations_ids5_optimized[j:j + RECOMMEND_BATCH_SIZE] = results_algo5_optimized[0]\n",
    "    recommendations_scores5_optimized[j:j + RECOMMEND_BATCH_SIZE] = results_algo5_optimized[1]\n",
    "\n",
    "total_time_algo5_optimized = time.time() - start_time\n",
    "\n",
    "print('recs iguais ?')\n",
    "print(np.array_equal(recommendations_ids5, recommendations_ids5_optimized))\n",
    "print(np.allclose(recommendations_scores5, recommendations_scores5_optimized))\n",
    "\n",
    "print('\\n\\n\\n-----------------------------------------------------------\\n')\n",
    "print(f\"Time taken by 10k users, 50k items and 250k interactions: {total_time_algo5_optimized:.2f} seconds ({total_time_algo5 / total_time_algo5_optimized:.2f}x mais rápido)\")\n",
    "print(f\"Memory used by 10k users, 50k items and 250k interactions: {memory_used_algo5_optimized:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f2ec5e",
   "metadata": {},
   "source": [
    "## Testes LinGreedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2166a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.0028831958770751953 segundos\n",
      "reset_arm_to_status demorou 1.430511474609375e-06 segundos\n",
      "parallel_fit demorou 0.05723381042480469 segundos\n",
      "Gerar as predições demorou 0.0014731884002685547 segundos\n",
      "predict_expectations demorou 0.0017085075378417969 segundos\n",
      "Ordenação top-K demorou 0.008001565933227539 segundos\n",
      "gerar lista de recomendações demorou 0.014766693115234375 segundos\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Time taken by 100 users, 100 items and 1k interactions: 0.36 seconds\n",
      "Memory used by 100 users, 100 items and 1k interactions: 2.14 MB\n"
     ]
    }
   ],
   "source": [
    "start_memory_usage = get_memory_usage()\n",
    "algo1 = LinGreedy(hyperparameters={'epsilon': 0.01})\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "algo1.train(train_100_100_1k, contexts=algo1_contexts)\n",
    "memory_used_algo1 = get_memory_usage() - start_memory_usage\n",
    "results_algo1 = algo1.recommend(users_ids=test_100_100_1k[src.COLUMN_USER_ID], contexts=algo1_contexts_rec)\n",
    "\n",
    "total_time_algo1 = time.time() - start_time\n",
    "\n",
    "print('\\n\\n\\n-----------------------------------------------------------\\n')\n",
    "print(f\"Time taken by 100 users, 100 items and 1k interactions: {total_time_algo1:.2f} seconds\")\n",
    "print(f\"Memory used by 100 users, 100 items and 1k interactions: {memory_used_algo1:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3e107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_memory_usage = get_memory_usage()\n",
    "algo1_optimized = LinGreedyOptimized(hyperparameters={'epsilon': 0.01})\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "algo1_optimized.train(train_100_100_1k, contexts=algo1_contexts)\n",
    "memory_used_algo1_optimized = get_memory_usage() - start_memory_usage\n",
    "results_algo1_optimized = algo1_optimized.recommend(users_ids=test_100_100_1k[src.COLUMN_USER_ID], contexts=algo1_contexts_rec)\n",
    "\n",
    "total_time_algo1_optimized = time.time() - start_time\n",
    "\n",
    "print('recs iguais ?')\n",
    "print(np.array_equal(results_algo1[0], results_algo1_optimized[0]))\n",
    "print(np.allclose(results_algo1[1], results_algo1_optimized[1]))\n",
    "print('\\n\\n\\n-----------------------------------\\n')\n",
    "\n",
    "print(f\"Time taken by 100 users, 100 items and 1k interactions: {total_time_algo1_optimized:.2f} seconds ({total_time_algo1 / total_time_algo1_optimized:.2f}x mais rápido)\")\n",
    "print(f\"Memory used by 100 users, 100 items and 1k interactions: {memory_used_algo1_optimized:.2f} MB ({memory_used_algo1 / (memory_used_algo1_optimized + 0.00001):.2f}x menos memória)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc33887",
   "metadata": {},
   "source": [
    "## Testes LinUcb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "70906833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.0022423267364501953 segundos\n",
      "reset_arm_to_status demorou 1.1920928955078125e-06 segundos\n",
      "parallel_fit demorou 0.047121286392211914 segundos\n",
      "Gerar as predições demorou 0.0074520111083984375 segundos\n",
      "predict_expectations demorou 0.007620334625244141 segundos\n",
      "Ordenação top-K demorou 0.0013415813446044922 segundos\n",
      "gerar lista de recomendações demorou 0.008218765258789062 segundos\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Time taken by 100 users, 100 items and 1k interactions: 0.09 seconds\n",
      "Memory used by 100 users, 100 items and 1k interactions: 0.25 MB\n"
     ]
    }
   ],
   "source": [
    "start_memory_usage = get_memory_usage()\n",
    "algo1 = LinUCB(hyperparameters={})\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "algo1.train(train_100_100_1k, contexts=algo1_contexts)\n",
    "memory_used_algo1 = get_memory_usage() - start_memory_usage\n",
    "results_algo1 = algo1.recommend(users_ids=test_100_100_1k[src.COLUMN_USER_ID], contexts=algo1_contexts_rec)\n",
    "\n",
    "total_time_algo1 = time.time() - start_time\n",
    "\n",
    "print('\\n\\n\\n-----------------------------------------------------------\\n')\n",
    "print(f\"Time taken by 100 users, 100 items and 1k interactions: {total_time_algo1:.2f} seconds\")\n",
    "print(f\"Memory used by 100 users, 100 items and 1k interactions: {memory_used_algo1:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "efe07ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.023614168167114258 segundos\n",
      "passar para cuda demorou 0.0005590915679931641 segundos\n",
      "A add demorou 0.00029540061950683594 segundos\n",
      "Xty demorou 0.0001354217529296875 segundos\n",
      "beta demorou 0.020980119705200195 segundos\n",
      "beta demorou 0.0005040168762207031 segundos\n",
      "beta demorou 0.00045561790466308594 segundos\n",
      "beta demorou 0.0004436969757080078 segundos\n",
      "beta demorou 0.0004417896270751953 segundos\n",
      "beta demorou 0.0004360675811767578 segundos\n",
      "beta demorou 0.0004553794860839844 segundos\n",
      "beta demorou 0.0004355907440185547 segundos\n",
      "beta demorou 0.00041294097900390625 segundos\n",
      "beta demorou 0.00040984153747558594 segundos\n",
      "beta demorou 0.00041031837463378906 segundos\n",
      "beta demorou 0.0004119873046875 segundos\n",
      "beta demorou 0.0004189014434814453 segundos\n",
      "beta demorou 0.00040078163146972656 segundos\n",
      "beta demorou 0.00040268898010253906 segundos\n",
      "beta demorou 0.00043082237243652344 segundos\n",
      "beta demorou 0.00038170814514160156 segundos\n",
      "beta demorou 0.000385284423828125 segundos\n",
      "beta demorou 0.0003838539123535156 segundos\n",
      "beta demorou 0.0003845691680908203 segundos\n",
      "beta demorou 0.00038313865661621094 segundos\n",
      "beta demorou 0.0003821849822998047 segundos\n",
      "beta demorou 0.0003833770751953125 segundos\n",
      "beta demorou 0.0003826618194580078 segundos\n",
      "beta demorou 0.0004000663757324219 segundos\n",
      "beta demorou 0.0003848075866699219 segundos\n",
      "beta demorou 0.00037169456481933594 segundos\n",
      "beta demorou 0.0003674030303955078 segundos\n",
      "beta demorou 0.0003662109375 segundos\n",
      "beta demorou 0.00036525726318359375 segundos\n",
      "beta demorou 0.00036716461181640625 segundos\n",
      "beta demorou 0.0003688335418701172 segundos\n",
      "beta demorou 0.00036716461181640625 segundos\n",
      "beta demorou 0.00036787986755371094 segundos\n",
      "beta demorou 0.00038909912109375 segundos\n",
      "beta demorou 0.00038242340087890625 segundos\n",
      "beta demorou 0.00034618377685546875 segundos\n",
      "beta demorou 0.00034546852111816406 segundos\n",
      "beta demorou 0.0003457069396972656 segundos\n",
      "beta demorou 0.0003464221954345703 segundos\n",
      "beta demorou 0.0003437995910644531 segundos\n",
      "beta demorou 0.0003445148468017578 segundos\n",
      "beta demorou 0.0003478527069091797 segundos\n",
      "beta demorou 0.00034499168395996094 segundos\n",
      "beta demorou 0.000347137451171875 segundos\n",
      "beta demorou 0.00037169456481933594 segundos\n",
      "beta demorou 0.0003349781036376953 segundos\n",
      "beta demorou 0.00033092498779296875 segundos\n",
      "beta demorou 0.0003314018249511719 segundos\n",
      "beta demorou 0.00034499168395996094 segundos\n",
      "beta demorou 0.0003368854522705078 segundos\n",
      "beta demorou 0.00033473968505859375 segundos\n",
      "beta demorou 0.0003323554992675781 segundos\n",
      "beta demorou 0.0003325939178466797 segundos\n",
      "beta demorou 0.000331878662109375 segundos\n",
      "beta demorou 0.0003459453582763672 segundos\n",
      "beta demorou 0.0003268718719482422 segundos\n",
      "beta demorou 0.0003237724304199219 segundos\n",
      "beta demorou 0.0003235340118408203 segundos\n",
      "beta demorou 0.000324249267578125 segundos\n",
      "beta demorou 0.000324249267578125 segundos\n",
      "beta demorou 0.00032448768615722656 segundos\n",
      "beta demorou 0.00032138824462890625 segundos\n",
      "beta demorou 0.0003368854522705078 segundos\n",
      "beta demorou 0.00031375885009765625 segundos\n",
      "beta demorou 0.00031638145446777344 segundos\n",
      "beta demorou 0.0003209114074707031 segundos\n",
      "beta demorou 0.00033283233642578125 segundos\n",
      "beta demorou 0.0003178119659423828 segundos\n",
      "beta demorou 0.00030517578125 segundos\n",
      "beta demorou 0.00030684471130371094 segundos\n",
      "beta demorou 0.0003058910369873047 segundos\n",
      "beta demorou 0.00030517578125 segundos\n",
      "beta demorou 0.0003075599670410156 segundos\n",
      "beta demorou 0.00030541419982910156 segundos\n",
      "beta demorou 0.0003066062927246094 segundos\n",
      "beta demorou 0.00030612945556640625 segundos\n",
      "beta demorou 0.00030684471130371094 segundos\n",
      "beta demorou 0.0003058910369873047 segundos\n",
      "beta demorou 0.0003249645233154297 segundos\n",
      "beta demorou 0.0002963542938232422 segundos\n",
      "beta demorou 0.00029659271240234375 segundos\n",
      "beta demorou 0.0002968311309814453 segundos\n",
      "beta demorou 0.0002980232238769531 segundos\n",
      "beta demorou 0.0002970695495605469 segundos\n",
      "beta demorou 0.00029754638671875 segundos\n",
      "beta demorou 0.0002961158752441406 segundos\n",
      "beta demorou 0.00029397010803222656 segundos\n",
      "beta demorou 0.00029754638671875 segundos\n",
      "beta demorou 0.0002963542938232422 segundos\n",
      "beta demorou 0.0002951622009277344 segundos\n",
      "beta demorou 0.00031685829162597656 segundos\n",
      "beta demorou 0.00031304359436035156 segundos\n",
      "beta demorou 0.00029015541076660156 segundos\n",
      "beta demorou 0.0002892017364501953 segundos\n",
      "beta demorou 0.0002894401550292969 segundos\n",
      "beta demorou 0.0003008842468261719 segundos\n",
      "beta demorou 0.0002913475036621094 segundos\n",
      "beta demorou 0.0002913475036621094 segundos\n",
      "beta demorou 0.00029158592224121094 segundos\n",
      "parallel_fit demorou 0.05961751937866211 segundos\n",
      "predict_expectations demorou 0.08847284317016602 segundos\n",
      "Exclude mask demorou 0.0001506805419921875 segundos\n",
      "Ordenação top-K demorou 0.00033020973205566406 segundos\n",
      "recs iguais ?\n",
      "True\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "Time taken by 100 users, 100 items and 1k interactions: 0.18 seconds (0.52x mais rápido)\n",
      "Memory used by 100 users, 100 items and 1k interactions: 8.03 MB (0.03x menos memória)\n"
     ]
    }
   ],
   "source": [
    "ITEMS_PER_BATCH = 1\n",
    "start_memory_usage = get_memory_usage()\n",
    "algo1_optimized = LinUCBOptimized(train_100_100_1k[src.COLUMN_USER_ID].nunique(), train_100_100_1k[src.COLUMN_ITEM_ID].nunique(), num_features=algo1_contexts.shape[1])\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "algo1_optimized.train(train_100_100_1k, contexts=algo1_contexts)\n",
    "memory_used_algo1_optimized = get_memory_usage() - start_memory_usage\n",
    "results_algo1_optimized = algo1_optimized.recommend(users_ids=test_100_100_1k[src.COLUMN_USER_ID], contexts=algo1_contexts_rec)\n",
    "\n",
    "total_time_algo1_optimized = time.time() - start_time\n",
    "\n",
    "print('recs iguais ?')\n",
    "print(np.array_equal(results_algo1[0], results_algo1_optimized[0]))\n",
    "print(np.allclose(results_algo1[1], results_algo1_optimized[1]))\n",
    "print('\\n\\n\\n-----------------------------------\\n')\n",
    "\n",
    "print(f\"Time taken by 100 users, 100 items and 1k interactions: {total_time_algo1_optimized:.2f} seconds ({total_time_algo1 / total_time_algo1_optimized:.2f}x mais rápido)\")\n",
    "print(f\"Memory used by 100 users, 100 items and 1k interactions: {memory_used_algo1_optimized:.2f} MB ({memory_used_algo1 / (memory_used_algo1_optimized + 0.00001):.2f}x menos memória)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df62ed99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 12.653273582458496 segundos\n",
      "reset_arm_to_status demorou 4.0531158447265625e-06 segundos\n",
      "parallel_fit demorou 109.72455358505249 segundos\n",
      "Gerar as predições demorou 83.270334482193 segundos\n",
      "predict_expectations demorou 83.27344107627869 segundos\n",
      "Ordenação top-K demorou 0.358318567276001 segundos\n",
      "gerar lista de recomendações demorou 0.01276850700378418 segundos\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Time taken by 10k users, 50k items and 250k interactions: 206.89 seconds\n",
      "Memory used by 10k users, 50k items and 250k interactions: 12720.37 MB\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1_000\n",
    "start_memory_usage = get_memory_usage()\n",
    "algo5 = LinUCB()\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "algo5.train(train_10k_50k_250k, contexts=algo5_contexts)\n",
    "memory_used_algo5 = get_memory_usage() - start_memory_usage\n",
    "\n",
    "recommendations_ids5 = np.zeros((test_10k_50k_250k.shape[0], src.TOP_N), dtype=int)\n",
    "recommendations_scores5 = np.zeros((test_10k_50k_250k.shape[0], src.TOP_N), dtype=float)\n",
    "\n",
    "for j in range(0, len(test_10k_50k_250k), BATCH_SIZE):\n",
    "    batch_df = test_10k_50k_250k.iloc[j:j + BATCH_SIZE]\n",
    "    batch_contexts = algo5_contexts_rec[j:j + BATCH_SIZE]\n",
    "    results_algo5 = algo5.recommend(users_ids=batch_df[src.COLUMN_USER_ID], contexts=batch_contexts)\n",
    "    recommendations_ids5[j:j + BATCH_SIZE] = results_algo5[0]\n",
    "    recommendations_scores5[j:j + BATCH_SIZE] = results_algo5[1]\n",
    "    break\n",
    "\n",
    "total_time_algo5 = time.time() - start_time\n",
    "\n",
    "print('\\n\\n\\n-----------------------------------------------------------\\n')\n",
    "print(f\"Time taken by 10k users, 50k items and 250k interactions: {total_time_algo5:.2f} seconds\")\n",
    "print(f\"Memory used by 10k users, 50k items and 250k interactions: {memory_used_algo5:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2f8b0cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.43347907066345215 segundos\n",
      "passar para cuda demorou 0.019133329391479492 segundos\n",
      "A add demorou 0.03985595703125 segundos\n",
      "Xty demorou 0.0002586841583251953 segundos\n",
      "beta demorou 0.11709332466125488 segundos\n",
      "beta demorou 0.03383684158325195 segundos\n",
      "beta demorou 0.03361153602600098 segundos\n",
      "beta demorou 0.0335845947265625 segundos\n",
      "beta demorou 0.032950639724731445 segundos\n",
      "beta demorou 0.030330896377563477 segundos\n",
      "beta demorou 0.03026580810546875 segundos\n",
      "beta demorou 0.03025650978088379 segundos\n",
      "beta demorou 0.030420303344726562 segundos\n",
      "beta demorou 0.030422687530517578 segundos\n",
      "parallel_fit demorou 0.46248936653137207 segundos\n",
      "passar para cuda demorou 0.007070302963256836 segundos\n",
      "A add demorou 0.00021910667419433594 segundos\n",
      "Xty demorou 6.556510925292969e-05 segundos\n",
      "beta demorou 0.06274843215942383 segundos\n",
      "beta demorou 0.030429363250732422 segundos\n",
      "beta demorou 0.030375242233276367 segundos\n",
      "beta demorou 0.03040480613708496 segundos\n",
      "beta demorou 0.030420780181884766 segundos\n",
      "beta demorou 0.030440568923950195 segundos\n",
      "beta demorou 0.03040289878845215 segundos\n",
      "beta demorou 0.030449390411376953 segundos\n",
      "beta demorou 0.030384540557861328 segundos\n",
      "beta demorou 0.030437231063842773 segundos\n",
      "parallel_fit demorou 0.3441944122314453 segundos\n",
      "passar para cuda demorou 0.0039217472076416016 segundos\n",
      "A add demorou 0.00017404556274414062 segundos\n",
      "Xty demorou 6.151199340820312e-05 segundos\n",
      "beta demorou 0.04648995399475098 segundos\n",
      "beta demorou 0.030371427536010742 segundos\n",
      "beta demorou 0.030426740646362305 segundos\n",
      "beta demorou 0.030434846878051758 segundos\n",
      "beta demorou 0.030408859252929688 segundos\n",
      "beta demorou 0.030390501022338867 segundos\n",
      "beta demorou 0.03036808967590332 segundos\n",
      "beta demorou 0.030421972274780273 segundos\n",
      "beta demorou 0.030415058135986328 segundos\n",
      "beta demorou 0.03042435646057129 segundos\n",
      "parallel_fit demorou 0.3246779441833496 segundos\n",
      "predict_expectations demorou 4.098702907562256 segundos\n",
      "Exclude mask demorou 0.3462371826171875 segundos\n",
      "Ordenação top-K demorou 0.032067060470581055 segundos\n",
      "recs iguais ?\n",
      "True\n",
      "True\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Time taken by 10k users, 50k items and 250k interactions: 6.09 seconds (33.96x mais rápido)\n",
      "Memory used by 10k users, 50k items and 250k interactions: 144.48 MB\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 50_000\n",
    "RECOMMEND_BATCH_SIZE = 1_000\n",
    "ITEMS_PER_BATCH = 5_000\n",
    "start_memory_usage = get_memory_usage()\n",
    "algo5_optimized = LinUCBOptimized(train_10k_50k_250k[src.COLUMN_USER_ID].nunique(), train_10k_50k_250k[src.COLUMN_ITEM_ID].nunique(), algo5_contexts.shape[1])\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "for j in range(0, len(train_10k_50k_250k), TRAIN_BATCH_SIZE):\n",
    "    batch_df = train_10k_50k_250k.iloc[j:j + TRAIN_BATCH_SIZE]\n",
    "    batch_contexts = algo5_contexts[j:j + TRAIN_BATCH_SIZE]\n",
    "    algo5_optimized.train(batch_df, contexts=batch_contexts)\n",
    "#algo5_optimized.train(train_10k_50k_250k, contexts=algo5_contexts)\n",
    "memory_used_algo5_optimized = get_memory_usage() - start_memory_usage\n",
    "\n",
    "recommendations_ids5_optimized = np.zeros((test_10k_50k_250k.shape[0], src.TOP_N), dtype=int)\n",
    "recommendations_scores5_optimized = np.zeros((test_10k_50k_250k.shape[0], src.TOP_N), dtype=float)\n",
    "\n",
    "for j in range(0, len(test_10k_50k_250k), RECOMMEND_BATCH_SIZE):\n",
    "    batch_df = test_10k_50k_250k.iloc[j:j + RECOMMEND_BATCH_SIZE]\n",
    "    batch_contexts = algo5_contexts_rec[j:j + RECOMMEND_BATCH_SIZE]\n",
    "    results_algo5_optimized = algo5_optimized.recommend(users_ids=batch_df[src.COLUMN_USER_ID], contexts=batch_contexts)\n",
    "    recommendations_ids5_optimized[j:j + RECOMMEND_BATCH_SIZE] = results_algo5_optimized[0]\n",
    "    recommendations_scores5_optimized[j:j + RECOMMEND_BATCH_SIZE] = results_algo5_optimized[1]\n",
    "    break\n",
    "\n",
    "total_time_algo5_optimized = time.time() - start_time\n",
    "\n",
    "print('recs iguais ?')\n",
    "print(np.array_equal(recommendations_ids5, recommendations_ids5_optimized))\n",
    "print(np.allclose(recommendations_scores5, recommendations_scores5_optimized))\n",
    "\n",
    "print('\\n\\n\\n-----------------------------------------------------------\\n')\n",
    "print(f\"Time taken by 10k users, 50k items and 250k interactions: {total_time_algo5_optimized:.2f} seconds ({total_time_algo5 / total_time_algo5_optimized:.2f}x mais rápido)\")\n",
    "print(f\"Memory used by 10k users, 50k items and 250k interactions: {memory_used_algo5_optimized:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4629341",
   "metadata": {},
   "source": [
    "## Testes LinTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b973241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.37107110023498535 segundos\n",
      "passar para cuda demorou 0.0015044212341308594 segundos\n",
      "A add demorou 0.11273717880249023 segundos\n",
      "Xty demorou 0.00038909912109375 segundos\n",
      "beta demorou 0.08366703987121582 segundos\n",
      "init demorou 0.0004889965057373047 segundos\n",
      "passar para cuda demorou 0.0007333755493164062 segundos\n",
      "A add demorou 0.00021529197692871094 segundos\n",
      "Xty demorou 0.00011515617370605469 segundos\n",
      "beta demorou 0.0006401538848876953 segundos\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3906435/2604159387.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  print(torch.allclose(torch.tensor(scores1, device='cuda'), scores2))\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 50_000\n",
    "RECOMMEND_BATCH_SIZE = 1_000\n",
    "ITEMS_PER_BATCH = 1\n",
    "INTERACTIONS_PER_BATCH_LINTS = 1\n",
    "\n",
    "from mabwiser.linear import _LinTS\n",
    "from mabwiser.utils import create_rng\n",
    "\n",
    "algo1 = _LinTSOptimized(create_rng(src.RANDOM_STATE), 1)\n",
    "algo1.init(10, 1)\n",
    "algo1.fit(train_100_100_1k[src.COLUMN_ITEM_ID][train_100_100_1k[src.COLUMN_ITEM_ID] == 0].values, algo1_contexts[train_100_100_1k[src.COLUMN_ITEM_ID] == 0], train_100_100_1k[src.COLUMN_RATING][train_100_100_1k[src.COLUMN_ITEM_ID] == 0].values)\n",
    "scores1 = algo1.predict(algo1_contexts_rec)\n",
    "\n",
    "algo2 = _LinTSOptimized(create_rng(src.RANDOM_STATE), 1)\n",
    "algo2.init(10, 1)\n",
    "algo2.fit(train_100_100_1k[src.COLUMN_ITEM_ID][train_100_100_1k[src.COLUMN_ITEM_ID] == 0].values, algo1_contexts[train_100_100_1k[src.COLUMN_ITEM_ID] == 0], train_100_100_1k[src.COLUMN_RATING][train_100_100_1k[src.COLUMN_ITEM_ID] == 0].values)\n",
    "scores2 = algo2.predict(algo1_contexts_rec)\n",
    "\n",
    "print(torch.allclose(torch.tensor(scores1, device='cuda'), scores2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d587c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.001753091812133789 segundos\n",
      "reset_arm_to_status demorou 4.76837158203125e-07 segundos\n",
      "parallel_fit demorou 0.04980802536010742 segundos\n",
      "Gerar as predições demorou 0.039191246032714844 segundos\n",
      "predict_expectations demorou 0.03983616828918457 segundos\n",
      "Ordenação top-K demorou 0.0018582344055175781 segundos\n",
      "gerar lista de recomendações demorou 0.016396522521972656 segundos\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------------\n",
      "\n",
      "Time taken by 100 users, 100 items and 1k interactions: 0.19 seconds\n",
      "Memory used by 100 users, 100 items and 1k interactions: 1.69 MB\n"
     ]
    }
   ],
   "source": [
    "start_memory_usage = get_memory_usage()\n",
    "algo1 = LinTS(hyperparameters={'alpha': 0.0005})\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "algo1.train(train_100_100_1k, contexts=algo1_contexts)\n",
    "memory_used_algo1 = get_memory_usage() - start_memory_usage\n",
    "results_algo1 = algo1.recommend(users_ids=test_100_100_1k[src.COLUMN_USER_ID], contexts=algo1_contexts_rec)\n",
    "\n",
    "total_time_algo1 = time.time() - start_time\n",
    "\n",
    "print('\\n\\n\\n-----------------------------------------------------------\\n')\n",
    "print(f\"Time taken by 100 users, 100 items and 1k interactions: {total_time_algo1:.2f} seconds\")\n",
    "print(f\"Memory used by 100 users, 100 items and 1k interactions: {memory_used_algo1:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b5d8b72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.0006394386291503906 segundos\n",
      "passar para cuda demorou 0.0006206035614013672 segundos\n",
      "A add demorou 0.0005955696105957031 segundos\n",
      "Xty demorou 0.0001201629638671875 segundos\n",
      "beta demorou 0.0017261505126953125 segundos\n",
      "beta demorou 0.0003895759582519531 segundos\n",
      "beta demorou 0.00037360191345214844 segundos\n",
      "beta demorou 0.00038743019104003906 segundos\n",
      "beta demorou 0.0003604888916015625 segundos\n",
      "beta demorou 0.00037932395935058594 segundos\n",
      "beta demorou 0.0003757476806640625 segundos\n",
      "beta demorou 0.0004506111145019531 segundos\n",
      "parallel_fit demorou 0.0062296390533447266 segundos\n",
      "predict_expectations demorou 0.008992910385131836 segundos\n",
      "Exclude mask demorou 0.017832040786743164 segundos\n",
      "Ordenação top-K demorou 0.2548861503601074 segundos\n",
      "recs iguais ?\n",
      "False\n",
      "False\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------\n",
      "\n",
      "Time taken by 100 users, 100 items and 1k interactions: 0.29 seconds (0.65x mais rápido)\n",
      "Memory used by 100 users, 100 items and 1k interactions: 1.01 MB (1.67x menos memória)\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 3\n",
    "RECOMMEND_BATCH_SIZE = 3\n",
    "ITEMS_PER_BATCH = 14\n",
    "start_memory_usage = get_memory_usage()\n",
    "algo1_optimized = LinTSOptimized(train_100_100_1k[src.COLUMN_USER_ID].nunique(), train_100_100_1k[src.COLUMN_ITEM_ID].nunique(), algo1_contexts.shape[1], hyperparameters={'alpha': 0.0005})\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "algo1_optimized.train(train_100_100_1k, contexts=algo1_contexts)\n",
    "memory_used_algo1_optimized = get_memory_usage() - start_memory_usage\n",
    "results_algo1_optimized = algo1_optimized.recommend(users_ids=test_100_100_1k[src.COLUMN_USER_ID], contexts=algo1_contexts_rec)\n",
    "\n",
    "total_time_algo1_optimized = time.time() - start_time\n",
    "\n",
    "print('recs iguais ?')\n",
    "print(np.array_equal(results_algo1[0], results_algo1_optimized[0]))\n",
    "print(np.allclose(results_algo1[1], results_algo1_optimized[1]))\n",
    "print('\\n\\n\\n-----------------------------------\\n')\n",
    "\n",
    "print(f\"Time taken by 100 users, 100 items and 1k interactions: {total_time_algo1_optimized:.2f} seconds ({total_time_algo1 / total_time_algo1_optimized:.2f}x mais rápido)\")\n",
    "print(f\"Memory used by 100 users, 100 items and 1k interactions: {memory_used_algo1_optimized:.2f} MB ({memory_used_algo1 / (memory_used_algo1_optimized + 0.00001):.2f}x menos memória)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b77e9d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[26, 59, 54, ..., 27, 24, 78],\n",
       "       [58, 20, 74, ..., 82, 99, 54],\n",
       "       [35, 78, 67, ..., 45,  0, 22],\n",
       "       ...,\n",
       "       [54, 99, 98, ..., 60, 58, 45],\n",
       "       [74, 34, 60, ..., 54,  2, 78],\n",
       "       [18, 78, 35, ..., 30, 45, 27]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_algo1_optimized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d36746e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[26,\n",
       "  59,\n",
       "  54,\n",
       "  7,\n",
       "  99,\n",
       "  85,\n",
       "  25,\n",
       "  34,\n",
       "  87,\n",
       "  60,\n",
       "  52,\n",
       "  16,\n",
       "  18,\n",
       "  20,\n",
       "  77,\n",
       "  89,\n",
       "  30,\n",
       "  27,\n",
       "  24,\n",
       "  78],\n",
       " [58,\n",
       "  20,\n",
       "  74,\n",
       "  60,\n",
       "  18,\n",
       "  59,\n",
       "  35,\n",
       "  17,\n",
       "  50,\n",
       "  2,\n",
       "  23,\n",
       "  98,\n",
       "  13,\n",
       "  21,\n",
       "  37,\n",
       "  27,\n",
       "  31,\n",
       "  82,\n",
       "  99,\n",
       "  54],\n",
       " [35,\n",
       "  78,\n",
       "  67,\n",
       "  54,\n",
       "  74,\n",
       "  77,\n",
       "  73,\n",
       "  34,\n",
       "  2,\n",
       "  98,\n",
       "  20,\n",
       "  52,\n",
       "  15,\n",
       "  60,\n",
       "  94,\n",
       "  24,\n",
       "  27,\n",
       "  45,\n",
       "  0,\n",
       "  22],\n",
       " [18,\n",
       "  34,\n",
       "  74,\n",
       "  15,\n",
       "  54,\n",
       "  38,\n",
       "  31,\n",
       "  49,\n",
       "  98,\n",
       "  46,\n",
       "  35,\n",
       "  45,\n",
       "  7,\n",
       "  60,\n",
       "  69,\n",
       "  14,\n",
       "  25,\n",
       "  30,\n",
       "  12,\n",
       "  61],\n",
       " [34,\n",
       "  78,\n",
       "  59,\n",
       "  54,\n",
       "  20,\n",
       "  77,\n",
       "  18,\n",
       "  99,\n",
       "  60,\n",
       "  35,\n",
       "  27,\n",
       "  94,\n",
       "  74,\n",
       "  98,\n",
       "  55,\n",
       "  15,\n",
       "  25,\n",
       "  37,\n",
       "  30,\n",
       "  67],\n",
       " [58,\n",
       "  78,\n",
       "  18,\n",
       "  2,\n",
       "  20,\n",
       "  74,\n",
       "  35,\n",
       "  60,\n",
       "  22,\n",
       "  37,\n",
       "  21,\n",
       "  23,\n",
       "  34,\n",
       "  59,\n",
       "  98,\n",
       "  31,\n",
       "  13,\n",
       "  6,\n",
       "  50,\n",
       "  17],\n",
       " [99,\n",
       "  54,\n",
       "  20,\n",
       "  15,\n",
       "  18,\n",
       "  30,\n",
       "  60,\n",
       "  2,\n",
       "  78,\n",
       "  16,\n",
       "  34,\n",
       "  35,\n",
       "  56,\n",
       "  31,\n",
       "  59,\n",
       "  49,\n",
       "  55,\n",
       "  57,\n",
       "  46,\n",
       "  45],\n",
       " [18,\n",
       "  54,\n",
       "  99,\n",
       "  34,\n",
       "  22,\n",
       "  58,\n",
       "  98,\n",
       "  74,\n",
       "  35,\n",
       "  2,\n",
       "  38,\n",
       "  15,\n",
       "  49,\n",
       "  51,\n",
       "  56,\n",
       "  69,\n",
       "  20,\n",
       "  31,\n",
       "  73,\n",
       "  60],\n",
       " [37,\n",
       "  18,\n",
       "  35,\n",
       "  22,\n",
       "  15,\n",
       "  38,\n",
       "  55,\n",
       "  73,\n",
       "  78,\n",
       "  96,\n",
       "  45,\n",
       "  42,\n",
       "  49,\n",
       "  56,\n",
       "  94,\n",
       "  31,\n",
       "  0,\n",
       "  54,\n",
       "  6,\n",
       "  51],\n",
       " [35,\n",
       "  2,\n",
       "  18,\n",
       "  54,\n",
       "  20,\n",
       "  22,\n",
       "  16,\n",
       "  73,\n",
       "  67,\n",
       "  45,\n",
       "  78,\n",
       "  99,\n",
       "  15,\n",
       "  42,\n",
       "  96,\n",
       "  49,\n",
       "  51,\n",
       "  8,\n",
       "  52,\n",
       "  31],\n",
       " [7,\n",
       "  26,\n",
       "  99,\n",
       "  21,\n",
       "  51,\n",
       "  85,\n",
       "  18,\n",
       "  49,\n",
       "  16,\n",
       "  56,\n",
       "  45,\n",
       "  6,\n",
       "  14,\n",
       "  52,\n",
       "  12,\n",
       "  40,\n",
       "  42,\n",
       "  46,\n",
       "  30,\n",
       "  15],\n",
       " [18,\n",
       "  35,\n",
       "  20,\n",
       "  67,\n",
       "  73,\n",
       "  22,\n",
       "  54,\n",
       "  74,\n",
       "  31,\n",
       "  62,\n",
       "  98,\n",
       "  99,\n",
       "  78,\n",
       "  15,\n",
       "  34,\n",
       "  60,\n",
       "  59,\n",
       "  46,\n",
       "  30,\n",
       "  16],\n",
       " [69, 2, 60, 63, 18, 67, 31, 85, 74, 56, 9, 49, 22, 4, 45, 15, 42, 6, 73, 92],\n",
       " [15,\n",
       "  69,\n",
       "  18,\n",
       "  2,\n",
       "  56,\n",
       "  49,\n",
       "  35,\n",
       "  54,\n",
       "  31,\n",
       "  60,\n",
       "  38,\n",
       "  45,\n",
       "  74,\n",
       "  34,\n",
       "  99,\n",
       "  20,\n",
       "  78,\n",
       "  57,\n",
       "  93,\n",
       "  62],\n",
       " [85, 69, 56, 30, 7, 63, 60, 18, 45, 31, 15, 99, 1, 9, 34, 8, 25, 67, 12, 2],\n",
       " [37,\n",
       "  78,\n",
       "  18,\n",
       "  55,\n",
       "  35,\n",
       "  20,\n",
       "  67,\n",
       "  2,\n",
       "  0,\n",
       "  30,\n",
       "  22,\n",
       "  60,\n",
       "  73,\n",
       "  42,\n",
       "  46,\n",
       "  27,\n",
       "  15,\n",
       "  62,\n",
       "  14,\n",
       "  31],\n",
       " [18,\n",
       "  35,\n",
       "  74,\n",
       "  15,\n",
       "  45,\n",
       "  56,\n",
       "  61,\n",
       "  31,\n",
       "  54,\n",
       "  38,\n",
       "  78,\n",
       "  94,\n",
       "  73,\n",
       "  52,\n",
       "  67,\n",
       "  0,\n",
       "  60,\n",
       "  2,\n",
       "  14,\n",
       "  42],\n",
       " [2,\n",
       "  20,\n",
       "  18,\n",
       "  35,\n",
       "  16,\n",
       "  42,\n",
       "  22,\n",
       "  15,\n",
       "  69,\n",
       "  45,\n",
       "  67,\n",
       "  99,\n",
       "  63,\n",
       "  73,\n",
       "  31,\n",
       "  54,\n",
       "  62,\n",
       "  30,\n",
       "  78,\n",
       "  96],\n",
       " [30,\n",
       "  18,\n",
       "  60,\n",
       "  67,\n",
       "  20,\n",
       "  56,\n",
       "  42,\n",
       "  99,\n",
       "  2,\n",
       "  14,\n",
       "  15,\n",
       "  54,\n",
       "  22,\n",
       "  31,\n",
       "  46,\n",
       "  62,\n",
       "  73,\n",
       "  69,\n",
       "  16,\n",
       "  85],\n",
       " [18,\n",
       "  2,\n",
       "  69,\n",
       "  31,\n",
       "  42,\n",
       "  60,\n",
       "  45,\n",
       "  15,\n",
       "  21,\n",
       "  56,\n",
       "  67,\n",
       "  49,\n",
       "  85,\n",
       "  74,\n",
       "  35,\n",
       "  63,\n",
       "  30,\n",
       "  92,\n",
       "  22,\n",
       "  20],\n",
       " [45,\n",
       "  18,\n",
       "  15,\n",
       "  16,\n",
       "  99,\n",
       "  54,\n",
       "  35,\n",
       "  2,\n",
       "  78,\n",
       "  42,\n",
       "  30,\n",
       "  31,\n",
       "  20,\n",
       "  52,\n",
       "  37,\n",
       "  94,\n",
       "  55,\n",
       "  60,\n",
       "  57,\n",
       "  51],\n",
       " [18,\n",
       "  60,\n",
       "  34,\n",
       "  56,\n",
       "  99,\n",
       "  54,\n",
       "  15,\n",
       "  30,\n",
       "  31,\n",
       "  20,\n",
       "  46,\n",
       "  74,\n",
       "  7,\n",
       "  69,\n",
       "  14,\n",
       "  59,\n",
       "  78,\n",
       "  12,\n",
       "  49,\n",
       "  2],\n",
       " [67, 30, 8, 42, 63, 73, 85, 20, 2, 69, 99, 14, 56, 16, 18, 22, 54, 60, 6, 45],\n",
       " [18,\n",
       "  74,\n",
       "  34,\n",
       "  31,\n",
       "  73,\n",
       "  60,\n",
       "  59,\n",
       "  35,\n",
       "  24,\n",
       "  30,\n",
       "  61,\n",
       "  52,\n",
       "  54,\n",
       "  69,\n",
       "  2,\n",
       "  14,\n",
       "  63,\n",
       "  78,\n",
       "  72,\n",
       "  7],\n",
       " [2,\n",
       "  35,\n",
       "  18,\n",
       "  96,\n",
       "  58,\n",
       "  38,\n",
       "  62,\n",
       "  22,\n",
       "  15,\n",
       "  69,\n",
       "  57,\n",
       "  98,\n",
       "  21,\n",
       "  31,\n",
       "  49,\n",
       "  71,\n",
       "  40,\n",
       "  20,\n",
       "  73,\n",
       "  42],\n",
       " [59,\n",
       "  74,\n",
       "  18,\n",
       "  34,\n",
       "  54,\n",
       "  35,\n",
       "  60,\n",
       "  78,\n",
       "  98,\n",
       "  2,\n",
       "  50,\n",
       "  31,\n",
       "  69,\n",
       "  99,\n",
       "  13,\n",
       "  15,\n",
       "  26,\n",
       "  77,\n",
       "  22,\n",
       "  16],\n",
       " [18,\n",
       "  7,\n",
       "  59,\n",
       "  26,\n",
       "  74,\n",
       "  34,\n",
       "  54,\n",
       "  35,\n",
       "  31,\n",
       "  99,\n",
       "  85,\n",
       "  20,\n",
       "  2,\n",
       "  52,\n",
       "  21,\n",
       "  46,\n",
       "  69,\n",
       "  98,\n",
       "  25,\n",
       "  60],\n",
       " [35,\n",
       "  58,\n",
       "  18,\n",
       "  2,\n",
       "  78,\n",
       "  20,\n",
       "  74,\n",
       "  21,\n",
       "  22,\n",
       "  0,\n",
       "  98,\n",
       "  37,\n",
       "  94,\n",
       "  16,\n",
       "  96,\n",
       "  54,\n",
       "  31,\n",
       "  99,\n",
       "  15,\n",
       "  34],\n",
       " [18,\n",
       "  74,\n",
       "  61,\n",
       "  60,\n",
       "  34,\n",
       "  31,\n",
       "  56,\n",
       "  15,\n",
       "  78,\n",
       "  24,\n",
       "  26,\n",
       "  35,\n",
       "  37,\n",
       "  49,\n",
       "  54,\n",
       "  12,\n",
       "  59,\n",
       "  69,\n",
       "  14,\n",
       "  45],\n",
       " [2,\n",
       "  18,\n",
       "  69,\n",
       "  31,\n",
       "  45,\n",
       "  15,\n",
       "  35,\n",
       "  67,\n",
       "  42,\n",
       "  60,\n",
       "  63,\n",
       "  78,\n",
       "  49,\n",
       "  20,\n",
       "  73,\n",
       "  30,\n",
       "  56,\n",
       "  22,\n",
       "  9,\n",
       "  62],\n",
       " [20,\n",
       "  99,\n",
       "  54,\n",
       "  60,\n",
       "  16,\n",
       "  22,\n",
       "  13,\n",
       "  2,\n",
       "  78,\n",
       "  69,\n",
       "  18,\n",
       "  59,\n",
       "  34,\n",
       "  15,\n",
       "  8,\n",
       "  50,\n",
       "  98,\n",
       "  35,\n",
       "  55,\n",
       "  27],\n",
       " [69,\n",
       "  2,\n",
       "  67,\n",
       "  18,\n",
       "  73,\n",
       "  74,\n",
       "  63,\n",
       "  31,\n",
       "  62,\n",
       "  60,\n",
       "  30,\n",
       "  42,\n",
       "  15,\n",
       "  22,\n",
       "  34,\n",
       "  8,\n",
       "  35,\n",
       "  20,\n",
       "  65,\n",
       "  45],\n",
       " [18,\n",
       "  74,\n",
       "  15,\n",
       "  56,\n",
       "  49,\n",
       "  34,\n",
       "  60,\n",
       "  31,\n",
       "  35,\n",
       "  38,\n",
       "  54,\n",
       "  98,\n",
       "  69,\n",
       "  99,\n",
       "  26,\n",
       "  2,\n",
       "  58,\n",
       "  45,\n",
       "  40,\n",
       "  37],\n",
       " [45,\n",
       "  49,\n",
       "  54,\n",
       "  26,\n",
       "  93,\n",
       "  69,\n",
       "  56,\n",
       "  99,\n",
       "  15,\n",
       "  16,\n",
       "  89,\n",
       "  28,\n",
       "  18,\n",
       "  52,\n",
       "  31,\n",
       "  51,\n",
       "  8,\n",
       "  78,\n",
       "  7,\n",
       "  60],\n",
       " [69, 60, 56, 4, 2, 18, 15, 22, 37, 99, 20, 49, 1, 85, 30, 42, 9, 57, 16, 62],\n",
       " [54,\n",
       "  59,\n",
       "  26,\n",
       "  18,\n",
       "  35,\n",
       "  78,\n",
       "  99,\n",
       "  94,\n",
       "  15,\n",
       "  45,\n",
       "  34,\n",
       "  49,\n",
       "  7,\n",
       "  52,\n",
       "  31,\n",
       "  60,\n",
       "  74,\n",
       "  77,\n",
       "  24,\n",
       "  98],\n",
       " [2,\n",
       "  18,\n",
       "  20,\n",
       "  22,\n",
       "  74,\n",
       "  60,\n",
       "  35,\n",
       "  31,\n",
       "  73,\n",
       "  67,\n",
       "  15,\n",
       "  99,\n",
       "  34,\n",
       "  21,\n",
       "  54,\n",
       "  98,\n",
       "  42,\n",
       "  16,\n",
       "  58,\n",
       "  51],\n",
       " [18,\n",
       "  74,\n",
       "  34,\n",
       "  21,\n",
       "  46,\n",
       "  2,\n",
       "  69,\n",
       "  7,\n",
       "  62,\n",
       "  56,\n",
       "  40,\n",
       "  31,\n",
       "  71,\n",
       "  11,\n",
       "  99,\n",
       "  25,\n",
       "  22,\n",
       "  98,\n",
       "  54,\n",
       "  58],\n",
       " [34,\n",
       "  74,\n",
       "  69,\n",
       "  60,\n",
       "  56,\n",
       "  18,\n",
       "  31,\n",
       "  15,\n",
       "  25,\n",
       "  54,\n",
       "  7,\n",
       "  98,\n",
       "  62,\n",
       "  85,\n",
       "  99,\n",
       "  59,\n",
       "  49,\n",
       "  46,\n",
       "  2,\n",
       "  38],\n",
       " [18,\n",
       "  60,\n",
       "  20,\n",
       "  54,\n",
       "  30,\n",
       "  15,\n",
       "  35,\n",
       "  31,\n",
       "  99,\n",
       "  78,\n",
       "  34,\n",
       "  2,\n",
       "  59,\n",
       "  46,\n",
       "  56,\n",
       "  74,\n",
       "  16,\n",
       "  42,\n",
       "  52,\n",
       "  67],\n",
       " [67,\n",
       "  2,\n",
       "  73,\n",
       "  18,\n",
       "  30,\n",
       "  46,\n",
       "  20,\n",
       "  22,\n",
       "  42,\n",
       "  62,\n",
       "  69,\n",
       "  63,\n",
       "  54,\n",
       "  11,\n",
       "  31,\n",
       "  35,\n",
       "  14,\n",
       "  8,\n",
       "  82,\n",
       "  34],\n",
       " [35,\n",
       "  18,\n",
       "  2,\n",
       "  20,\n",
       "  54,\n",
       "  99,\n",
       "  15,\n",
       "  30,\n",
       "  45,\n",
       "  96,\n",
       "  98,\n",
       "  31,\n",
       "  16,\n",
       "  59,\n",
       "  46,\n",
       "  42,\n",
       "  78,\n",
       "  52,\n",
       "  58,\n",
       "  49],\n",
       " [74,\n",
       "  35,\n",
       "  18,\n",
       "  98,\n",
       "  60,\n",
       "  78,\n",
       "  2,\n",
       "  20,\n",
       "  54,\n",
       "  59,\n",
       "  22,\n",
       "  73,\n",
       "  31,\n",
       "  15,\n",
       "  67,\n",
       "  69,\n",
       "  62,\n",
       "  46,\n",
       "  38,\n",
       "  99],\n",
       " [58,\n",
       "  35,\n",
       "  18,\n",
       "  78,\n",
       "  59,\n",
       "  99,\n",
       "  94,\n",
       "  15,\n",
       "  16,\n",
       "  20,\n",
       "  17,\n",
       "  98,\n",
       "  54,\n",
       "  23,\n",
       "  31,\n",
       "  29,\n",
       "  34,\n",
       "  49,\n",
       "  74,\n",
       "  13],\n",
       " [74,\n",
       "  18,\n",
       "  2,\n",
       "  35,\n",
       "  31,\n",
       "  69,\n",
       "  67,\n",
       "  34,\n",
       "  59,\n",
       "  98,\n",
       "  73,\n",
       "  78,\n",
       "  62,\n",
       "  20,\n",
       "  63,\n",
       "  15,\n",
       "  9,\n",
       "  22,\n",
       "  52,\n",
       "  38],\n",
       " [18,\n",
       "  35,\n",
       "  78,\n",
       "  15,\n",
       "  31,\n",
       "  2,\n",
       "  45,\n",
       "  30,\n",
       "  54,\n",
       "  67,\n",
       "  20,\n",
       "  42,\n",
       "  74,\n",
       "  52,\n",
       "  61,\n",
       "  59,\n",
       "  49,\n",
       "  73,\n",
       "  34,\n",
       "  56],\n",
       " [35,\n",
       "  18,\n",
       "  78,\n",
       "  2,\n",
       "  16,\n",
       "  20,\n",
       "  45,\n",
       "  54,\n",
       "  15,\n",
       "  99,\n",
       "  31,\n",
       "  59,\n",
       "  30,\n",
       "  52,\n",
       "  94,\n",
       "  49,\n",
       "  42,\n",
       "  23,\n",
       "  24,\n",
       "  57],\n",
       " [2, 60, 62, 18, 31, 15, 23, 57, 22, 50, 74, 99, 21, 16, 34, 1, 85, 9, 58, 30],\n",
       " [99,\n",
       "  20,\n",
       "  2,\n",
       "  16,\n",
       "  54,\n",
       "  30,\n",
       "  15,\n",
       "  18,\n",
       "  56,\n",
       "  49,\n",
       "  45,\n",
       "  42,\n",
       "  69,\n",
       "  57,\n",
       "  31,\n",
       "  4,\n",
       "  60,\n",
       "  35,\n",
       "  71,\n",
       "  62],\n",
       " [99,\n",
       "  56,\n",
       "  69,\n",
       "  49,\n",
       "  54,\n",
       "  15,\n",
       "  51,\n",
       "  18,\n",
       "  45,\n",
       "  4,\n",
       "  93,\n",
       "  13,\n",
       "  34,\n",
       "  60,\n",
       "  38,\n",
       "  98,\n",
       "  71,\n",
       "  31,\n",
       "  16,\n",
       "  20],\n",
       " [99,\n",
       "  56,\n",
       "  54,\n",
       "  7,\n",
       "  85,\n",
       "  18,\n",
       "  49,\n",
       "  30,\n",
       "  34,\n",
       "  15,\n",
       "  26,\n",
       "  60,\n",
       "  25,\n",
       "  45,\n",
       "  31,\n",
       "  59,\n",
       "  20,\n",
       "  16,\n",
       "  12,\n",
       "  46],\n",
       " [56,\n",
       "  18,\n",
       "  54,\n",
       "  49,\n",
       "  73,\n",
       "  45,\n",
       "  69,\n",
       "  38,\n",
       "  15,\n",
       "  46,\n",
       "  22,\n",
       "  8,\n",
       "  74,\n",
       "  34,\n",
       "  51,\n",
       "  99,\n",
       "  37,\n",
       "  31,\n",
       "  67,\n",
       "  12],\n",
       " [60,\n",
       "  18,\n",
       "  56,\n",
       "  15,\n",
       "  78,\n",
       "  49,\n",
       "  61,\n",
       "  69,\n",
       "  31,\n",
       "  74,\n",
       "  54,\n",
       "  55,\n",
       "  99,\n",
       "  12,\n",
       "  6,\n",
       "  4,\n",
       "  38,\n",
       "  27,\n",
       "  94,\n",
       "  14],\n",
       " [6,\n",
       "  22,\n",
       "  18,\n",
       "  67,\n",
       "  54,\n",
       "  69,\n",
       "  2,\n",
       "  73,\n",
       "  60,\n",
       "  35,\n",
       "  16,\n",
       "  49,\n",
       "  78,\n",
       "  99,\n",
       "  77,\n",
       "  31,\n",
       "  74,\n",
       "  52,\n",
       "  51,\n",
       "  20],\n",
       " [67, 69, 60, 18, 74, 8, 73, 9, 31, 24, 34, 61, 56, 2, 54, 63, 15, 28, 30, 78],\n",
       " [69, 54, 49, 45, 56, 2, 8, 18, 15, 99, 51, 22, 4, 35, 38, 28, 93, 31, 73, 98],\n",
       " [18,\n",
       "  35,\n",
       "  2,\n",
       "  54,\n",
       "  20,\n",
       "  16,\n",
       "  59,\n",
       "  58,\n",
       "  74,\n",
       "  49,\n",
       "  31,\n",
       "  22,\n",
       "  21,\n",
       "  15,\n",
       "  98,\n",
       "  26,\n",
       "  51,\n",
       "  6,\n",
       "  45,\n",
       "  69],\n",
       " [67,\n",
       "  18,\n",
       "  14,\n",
       "  7,\n",
       "  6,\n",
       "  52,\n",
       "  66,\n",
       "  61,\n",
       "  42,\n",
       "  31,\n",
       "  30,\n",
       "  63,\n",
       "  74,\n",
       "  73,\n",
       "  85,\n",
       "  35,\n",
       "  56,\n",
       "  46,\n",
       "  12,\n",
       "  54],\n",
       " [7,\n",
       "  85,\n",
       "  34,\n",
       "  25,\n",
       "  59,\n",
       "  26,\n",
       "  54,\n",
       "  60,\n",
       "  99,\n",
       "  18,\n",
       "  30,\n",
       "  56,\n",
       "  14,\n",
       "  46,\n",
       "  1,\n",
       "  20,\n",
       "  69,\n",
       "  31,\n",
       "  87,\n",
       "  12],\n",
       " [46,\n",
       "  99,\n",
       "  22,\n",
       "  54,\n",
       "  18,\n",
       "  37,\n",
       "  14,\n",
       "  20,\n",
       "  34,\n",
       "  0,\n",
       "  56,\n",
       "  30,\n",
       "  15,\n",
       "  73,\n",
       "  55,\n",
       "  60,\n",
       "  11,\n",
       "  77,\n",
       "  16,\n",
       "  27],\n",
       " [69,\n",
       "  4,\n",
       "  51,\n",
       "  13,\n",
       "  22,\n",
       "  99,\n",
       "  49,\n",
       "  21,\n",
       "  58,\n",
       "  16,\n",
       "  60,\n",
       "  54,\n",
       "  93,\n",
       "  84,\n",
       "  33,\n",
       "  20,\n",
       "  85,\n",
       "  50,\n",
       "  6,\n",
       "  87],\n",
       " [34, 74, 50, 62, 60, 20, 2, 69, 59, 17, 98, 31, 85, 58, 13, 1, 35, 99, 46, 9],\n",
       " [16, 18, 99, 2, 22, 20, 54, 37, 78, 15, 35, 49, 51, 60, 6, 42, 4, 58, 31, 45],\n",
       " [18,\n",
       "  35,\n",
       "  74,\n",
       "  20,\n",
       "  59,\n",
       "  34,\n",
       "  58,\n",
       "  60,\n",
       "  98,\n",
       "  2,\n",
       "  31,\n",
       "  54,\n",
       "  17,\n",
       "  50,\n",
       "  99,\n",
       "  23,\n",
       "  15,\n",
       "  16,\n",
       "  26,\n",
       "  13],\n",
       " [67,\n",
       "  69,\n",
       "  8,\n",
       "  63,\n",
       "  2,\n",
       "  73,\n",
       "  30,\n",
       "  60,\n",
       "  42,\n",
       "  45,\n",
       "  18,\n",
       "  22,\n",
       "  54,\n",
       "  20,\n",
       "  31,\n",
       "  56,\n",
       "  85,\n",
       "  15,\n",
       "  82,\n",
       "  78],\n",
       " [99,\n",
       "  46,\n",
       "  59,\n",
       "  94,\n",
       "  18,\n",
       "  7,\n",
       "  30,\n",
       "  35,\n",
       "  20,\n",
       "  34,\n",
       "  15,\n",
       "  16,\n",
       "  52,\n",
       "  14,\n",
       "  77,\n",
       "  95,\n",
       "  78,\n",
       "  26,\n",
       "  98,\n",
       "  0],\n",
       " [99, 85, 54, 30, 56, 7, 16, 20, 69, 45, 49, 8, 51, 15, 25, 18, 2, 59, 63, 71],\n",
       " [74,\n",
       "  18,\n",
       "  34,\n",
       "  26,\n",
       "  15,\n",
       "  60,\n",
       "  31,\n",
       "  35,\n",
       "  78,\n",
       "  61,\n",
       "  56,\n",
       "  59,\n",
       "  54,\n",
       "  24,\n",
       "  49,\n",
       "  94,\n",
       "  45,\n",
       "  38,\n",
       "  7,\n",
       "  90],\n",
       " [45,\n",
       "  54,\n",
       "  18,\n",
       "  30,\n",
       "  16,\n",
       "  99,\n",
       "  78,\n",
       "  35,\n",
       "  49,\n",
       "  52,\n",
       "  42,\n",
       "  67,\n",
       "  56,\n",
       "  8,\n",
       "  61,\n",
       "  20,\n",
       "  31,\n",
       "  2,\n",
       "  63,\n",
       "  60],\n",
       " [56,\n",
       "  61,\n",
       "  18,\n",
       "  14,\n",
       "  60,\n",
       "  69,\n",
       "  74,\n",
       "  37,\n",
       "  31,\n",
       "  15,\n",
       "  34,\n",
       "  67,\n",
       "  1,\n",
       "  30,\n",
       "  42,\n",
       "  49,\n",
       "  9,\n",
       "  45,\n",
       "  66,\n",
       "  72],\n",
       " [34,\n",
       "  69,\n",
       "  74,\n",
       "  18,\n",
       "  56,\n",
       "  62,\n",
       "  9,\n",
       "  31,\n",
       "  15,\n",
       "  2,\n",
       "  67,\n",
       "  65,\n",
       "  30,\n",
       "  73,\n",
       "  38,\n",
       "  41,\n",
       "  22,\n",
       "  98,\n",
       "  61,\n",
       "  54],\n",
       " [2,\n",
       "  20,\n",
       "  35,\n",
       "  78,\n",
       "  16,\n",
       "  60,\n",
       "  31,\n",
       "  22,\n",
       "  69,\n",
       "  15,\n",
       "  58,\n",
       "  54,\n",
       "  99,\n",
       "  21,\n",
       "  74,\n",
       "  49,\n",
       "  13,\n",
       "  23,\n",
       "  59,\n",
       "  6],\n",
       " [8, 69, 54, 67, 99, 2, 22, 20, 73, 28, 85, 77, 18, 87, 63, 59, 6, 45, 51, 49],\n",
       " [99,\n",
       "  16,\n",
       "  94,\n",
       "  54,\n",
       "  18,\n",
       "  20,\n",
       "  78,\n",
       "  15,\n",
       "  35,\n",
       "  37,\n",
       "  30,\n",
       "  45,\n",
       "  49,\n",
       "  42,\n",
       "  55,\n",
       "  58,\n",
       "  51,\n",
       "  52,\n",
       "  46,\n",
       "  56],\n",
       " [2,\n",
       "  74,\n",
       "  69,\n",
       "  31,\n",
       "  21,\n",
       "  45,\n",
       "  35,\n",
       "  26,\n",
       "  49,\n",
       "  15,\n",
       "  63,\n",
       "  62,\n",
       "  60,\n",
       "  85,\n",
       "  42,\n",
       "  56,\n",
       "  40,\n",
       "  52,\n",
       "  57,\n",
       "  92],\n",
       " [2,\n",
       "  18,\n",
       "  69,\n",
       "  54,\n",
       "  35,\n",
       "  67,\n",
       "  73,\n",
       "  45,\n",
       "  22,\n",
       "  20,\n",
       "  8,\n",
       "  99,\n",
       "  49,\n",
       "  31,\n",
       "  51,\n",
       "  16,\n",
       "  63,\n",
       "  15,\n",
       "  42,\n",
       "  59],\n",
       " [69,\n",
       "  2,\n",
       "  4,\n",
       "  22,\n",
       "  62,\n",
       "  18,\n",
       "  74,\n",
       "  38,\n",
       "  15,\n",
       "  58,\n",
       "  56,\n",
       "  49,\n",
       "  51,\n",
       "  31,\n",
       "  84,\n",
       "  96,\n",
       "  93,\n",
       "  98,\n",
       "  71,\n",
       "  20],\n",
       " [58,\n",
       "  74,\n",
       "  2,\n",
       "  18,\n",
       "  35,\n",
       "  34,\n",
       "  98,\n",
       "  20,\n",
       "  62,\n",
       "  60,\n",
       "  15,\n",
       "  50,\n",
       "  31,\n",
       "  38,\n",
       "  78,\n",
       "  99,\n",
       "  57,\n",
       "  59,\n",
       "  96,\n",
       "  69],\n",
       " [69, 67, 18, 2, 85, 22, 73, 54, 74, 60, 31, 7, 34, 99, 25, 63, 20, 8, 30, 46],\n",
       " [74,\n",
       "  34,\n",
       "  60,\n",
       "  18,\n",
       "  62,\n",
       "  69,\n",
       "  2,\n",
       "  98,\n",
       "  31,\n",
       "  78,\n",
       "  9,\n",
       "  15,\n",
       "  35,\n",
       "  20,\n",
       "  38,\n",
       "  22,\n",
       "  59,\n",
       "  76,\n",
       "  54,\n",
       "  24],\n",
       " [99,\n",
       "  30,\n",
       "  54,\n",
       "  14,\n",
       "  7,\n",
       "  85,\n",
       "  16,\n",
       "  46,\n",
       "  25,\n",
       "  18,\n",
       "  20,\n",
       "  15,\n",
       "  49,\n",
       "  60,\n",
       "  45,\n",
       "  42,\n",
       "  1,\n",
       "  52,\n",
       "  27,\n",
       "  31],\n",
       " [2,\n",
       "  62,\n",
       "  20,\n",
       "  18,\n",
       "  67,\n",
       "  73,\n",
       "  42,\n",
       "  63,\n",
       "  69,\n",
       "  35,\n",
       "  11,\n",
       "  51,\n",
       "  16,\n",
       "  21,\n",
       "  71,\n",
       "  46,\n",
       "  65,\n",
       "  99,\n",
       "  85,\n",
       "  82],\n",
       " [18,\n",
       "  60,\n",
       "  69,\n",
       "  2,\n",
       "  31,\n",
       "  62,\n",
       "  34,\n",
       "  15,\n",
       "  38,\n",
       "  56,\n",
       "  9,\n",
       "  35,\n",
       "  37,\n",
       "  22,\n",
       "  57,\n",
       "  78,\n",
       "  49,\n",
       "  73,\n",
       "  67,\n",
       "  65],\n",
       " [62,\n",
       "  60,\n",
       "  18,\n",
       "  2,\n",
       "  34,\n",
       "  22,\n",
       "  69,\n",
       "  74,\n",
       "  56,\n",
       "  15,\n",
       "  20,\n",
       "  37,\n",
       "  31,\n",
       "  38,\n",
       "  46,\n",
       "  65,\n",
       "  57,\n",
       "  73,\n",
       "  98,\n",
       "  58],\n",
       " [54,\n",
       "  99,\n",
       "  20,\n",
       "  8,\n",
       "  18,\n",
       "  15,\n",
       "  35,\n",
       "  46,\n",
       "  59,\n",
       "  16,\n",
       "  56,\n",
       "  34,\n",
       "  78,\n",
       "  77,\n",
       "  45,\n",
       "  49,\n",
       "  60,\n",
       "  98,\n",
       "  2,\n",
       "  25],\n",
       " [18, 35, 2, 45, 42, 52, 31, 26, 21, 16, 7, 6, 67, 30, 20, 63, 59, 78, 15, 54],\n",
       " [35,\n",
       "  2,\n",
       "  98,\n",
       "  18,\n",
       "  74,\n",
       "  20,\n",
       "  62,\n",
       "  34,\n",
       "  73,\n",
       "  59,\n",
       "  54,\n",
       "  96,\n",
       "  67,\n",
       "  75,\n",
       "  31,\n",
       "  15,\n",
       "  46,\n",
       "  78,\n",
       "  38,\n",
       "  99],\n",
       " [59, 16, 85, 26, 20, 7, 99, 52, 2, 18, 35, 67, 63, 50, 45, 6, 21, 13, 82, 31],\n",
       " [20,\n",
       "  30,\n",
       "  2,\n",
       "  99,\n",
       "  60,\n",
       "  54,\n",
       "  67,\n",
       "  69,\n",
       "  62,\n",
       "  15,\n",
       "  34,\n",
       "  31,\n",
       "  85,\n",
       "  16,\n",
       "  56,\n",
       "  42,\n",
       "  46,\n",
       "  8,\n",
       "  59,\n",
       "  78],\n",
       " [74,\n",
       "  18,\n",
       "  69,\n",
       "  38,\n",
       "  35,\n",
       "  15,\n",
       "  31,\n",
       "  98,\n",
       "  34,\n",
       "  60,\n",
       "  9,\n",
       "  96,\n",
       "  56,\n",
       "  65,\n",
       "  57,\n",
       "  73,\n",
       "  49,\n",
       "  22,\n",
       "  78,\n",
       "  45],\n",
       " [18,\n",
       "  15,\n",
       "  56,\n",
       "  34,\n",
       "  54,\n",
       "  60,\n",
       "  31,\n",
       "  74,\n",
       "  49,\n",
       "  69,\n",
       "  45,\n",
       "  30,\n",
       "  99,\n",
       "  59,\n",
       "  2,\n",
       "  98,\n",
       "  78,\n",
       "  38,\n",
       "  9,\n",
       "  61],\n",
       " [2,\n",
       "  58,\n",
       "  62,\n",
       "  18,\n",
       "  22,\n",
       "  20,\n",
       "  99,\n",
       "  71,\n",
       "  46,\n",
       "  96,\n",
       "  35,\n",
       "  21,\n",
       "  51,\n",
       "  15,\n",
       "  98,\n",
       "  57,\n",
       "  40,\n",
       "  38,\n",
       "  34,\n",
       "  11],\n",
       " [69,\n",
       "  2,\n",
       "  71,\n",
       "  56,\n",
       "  49,\n",
       "  18,\n",
       "  62,\n",
       "  99,\n",
       "  38,\n",
       "  50,\n",
       "  31,\n",
       "  57,\n",
       "  98,\n",
       "  20,\n",
       "  60,\n",
       "  54,\n",
       "  74,\n",
       "  34,\n",
       "  58,\n",
       "  45],\n",
       " [56, 69, 85, 49, 15, 30, 18, 12, 31, 71, 61, 9, 7, 63, 4, 1, 34, 2, 42, 62],\n",
       " [67,\n",
       "  18,\n",
       "  69,\n",
       "  45,\n",
       "  31,\n",
       "  61,\n",
       "  30,\n",
       "  24,\n",
       "  2,\n",
       "  56,\n",
       "  42,\n",
       "  85,\n",
       "  60,\n",
       "  74,\n",
       "  66,\n",
       "  52,\n",
       "  15,\n",
       "  9,\n",
       "  73,\n",
       "  72],\n",
       " [85, 54, 2, 99, 59, 20, 7, 50, 18, 34, 8, 60, 56, 31, 25, 49, 63, 74, 13, 30],\n",
       " [37, 18, 2, 22, 42, 15, 73, 60, 31, 69, 67, 35, 62, 49, 38, 6, 45, 14, 74, 4],\n",
       " [18, 2, 74, 37, 35, 60, 31, 6, 22, 78, 20, 0, 42, 34, 67, 15, 62, 92, 58, 73],\n",
       " [2,\n",
       "  20,\n",
       "  69,\n",
       "  60,\n",
       "  18,\n",
       "  22,\n",
       "  67,\n",
       "  30,\n",
       "  31,\n",
       "  99,\n",
       "  15,\n",
       "  34,\n",
       "  63,\n",
       "  16,\n",
       "  85,\n",
       "  57,\n",
       "  42,\n",
       "  82,\n",
       "  9,\n",
       "  65],\n",
       " [37,\n",
       "  54,\n",
       "  18,\n",
       "  99,\n",
       "  56,\n",
       "  22,\n",
       "  15,\n",
       "  55,\n",
       "  60,\n",
       "  49,\n",
       "  34,\n",
       "  94,\n",
       "  46,\n",
       "  78,\n",
       "  35,\n",
       "  14,\n",
       "  77,\n",
       "  38,\n",
       "  4,\n",
       "  31],\n",
       " [45,\n",
       "  61,\n",
       "  56,\n",
       "  30,\n",
       "  63,\n",
       "  67,\n",
       "  18,\n",
       "  42,\n",
       "  85,\n",
       "  52,\n",
       "  31,\n",
       "  69,\n",
       "  49,\n",
       "  7,\n",
       "  15,\n",
       "  66,\n",
       "  54,\n",
       "  14,\n",
       "  6,\n",
       "  60],\n",
       " [18,\n",
       "  6,\n",
       "  35,\n",
       "  16,\n",
       "  54,\n",
       "  78,\n",
       "  59,\n",
       "  26,\n",
       "  2,\n",
       "  20,\n",
       "  22,\n",
       "  52,\n",
       "  21,\n",
       "  31,\n",
       "  67,\n",
       "  99,\n",
       "  37,\n",
       "  60,\n",
       "  74,\n",
       "  49],\n",
       " [20, 69, 85, 50, 2, 59, 54, 60, 99, 16, 13, 18, 30, 67, 31, 63, 34, 8, 9, 82],\n",
       " [18,\n",
       "  34,\n",
       "  20,\n",
       "  60,\n",
       "  74,\n",
       "  54,\n",
       "  59,\n",
       "  31,\n",
       "  2,\n",
       "  35,\n",
       "  99,\n",
       "  15,\n",
       "  30,\n",
       "  67,\n",
       "  78,\n",
       "  46,\n",
       "  69,\n",
       "  98,\n",
       "  62,\n",
       "  7],\n",
       " [69, 18, 56, 2, 60, 74, 31, 6, 49, 21, 15, 22, 92, 26, 45, 7, 42, 34, 52, 54],\n",
       " [69, 60, 99, 85, 4, 22, 37, 54, 1, 14, 20, 67, 6, 18, 34, 30, 8, 27, 16, 87],\n",
       " [56, 18, 61, 30, 67, 60, 15, 69, 31, 45, 54, 42, 2, 73, 14, 8, 49, 34, 9, 63],\n",
       " [35,\n",
       "  18,\n",
       "  78,\n",
       "  45,\n",
       "  94,\n",
       "  15,\n",
       "  54,\n",
       "  42,\n",
       "  52,\n",
       "  61,\n",
       "  31,\n",
       "  37,\n",
       "  30,\n",
       "  2,\n",
       "  49,\n",
       "  16,\n",
       "  55,\n",
       "  67,\n",
       "  20,\n",
       "  99],\n",
       " [78,\n",
       "  35,\n",
       "  18,\n",
       "  20,\n",
       "  59,\n",
       "  37,\n",
       "  16,\n",
       "  6,\n",
       "  58,\n",
       "  94,\n",
       "  60,\n",
       "  22,\n",
       "  54,\n",
       "  2,\n",
       "  74,\n",
       "  31,\n",
       "  27,\n",
       "  77,\n",
       "  26,\n",
       "  99],\n",
       " [15,\n",
       "  2,\n",
       "  35,\n",
       "  18,\n",
       "  49,\n",
       "  38,\n",
       "  54,\n",
       "  99,\n",
       "  45,\n",
       "  98,\n",
       "  69,\n",
       "  20,\n",
       "  31,\n",
       "  58,\n",
       "  57,\n",
       "  93,\n",
       "  78,\n",
       "  60,\n",
       "  22,\n",
       "  51],\n",
       " [69,\n",
       "  60,\n",
       "  2,\n",
       "  4,\n",
       "  99,\n",
       "  20,\n",
       "  18,\n",
       "  16,\n",
       "  15,\n",
       "  22,\n",
       "  13,\n",
       "  49,\n",
       "  21,\n",
       "  56,\n",
       "  58,\n",
       "  31,\n",
       "  57,\n",
       "  51,\n",
       "  85,\n",
       "  23],\n",
       " [18,\n",
       "  15,\n",
       "  37,\n",
       "  56,\n",
       "  78,\n",
       "  35,\n",
       "  60,\n",
       "  31,\n",
       "  2,\n",
       "  30,\n",
       "  54,\n",
       "  61,\n",
       "  42,\n",
       "  55,\n",
       "  45,\n",
       "  67,\n",
       "  49,\n",
       "  99,\n",
       "  22,\n",
       "  73],\n",
       " [60,\n",
       "  69,\n",
       "  18,\n",
       "  2,\n",
       "  20,\n",
       "  22,\n",
       "  16,\n",
       "  85,\n",
       "  31,\n",
       "  21,\n",
       "  74,\n",
       "  78,\n",
       "  13,\n",
       "  6,\n",
       "  99,\n",
       "  82,\n",
       "  37,\n",
       "  23,\n",
       "  50,\n",
       "  15],\n",
       " [18,\n",
       "  74,\n",
       "  60,\n",
       "  34,\n",
       "  69,\n",
       "  54,\n",
       "  15,\n",
       "  31,\n",
       "  37,\n",
       "  22,\n",
       "  49,\n",
       "  78,\n",
       "  38,\n",
       "  35,\n",
       "  61,\n",
       "  98,\n",
       "  67,\n",
       "  73,\n",
       "  99,\n",
       "  2],\n",
       " [35,\n",
       "  78,\n",
       "  18,\n",
       "  2,\n",
       "  37,\n",
       "  96,\n",
       "  94,\n",
       "  0,\n",
       "  74,\n",
       "  73,\n",
       "  55,\n",
       "  67,\n",
       "  20,\n",
       "  98,\n",
       "  58,\n",
       "  23,\n",
       "  31,\n",
       "  22,\n",
       "  15,\n",
       "  77],\n",
       " [2,\n",
       "  67,\n",
       "  35,\n",
       "  63,\n",
       "  18,\n",
       "  20,\n",
       "  42,\n",
       "  54,\n",
       "  52,\n",
       "  82,\n",
       "  30,\n",
       "  59,\n",
       "  22,\n",
       "  16,\n",
       "  45,\n",
       "  46,\n",
       "  69,\n",
       "  8,\n",
       "  62,\n",
       "  11],\n",
       " [45,\n",
       "  7,\n",
       "  8,\n",
       "  30,\n",
       "  54,\n",
       "  63,\n",
       "  67,\n",
       "  18,\n",
       "  85,\n",
       "  52,\n",
       "  49,\n",
       "  25,\n",
       "  69,\n",
       "  14,\n",
       "  42,\n",
       "  99,\n",
       "  15,\n",
       "  31,\n",
       "  61,\n",
       "  73],\n",
       " [74, 69, 60, 18, 34, 26, 31, 85, 36, 21, 2, 59, 92, 17, 72, 9, 67, 7, 1, 82],\n",
       " [18,\n",
       "  34,\n",
       "  54,\n",
       "  74,\n",
       "  99,\n",
       "  15,\n",
       "  35,\n",
       "  31,\n",
       "  60,\n",
       "  56,\n",
       "  7,\n",
       "  20,\n",
       "  30,\n",
       "  49,\n",
       "  98,\n",
       "  52,\n",
       "  78,\n",
       "  45,\n",
       "  26,\n",
       "  69],\n",
       " [34,\n",
       "  60,\n",
       "  74,\n",
       "  18,\n",
       "  20,\n",
       "  59,\n",
       "  1,\n",
       "  69,\n",
       "  31,\n",
       "  17,\n",
       "  85,\n",
       "  25,\n",
       "  50,\n",
       "  62,\n",
       "  24,\n",
       "  27,\n",
       "  46,\n",
       "  54,\n",
       "  99,\n",
       "  30],\n",
       " [50,\n",
       "  2,\n",
       "  20,\n",
       "  85,\n",
       "  16,\n",
       "  69,\n",
       "  13,\n",
       "  23,\n",
       "  58,\n",
       "  99,\n",
       "  18,\n",
       "  62,\n",
       "  82,\n",
       "  59,\n",
       "  31,\n",
       "  22,\n",
       "  17,\n",
       "  34,\n",
       "  74,\n",
       "  57],\n",
       " [69,\n",
       "  2,\n",
       "  60,\n",
       "  67,\n",
       "  18,\n",
       "  74,\n",
       "  85,\n",
       "  22,\n",
       "  20,\n",
       "  82,\n",
       "  31,\n",
       "  6,\n",
       "  63,\n",
       "  92,\n",
       "  73,\n",
       "  62,\n",
       "  34,\n",
       "  21,\n",
       "  59,\n",
       "  42],\n",
       " [74,\n",
       "  34,\n",
       "  60,\n",
       "  18,\n",
       "  31,\n",
       "  69,\n",
       "  15,\n",
       "  17,\n",
       "  98,\n",
       "  9,\n",
       "  50,\n",
       "  59,\n",
       "  62,\n",
       "  26,\n",
       "  2,\n",
       "  78,\n",
       "  56,\n",
       "  35,\n",
       "  20,\n",
       "  24],\n",
       " [45,\n",
       "  56,\n",
       "  18,\n",
       "  7,\n",
       "  61,\n",
       "  49,\n",
       "  26,\n",
       "  54,\n",
       "  15,\n",
       "  52,\n",
       "  74,\n",
       "  31,\n",
       "  35,\n",
       "  94,\n",
       "  14,\n",
       "  34,\n",
       "  12,\n",
       "  30,\n",
       "  59,\n",
       "  38],\n",
       " [69,\n",
       "  2,\n",
       "  74,\n",
       "  18,\n",
       "  35,\n",
       "  38,\n",
       "  49,\n",
       "  15,\n",
       "  31,\n",
       "  45,\n",
       "  98,\n",
       "  56,\n",
       "  54,\n",
       "  34,\n",
       "  62,\n",
       "  60,\n",
       "  96,\n",
       "  73,\n",
       "  93,\n",
       "  51],\n",
       " [60,\n",
       "  18,\n",
       "  37,\n",
       "  69,\n",
       "  56,\n",
       "  22,\n",
       "  99,\n",
       "  15,\n",
       "  20,\n",
       "  54,\n",
       "  78,\n",
       "  30,\n",
       "  2,\n",
       "  31,\n",
       "  67,\n",
       "  55,\n",
       "  34,\n",
       "  27,\n",
       "  61,\n",
       "  42],\n",
       " [34,\n",
       "  18,\n",
       "  99,\n",
       "  46,\n",
       "  54,\n",
       "  15,\n",
       "  60,\n",
       "  25,\n",
       "  30,\n",
       "  14,\n",
       "  12,\n",
       "  74,\n",
       "  7,\n",
       "  31,\n",
       "  98,\n",
       "  20,\n",
       "  49,\n",
       "  59,\n",
       "  62,\n",
       "  1],\n",
       " [62,\n",
       "  30,\n",
       "  67,\n",
       "  73,\n",
       "  18,\n",
       "  65,\n",
       "  2,\n",
       "  56,\n",
       "  46,\n",
       "  60,\n",
       "  34,\n",
       "  22,\n",
       "  15,\n",
       "  41,\n",
       "  14,\n",
       "  20,\n",
       "  69,\n",
       "  42,\n",
       "  31,\n",
       "  8],\n",
       " [62, 74, 69, 2, 60, 34, 18, 38, 15, 22, 98, 58, 56, 31, 57, 20, 9, 71, 4, 65],\n",
       " [54,\n",
       "  69,\n",
       "  99,\n",
       "  8,\n",
       "  59,\n",
       "  60,\n",
       "  18,\n",
       "  16,\n",
       "  49,\n",
       "  78,\n",
       "  20,\n",
       "  89,\n",
       "  45,\n",
       "  87,\n",
       "  77,\n",
       "  52,\n",
       "  13,\n",
       "  15,\n",
       "  31,\n",
       "  6],\n",
       " [67, 18, 2, 63, 73, 45, 69, 35, 6, 42, 31, 52, 78, 8, 28, 66, 61, 60, 24, 22],\n",
       " [67,\n",
       "  8,\n",
       "  30,\n",
       "  69,\n",
       "  56,\n",
       "  73,\n",
       "  54,\n",
       "  2,\n",
       "  18,\n",
       "  60,\n",
       "  63,\n",
       "  99,\n",
       "  15,\n",
       "  42,\n",
       "  20,\n",
       "  22,\n",
       "  65,\n",
       "  31,\n",
       "  14,\n",
       "  62],\n",
       " [60,\n",
       "  67,\n",
       "  69,\n",
       "  37,\n",
       "  14,\n",
       "  61,\n",
       "  56,\n",
       "  34,\n",
       "  6,\n",
       "  31,\n",
       "  22,\n",
       "  27,\n",
       "  74,\n",
       "  30,\n",
       "  73,\n",
       "  1,\n",
       "  78,\n",
       "  54,\n",
       "  15,\n",
       "  42],\n",
       " [34,\n",
       "  56,\n",
       "  18,\n",
       "  12,\n",
       "  99,\n",
       "  15,\n",
       "  14,\n",
       "  54,\n",
       "  74,\n",
       "  46,\n",
       "  7,\n",
       "  31,\n",
       "  30,\n",
       "  49,\n",
       "  25,\n",
       "  1,\n",
       "  61,\n",
       "  26,\n",
       "  69,\n",
       "  59],\n",
       " [74,\n",
       "  2,\n",
       "  69,\n",
       "  35,\n",
       "  31,\n",
       "  98,\n",
       "  60,\n",
       "  22,\n",
       "  38,\n",
       "  15,\n",
       "  34,\n",
       "  73,\n",
       "  78,\n",
       "  49,\n",
       "  54,\n",
       "  67,\n",
       "  59,\n",
       "  45,\n",
       "  20,\n",
       "  62],\n",
       " [18,\n",
       "  35,\n",
       "  7,\n",
       "  74,\n",
       "  26,\n",
       "  21,\n",
       "  40,\n",
       "  71,\n",
       "  45,\n",
       "  2,\n",
       "  98,\n",
       "  46,\n",
       "  31,\n",
       "  49,\n",
       "  34,\n",
       "  58,\n",
       "  54,\n",
       "  52,\n",
       "  15,\n",
       "  59],\n",
       " [60, 99, 15, 54, 18, 69, 34, 20, 78, 49, 55, 37, 4, 31, 22, 16, 57, 13, 2, 9],\n",
       " [67,\n",
       "  35,\n",
       "  18,\n",
       "  77,\n",
       "  78,\n",
       "  59,\n",
       "  74,\n",
       "  73,\n",
       "  34,\n",
       "  0,\n",
       "  46,\n",
       "  52,\n",
       "  27,\n",
       "  20,\n",
       "  31,\n",
       "  94,\n",
       "  24,\n",
       "  6,\n",
       "  60,\n",
       "  98],\n",
       " [34,\n",
       "  60,\n",
       "  59,\n",
       "  74,\n",
       "  18,\n",
       "  78,\n",
       "  20,\n",
       "  54,\n",
       "  50,\n",
       "  31,\n",
       "  98,\n",
       "  99,\n",
       "  35,\n",
       "  17,\n",
       "  26,\n",
       "  9,\n",
       "  30,\n",
       "  27,\n",
       "  62,\n",
       "  58],\n",
       " [94,\n",
       "  18,\n",
       "  35,\n",
       "  78,\n",
       "  74,\n",
       "  54,\n",
       "  15,\n",
       "  6,\n",
       "  31,\n",
       "  0,\n",
       "  52,\n",
       "  58,\n",
       "  29,\n",
       "  59,\n",
       "  98,\n",
       "  60,\n",
       "  45,\n",
       "  36,\n",
       "  38,\n",
       "  21],\n",
       " [35,\n",
       "  18,\n",
       "  2,\n",
       "  78,\n",
       "  74,\n",
       "  15,\n",
       "  58,\n",
       "  31,\n",
       "  45,\n",
       "  49,\n",
       "  26,\n",
       "  98,\n",
       "  96,\n",
       "  94,\n",
       "  38,\n",
       "  21,\n",
       "  54,\n",
       "  59,\n",
       "  20,\n",
       "  16],\n",
       " [60,\n",
       "  74,\n",
       "  34,\n",
       "  78,\n",
       "  15,\n",
       "  37,\n",
       "  35,\n",
       "  2,\n",
       "  55,\n",
       "  98,\n",
       "  22,\n",
       "  20,\n",
       "  54,\n",
       "  38,\n",
       "  69,\n",
       "  62,\n",
       "  9,\n",
       "  56,\n",
       "  27,\n",
       "  73],\n",
       " [30,\n",
       "  20,\n",
       "  18,\n",
       "  78,\n",
       "  60,\n",
       "  99,\n",
       "  54,\n",
       "  34,\n",
       "  46,\n",
       "  27,\n",
       "  67,\n",
       "  55,\n",
       "  15,\n",
       "  14,\n",
       "  37,\n",
       "  59,\n",
       "  31,\n",
       "  61,\n",
       "  16,\n",
       "  35],\n",
       " [2,\n",
       "  18,\n",
       "  35,\n",
       "  73,\n",
       "  54,\n",
       "  20,\n",
       "  67,\n",
       "  22,\n",
       "  15,\n",
       "  98,\n",
       "  34,\n",
       "  99,\n",
       "  69,\n",
       "  74,\n",
       "  60,\n",
       "  78,\n",
       "  62,\n",
       "  30,\n",
       "  46,\n",
       "  77],\n",
       " [14, 6, 18, 21, 37, 46, 22, 16, 99, 92, 20, 7, 85, 82, 60, 67, 1, 42, 12, 26],\n",
       " [18,\n",
       "  67,\n",
       "  2,\n",
       "  35,\n",
       "  78,\n",
       "  31,\n",
       "  45,\n",
       "  73,\n",
       "  63,\n",
       "  6,\n",
       "  60,\n",
       "  15,\n",
       "  52,\n",
       "  20,\n",
       "  30,\n",
       "  24,\n",
       "  37,\n",
       "  54,\n",
       "  74,\n",
       "  69],\n",
       " [18,\n",
       "  60,\n",
       "  15,\n",
       "  78,\n",
       "  56,\n",
       "  94,\n",
       "  54,\n",
       "  99,\n",
       "  31,\n",
       "  74,\n",
       "  29,\n",
       "  61,\n",
       "  59,\n",
       "  12,\n",
       "  55,\n",
       "  35,\n",
       "  7,\n",
       "  93,\n",
       "  58,\n",
       "  45],\n",
       " [26,\n",
       "  59,\n",
       "  7,\n",
       "  85,\n",
       "  69,\n",
       "  54,\n",
       "  18,\n",
       "  31,\n",
       "  74,\n",
       "  24,\n",
       "  34,\n",
       "  50,\n",
       "  45,\n",
       "  49,\n",
       "  60,\n",
       "  15,\n",
       "  52,\n",
       "  56,\n",
       "  9,\n",
       "  35],\n",
       " [16,\n",
       "  99,\n",
       "  78,\n",
       "  20,\n",
       "  37,\n",
       "  94,\n",
       "  15,\n",
       "  51,\n",
       "  18,\n",
       "  55,\n",
       "  4,\n",
       "  60,\n",
       "  45,\n",
       "  22,\n",
       "  13,\n",
       "  6,\n",
       "  26,\n",
       "  56,\n",
       "  52,\n",
       "  30],\n",
       " [2,\n",
       "  20,\n",
       "  60,\n",
       "  18,\n",
       "  22,\n",
       "  16,\n",
       "  85,\n",
       "  31,\n",
       "  67,\n",
       "  62,\n",
       "  99,\n",
       "  15,\n",
       "  63,\n",
       "  82,\n",
       "  13,\n",
       "  78,\n",
       "  42,\n",
       "  30,\n",
       "  50,\n",
       "  54],\n",
       " [56,\n",
       "  69,\n",
       "  18,\n",
       "  67,\n",
       "  73,\n",
       "  45,\n",
       "  2,\n",
       "  42,\n",
       "  49,\n",
       "  15,\n",
       "  14,\n",
       "  31,\n",
       "  22,\n",
       "  61,\n",
       "  63,\n",
       "  74,\n",
       "  38,\n",
       "  30,\n",
       "  60,\n",
       "  54],\n",
       " [69,\n",
       "  34,\n",
       "  74,\n",
       "  54,\n",
       "  60,\n",
       "  59,\n",
       "  18,\n",
       "  99,\n",
       "  98,\n",
       "  25,\n",
       "  50,\n",
       "  56,\n",
       "  85,\n",
       "  7,\n",
       "  31,\n",
       "  13,\n",
       "  20,\n",
       "  49,\n",
       "  26,\n",
       "  2],\n",
       " [35,\n",
       "  18,\n",
       "  59,\n",
       "  78,\n",
       "  20,\n",
       "  54,\n",
       "  2,\n",
       "  34,\n",
       "  98,\n",
       "  22,\n",
       "  77,\n",
       "  31,\n",
       "  67,\n",
       "  6,\n",
       "  58,\n",
       "  99,\n",
       "  16,\n",
       "  13,\n",
       "  27,\n",
       "  52],\n",
       " [74,\n",
       "  18,\n",
       "  38,\n",
       "  37,\n",
       "  56,\n",
       "  15,\n",
       "  60,\n",
       "  69,\n",
       "  35,\n",
       "  34,\n",
       "  31,\n",
       "  2,\n",
       "  22,\n",
       "  73,\n",
       "  98,\n",
       "  49,\n",
       "  78,\n",
       "  62,\n",
       "  45,\n",
       "  54],\n",
       " [35,\n",
       "  78,\n",
       "  59,\n",
       "  24,\n",
       "  20,\n",
       "  18,\n",
       "  54,\n",
       "  15,\n",
       "  31,\n",
       "  2,\n",
       "  30,\n",
       "  99,\n",
       "  50,\n",
       "  34,\n",
       "  60,\n",
       "  98,\n",
       "  16,\n",
       "  45,\n",
       "  9,\n",
       "  74],\n",
       " [18,\n",
       "  30,\n",
       "  56,\n",
       "  15,\n",
       "  61,\n",
       "  37,\n",
       "  55,\n",
       "  42,\n",
       "  45,\n",
       "  78,\n",
       "  54,\n",
       "  12,\n",
       "  60,\n",
       "  14,\n",
       "  49,\n",
       "  31,\n",
       "  94,\n",
       "  20,\n",
       "  16,\n",
       "  35],\n",
       " [16, 20, 21, 99, 18, 85, 2, 60, 22, 58, 1, 6, 30, 92, 54, 51, 46, 82, 31, 7],\n",
       " [69, 56, 60, 54, 49, 18, 45, 28, 15, 99, 31, 89, 6, 8, 85, 93, 78, 16, 26, 4],\n",
       " [18,\n",
       "  35,\n",
       "  2,\n",
       "  15,\n",
       "  54,\n",
       "  22,\n",
       "  58,\n",
       "  49,\n",
       "  74,\n",
       "  31,\n",
       "  99,\n",
       "  45,\n",
       "  98,\n",
       "  51,\n",
       "  21,\n",
       "  38,\n",
       "  37,\n",
       "  20,\n",
       "  78,\n",
       "  6],\n",
       " [20,\n",
       "  99,\n",
       "  69,\n",
       "  60,\n",
       "  54,\n",
       "  34,\n",
       "  2,\n",
       "  22,\n",
       "  8,\n",
       "  59,\n",
       "  13,\n",
       "  25,\n",
       "  46,\n",
       "  18,\n",
       "  30,\n",
       "  50,\n",
       "  73,\n",
       "  82,\n",
       "  16,\n",
       "  27],\n",
       " [16, 99, 26, 13, 54, 49, 69, 21, 51, 20, 85, 18, 58, 2, 50, 60, 4, 59, 15, 7],\n",
       " [45,\n",
       "  18,\n",
       "  35,\n",
       "  49,\n",
       "  26,\n",
       "  15,\n",
       "  54,\n",
       "  78,\n",
       "  31,\n",
       "  94,\n",
       "  52,\n",
       "  2,\n",
       "  16,\n",
       "  93,\n",
       "  99,\n",
       "  59,\n",
       "  56,\n",
       "  74,\n",
       "  38,\n",
       "  51],\n",
       " [34,\n",
       "  60,\n",
       "  18,\n",
       "  1,\n",
       "  14,\n",
       "  62,\n",
       "  30,\n",
       "  56,\n",
       "  46,\n",
       "  74,\n",
       "  69,\n",
       "  67,\n",
       "  31,\n",
       "  25,\n",
       "  85,\n",
       "  12,\n",
       "  41,\n",
       "  20,\n",
       "  2,\n",
       "  15],\n",
       " [69,\n",
       "  56,\n",
       "  18,\n",
       "  45,\n",
       "  49,\n",
       "  15,\n",
       "  31,\n",
       "  93,\n",
       "  2,\n",
       "  60,\n",
       "  74,\n",
       "  38,\n",
       "  9,\n",
       "  35,\n",
       "  54,\n",
       "  61,\n",
       "  57,\n",
       "  42,\n",
       "  34,\n",
       "  78],\n",
       " [18,\n",
       "  74,\n",
       "  59,\n",
       "  34,\n",
       "  20,\n",
       "  35,\n",
       "  2,\n",
       "  67,\n",
       "  60,\n",
       "  54,\n",
       "  31,\n",
       "  78,\n",
       "  98,\n",
       "  46,\n",
       "  22,\n",
       "  73,\n",
       "  82,\n",
       "  77,\n",
       "  99,\n",
       "  69],\n",
       " [2,\n",
       "  16,\n",
       "  20,\n",
       "  54,\n",
       "  69,\n",
       "  45,\n",
       "  8,\n",
       "  35,\n",
       "  13,\n",
       "  59,\n",
       "  18,\n",
       "  49,\n",
       "  15,\n",
       "  30,\n",
       "  85,\n",
       "  78,\n",
       "  31,\n",
       "  67,\n",
       "  50,\n",
       "  52],\n",
       " [45, 63, 85, 54, 67, 7, 52, 30, 18, 59, 8, 16, 2, 35, 26, 31, 99, 20, 42, 49],\n",
       " [94,\n",
       "  78,\n",
       "  16,\n",
       "  37,\n",
       "  18,\n",
       "  54,\n",
       "  35,\n",
       "  99,\n",
       "  6,\n",
       "  20,\n",
       "  52,\n",
       "  55,\n",
       "  15,\n",
       "  59,\n",
       "  26,\n",
       "  22,\n",
       "  27,\n",
       "  49,\n",
       "  42,\n",
       "  77],\n",
       " [56, 99, 69, 85, 4, 49, 54, 7, 51, 60, 8, 16, 15, 30, 45, 87, 26, 93, 33, 20],\n",
       " [45,\n",
       "  16,\n",
       "  99,\n",
       "  49,\n",
       "  30,\n",
       "  42,\n",
       "  56,\n",
       "  54,\n",
       "  15,\n",
       "  52,\n",
       "  51,\n",
       "  18,\n",
       "  94,\n",
       "  7,\n",
       "  61,\n",
       "  12,\n",
       "  6,\n",
       "  85,\n",
       "  20,\n",
       "  26],\n",
       " [67,\n",
       "  18,\n",
       "  73,\n",
       "  2,\n",
       "  35,\n",
       "  78,\n",
       "  69,\n",
       "  8,\n",
       "  22,\n",
       "  15,\n",
       "  54,\n",
       "  60,\n",
       "  74,\n",
       "  37,\n",
       "  31,\n",
       "  28,\n",
       "  45,\n",
       "  55,\n",
       "  56,\n",
       "  38],\n",
       " [69, 8, 67, 56, 73, 2, 63, 30, 60, 18, 54, 9, 31, 15, 34, 45, 74, 62, 65, 20],\n",
       " [18,\n",
       "  61,\n",
       "  15,\n",
       "  60,\n",
       "  37,\n",
       "  49,\n",
       "  45,\n",
       "  31,\n",
       "  30,\n",
       "  12,\n",
       "  78,\n",
       "  42,\n",
       "  55,\n",
       "  54,\n",
       "  99,\n",
       "  69,\n",
       "  14,\n",
       "  57,\n",
       "  2,\n",
       "  34],\n",
       " [21,\n",
       "  18,\n",
       "  26,\n",
       "  99,\n",
       "  16,\n",
       "  35,\n",
       "  17,\n",
       "  34,\n",
       "  40,\n",
       "  59,\n",
       "  54,\n",
       "  31,\n",
       "  15,\n",
       "  60,\n",
       "  2,\n",
       "  49,\n",
       "  74,\n",
       "  46,\n",
       "  94,\n",
       "  57],\n",
       " [56,\n",
       "  37,\n",
       "  60,\n",
       "  4,\n",
       "  15,\n",
       "  18,\n",
       "  34,\n",
       "  22,\n",
       "  69,\n",
       "  99,\n",
       "  12,\n",
       "  38,\n",
       "  55,\n",
       "  49,\n",
       "  74,\n",
       "  31,\n",
       "  57,\n",
       "  14,\n",
       "  40,\n",
       "  2],\n",
       " [18,\n",
       "  74,\n",
       "  35,\n",
       "  37,\n",
       "  78,\n",
       "  34,\n",
       "  60,\n",
       "  15,\n",
       "  31,\n",
       "  94,\n",
       "  98,\n",
       "  54,\n",
       "  38,\n",
       "  0,\n",
       "  22,\n",
       "  2,\n",
       "  49,\n",
       "  59,\n",
       "  56,\n",
       "  55],\n",
       " [35,\n",
       "  2,\n",
       "  18,\n",
       "  74,\n",
       "  98,\n",
       "  22,\n",
       "  58,\n",
       "  15,\n",
       "  96,\n",
       "  78,\n",
       "  54,\n",
       "  37,\n",
       "  60,\n",
       "  34,\n",
       "  31,\n",
       "  20,\n",
       "  55,\n",
       "  69,\n",
       "  62,\n",
       "  49],\n",
       " [18,\n",
       "  34,\n",
       "  46,\n",
       "  99,\n",
       "  22,\n",
       "  62,\n",
       "  74,\n",
       "  2,\n",
       "  54,\n",
       "  69,\n",
       "  60,\n",
       "  20,\n",
       "  15,\n",
       "  25,\n",
       "  14,\n",
       "  31,\n",
       "  11,\n",
       "  30,\n",
       "  98,\n",
       "  21],\n",
       " [60, 85, 20, 69, 30, 18, 1, 99, 31, 24, 34, 9, 56, 15, 78, 59, 16, 2, 72, 67],\n",
       " [18,\n",
       "  35,\n",
       "  54,\n",
       "  46,\n",
       "  77,\n",
       "  67,\n",
       "  73,\n",
       "  59,\n",
       "  34,\n",
       "  52,\n",
       "  74,\n",
       "  0,\n",
       "  45,\n",
       "  94,\n",
       "  98,\n",
       "  7,\n",
       "  30,\n",
       "  15,\n",
       "  31,\n",
       "  25],\n",
       " [60,\n",
       "  18,\n",
       "  74,\n",
       "  34,\n",
       "  56,\n",
       "  31,\n",
       "  69,\n",
       "  15,\n",
       "  26,\n",
       "  9,\n",
       "  12,\n",
       "  37,\n",
       "  1,\n",
       "  78,\n",
       "  24,\n",
       "  49,\n",
       "  72,\n",
       "  17,\n",
       "  29,\n",
       "  85],\n",
       " [69, 49, 56, 85, 54, 45, 99, 51, 4, 7, 8, 93, 26, 2, 15, 13, 18, 87, 16, 31],\n",
       " [34, 60, 18, 62, 17, 31, 98, 59, 20, 50, 15, 69, 78, 46, 35, 2, 9, 54, 1, 58],\n",
       " [34,\n",
       "  60,\n",
       "  74,\n",
       "  18,\n",
       "  56,\n",
       "  15,\n",
       "  69,\n",
       "  62,\n",
       "  31,\n",
       "  38,\n",
       "  2,\n",
       "  98,\n",
       "  9,\n",
       "  57,\n",
       "  99,\n",
       "  54,\n",
       "  20,\n",
       "  49,\n",
       "  35,\n",
       "  30],\n",
       " [56,\n",
       "  37,\n",
       "  18,\n",
       "  15,\n",
       "  22,\n",
       "  99,\n",
       "  54,\n",
       "  38,\n",
       "  49,\n",
       "  55,\n",
       "  34,\n",
       "  4,\n",
       "  46,\n",
       "  60,\n",
       "  98,\n",
       "  51,\n",
       "  2,\n",
       "  73,\n",
       "  58,\n",
       "  69],\n",
       " [69,\n",
       "  18,\n",
       "  2,\n",
       "  99,\n",
       "  54,\n",
       "  49,\n",
       "  60,\n",
       "  16,\n",
       "  31,\n",
       "  56,\n",
       "  85,\n",
       "  15,\n",
       "  22,\n",
       "  45,\n",
       "  6,\n",
       "  51,\n",
       "  21,\n",
       "  52,\n",
       "  42,\n",
       "  59],\n",
       " [69,\n",
       "  2,\n",
       "  18,\n",
       "  56,\n",
       "  31,\n",
       "  60,\n",
       "  54,\n",
       "  22,\n",
       "  74,\n",
       "  49,\n",
       "  73,\n",
       "  99,\n",
       "  30,\n",
       "  34,\n",
       "  67,\n",
       "  38,\n",
       "  63,\n",
       "  42,\n",
       "  35,\n",
       "  8],\n",
       " [54, 45, 69, 49, 99, 8, 18, 2, 59, 51, 7, 56, 26, 15, 16, 52, 31, 85, 98, 20],\n",
       " [69,\n",
       "  56,\n",
       "  60,\n",
       "  18,\n",
       "  67,\n",
       "  14,\n",
       "  61,\n",
       "  6,\n",
       "  37,\n",
       "  54,\n",
       "  22,\n",
       "  85,\n",
       "  31,\n",
       "  30,\n",
       "  49,\n",
       "  15,\n",
       "  42,\n",
       "  99,\n",
       "  8,\n",
       "  34],\n",
       " [69, 2, 67, 63, 18, 45, 85, 60, 42, 31, 20, 16, 8, 49, 22, 54, 6, 30, 73, 15],\n",
       " [99, 7, 25, 54, 56, 59, 60, 30, 20, 46, 1, 50, 18, 87, 12, 15, 16, 31, 8, 13],\n",
       " [99,\n",
       "  34,\n",
       "  60,\n",
       "  56,\n",
       "  54,\n",
       "  18,\n",
       "  15,\n",
       "  26,\n",
       "  7,\n",
       "  59,\n",
       "  31,\n",
       "  50,\n",
       "  30,\n",
       "  85,\n",
       "  49,\n",
       "  69,\n",
       "  74,\n",
       "  12,\n",
       "  98,\n",
       "  1],\n",
       " [69,\n",
       "  8,\n",
       "  54,\n",
       "  85,\n",
       "  99,\n",
       "  45,\n",
       "  56,\n",
       "  63,\n",
       "  49,\n",
       "  89,\n",
       "  30,\n",
       "  60,\n",
       "  16,\n",
       "  67,\n",
       "  20,\n",
       "  15,\n",
       "  87,\n",
       "  2,\n",
       "  31,\n",
       "  13],\n",
       " [60,\n",
       "  18,\n",
       "  2,\n",
       "  22,\n",
       "  6,\n",
       "  37,\n",
       "  16,\n",
       "  78,\n",
       "  31,\n",
       "  21,\n",
       "  82,\n",
       "  67,\n",
       "  74,\n",
       "  99,\n",
       "  85,\n",
       "  34,\n",
       "  92,\n",
       "  27,\n",
       "  54,\n",
       "  59],\n",
       " [18,\n",
       "  78,\n",
       "  60,\n",
       "  31,\n",
       "  61,\n",
       "  6,\n",
       "  35,\n",
       "  24,\n",
       "  15,\n",
       "  59,\n",
       "  52,\n",
       "  54,\n",
       "  74,\n",
       "  20,\n",
       "  16,\n",
       "  34,\n",
       "  27,\n",
       "  67,\n",
       "  49,\n",
       "  30],\n",
       " [18,\n",
       "  78,\n",
       "  94,\n",
       "  26,\n",
       "  54,\n",
       "  15,\n",
       "  59,\n",
       "  31,\n",
       "  60,\n",
       "  99,\n",
       "  37,\n",
       "  52,\n",
       "  49,\n",
       "  16,\n",
       "  45,\n",
       "  74,\n",
       "  34,\n",
       "  20,\n",
       "  61,\n",
       "  7],\n",
       " [99,\n",
       "  26,\n",
       "  56,\n",
       "  49,\n",
       "  54,\n",
       "  15,\n",
       "  16,\n",
       "  45,\n",
       "  93,\n",
       "  60,\n",
       "  18,\n",
       "  7,\n",
       "  94,\n",
       "  51,\n",
       "  69,\n",
       "  31,\n",
       "  12,\n",
       "  13,\n",
       "  4,\n",
       "  78],\n",
       " [74, 69, 34, 67, 60, 31, 2, 73, 56, 62, 59, 22, 46, 54, 85, 14, 15, 7, 6, 30],\n",
       " [94,\n",
       "  99,\n",
       "  54,\n",
       "  78,\n",
       "  18,\n",
       "  16,\n",
       "  59,\n",
       "  20,\n",
       "  7,\n",
       "  15,\n",
       "  60,\n",
       "  35,\n",
       "  46,\n",
       "  30,\n",
       "  12,\n",
       "  55,\n",
       "  58,\n",
       "  52,\n",
       "  31,\n",
       "  27],\n",
       " [18,\n",
       "  78,\n",
       "  74,\n",
       "  35,\n",
       "  26,\n",
       "  60,\n",
       "  31,\n",
       "  15,\n",
       "  61,\n",
       "  37,\n",
       "  94,\n",
       "  34,\n",
       "  49,\n",
       "  24,\n",
       "  56,\n",
       "  45,\n",
       "  12,\n",
       "  23,\n",
       "  21,\n",
       "  57],\n",
       " [20,\n",
       "  16,\n",
       "  99,\n",
       "  54,\n",
       "  78,\n",
       "  59,\n",
       "  60,\n",
       "  18,\n",
       "  13,\n",
       "  35,\n",
       "  50,\n",
       "  31,\n",
       "  2,\n",
       "  15,\n",
       "  69,\n",
       "  27,\n",
       "  22,\n",
       "  85,\n",
       "  49,\n",
       "  52],\n",
       " [99,\n",
       "  54,\n",
       "  98,\n",
       "  20,\n",
       "  2,\n",
       "  50,\n",
       "  15,\n",
       "  58,\n",
       "  34,\n",
       "  35,\n",
       "  13,\n",
       "  69,\n",
       "  38,\n",
       "  49,\n",
       "  59,\n",
       "  96,\n",
       "  71,\n",
       "  56,\n",
       "  8,\n",
       "  51],\n",
       " [35,\n",
       "  18,\n",
       "  2,\n",
       "  78,\n",
       "  31,\n",
       "  20,\n",
       "  26,\n",
       "  21,\n",
       "  74,\n",
       "  16,\n",
       "  58,\n",
       "  15,\n",
       "  59,\n",
       "  45,\n",
       "  52,\n",
       "  42,\n",
       "  57,\n",
       "  60,\n",
       "  98,\n",
       "  49],\n",
       " [18,\n",
       "  49,\n",
       "  15,\n",
       "  35,\n",
       "  54,\n",
       "  45,\n",
       "  56,\n",
       "  2,\n",
       "  26,\n",
       "  31,\n",
       "  21,\n",
       "  58,\n",
       "  51,\n",
       "  40,\n",
       "  7,\n",
       "  38,\n",
       "  98,\n",
       "  74,\n",
       "  16,\n",
       "  34],\n",
       " [18,\n",
       "  94,\n",
       "  35,\n",
       "  30,\n",
       "  15,\n",
       "  45,\n",
       "  12,\n",
       "  78,\n",
       "  26,\n",
       "  7,\n",
       "  61,\n",
       "  14,\n",
       "  99,\n",
       "  52,\n",
       "  42,\n",
       "  31,\n",
       "  16,\n",
       "  46,\n",
       "  56,\n",
       "  49],\n",
       " [54, 45, 99, 56, 16, 85, 18, 49, 6, 7, 30, 52, 67, 69, 8, 61, 42, 26, 15, 60],\n",
       " [18,\n",
       "  35,\n",
       "  26,\n",
       "  21,\n",
       "  58,\n",
       "  2,\n",
       "  16,\n",
       "  31,\n",
       "  49,\n",
       "  15,\n",
       "  54,\n",
       "  99,\n",
       "  20,\n",
       "  45,\n",
       "  78,\n",
       "  59,\n",
       "  74,\n",
       "  52,\n",
       "  94,\n",
       "  40],\n",
       " [18,\n",
       "  60,\n",
       "  78,\n",
       "  37,\n",
       "  31,\n",
       "  74,\n",
       "  15,\n",
       "  54,\n",
       "  69,\n",
       "  34,\n",
       "  2,\n",
       "  22,\n",
       "  56,\n",
       "  67,\n",
       "  99,\n",
       "  49,\n",
       "  27,\n",
       "  61,\n",
       "  59,\n",
       "  6],\n",
       " [99,\n",
       "  16,\n",
       "  54,\n",
       "  30,\n",
       "  46,\n",
       "  20,\n",
       "  18,\n",
       "  7,\n",
       "  51,\n",
       "  42,\n",
       "  45,\n",
       "  35,\n",
       "  52,\n",
       "  2,\n",
       "  11,\n",
       "  49,\n",
       "  15,\n",
       "  85,\n",
       "  21,\n",
       "  14],\n",
       " [99, 54, 18, 16, 85, 69, 22, 6, 56, 7, 30, 46, 14, 49, 51, 34, 2, 67, 31, 21],\n",
       " [99,\n",
       "  54,\n",
       "  60,\n",
       "  20,\n",
       "  56,\n",
       "  15,\n",
       "  22,\n",
       "  18,\n",
       "  37,\n",
       "  30,\n",
       "  69,\n",
       "  49,\n",
       "  55,\n",
       "  78,\n",
       "  34,\n",
       "  27,\n",
       "  51,\n",
       "  2,\n",
       "  13,\n",
       "  85],\n",
       " [74,\n",
       "  18,\n",
       "  35,\n",
       "  98,\n",
       "  38,\n",
       "  60,\n",
       "  15,\n",
       "  31,\n",
       "  2,\n",
       "  69,\n",
       "  78,\n",
       "  54,\n",
       "  62,\n",
       "  22,\n",
       "  58,\n",
       "  49,\n",
       "  59,\n",
       "  73,\n",
       "  9,\n",
       "  37],\n",
       " [20,\n",
       "  99,\n",
       "  59,\n",
       "  54,\n",
       "  25,\n",
       "  85,\n",
       "  46,\n",
       "  16,\n",
       "  7,\n",
       "  67,\n",
       "  34,\n",
       "  82,\n",
       "  18,\n",
       "  77,\n",
       "  30,\n",
       "  87,\n",
       "  60,\n",
       "  27,\n",
       "  22,\n",
       "  6],\n",
       " [78,\n",
       "  18,\n",
       "  24,\n",
       "  45,\n",
       "  31,\n",
       "  2,\n",
       "  15,\n",
       "  94,\n",
       "  52,\n",
       "  67,\n",
       "  59,\n",
       "  30,\n",
       "  54,\n",
       "  16,\n",
       "  42,\n",
       "  61,\n",
       "  60,\n",
       "  23,\n",
       "  55,\n",
       "  63],\n",
       " [74,\n",
       "  34,\n",
       "  18,\n",
       "  60,\n",
       "  35,\n",
       "  78,\n",
       "  59,\n",
       "  67,\n",
       "  31,\n",
       "  98,\n",
       "  20,\n",
       "  54,\n",
       "  2,\n",
       "  77,\n",
       "  73,\n",
       "  27,\n",
       "  22,\n",
       "  36,\n",
       "  46,\n",
       "  0],\n",
       " [18,\n",
       "  37,\n",
       "  35,\n",
       "  15,\n",
       "  54,\n",
       "  49,\n",
       "  45,\n",
       "  56,\n",
       "  94,\n",
       "  22,\n",
       "  78,\n",
       "  55,\n",
       "  38,\n",
       "  99,\n",
       "  2,\n",
       "  31,\n",
       "  73,\n",
       "  51,\n",
       "  42,\n",
       "  60],\n",
       " [2,\n",
       "  18,\n",
       "  22,\n",
       "  60,\n",
       "  31,\n",
       "  20,\n",
       "  15,\n",
       "  34,\n",
       "  54,\n",
       "  99,\n",
       "  49,\n",
       "  21,\n",
       "  35,\n",
       "  56,\n",
       "  51,\n",
       "  98,\n",
       "  62,\n",
       "  73,\n",
       "  6,\n",
       "  16],\n",
       " [20,\n",
       "  2,\n",
       "  50,\n",
       "  16,\n",
       "  59,\n",
       "  85,\n",
       "  35,\n",
       "  23,\n",
       "  26,\n",
       "  63,\n",
       "  31,\n",
       "  24,\n",
       "  78,\n",
       "  30,\n",
       "  13,\n",
       "  54,\n",
       "  99,\n",
       "  60,\n",
       "  52,\n",
       "  21],\n",
       " [18,\n",
       "  60,\n",
       "  54,\n",
       "  15,\n",
       "  34,\n",
       "  99,\n",
       "  74,\n",
       "  31,\n",
       "  78,\n",
       "  26,\n",
       "  49,\n",
       "  56,\n",
       "  59,\n",
       "  35,\n",
       "  98,\n",
       "  2,\n",
       "  16,\n",
       "  7,\n",
       "  45,\n",
       "  38],\n",
       " [69,\n",
       "  56,\n",
       "  85,\n",
       "  7,\n",
       "  26,\n",
       "  60,\n",
       "  18,\n",
       "  49,\n",
       "  34,\n",
       "  54,\n",
       "  31,\n",
       "  74,\n",
       "  15,\n",
       "  45,\n",
       "  59,\n",
       "  61,\n",
       "  30,\n",
       "  9,\n",
       "  12,\n",
       "  52],\n",
       " [45, 49, 56, 54, 51, 93, 18, 15, 52, 7, 42, 8, 4, 69, 33, 16, 26, 6, 61, 89],\n",
       " [54,\n",
       "  18,\n",
       "  15,\n",
       "  49,\n",
       "  56,\n",
       "  35,\n",
       "  99,\n",
       "  78,\n",
       "  77,\n",
       "  94,\n",
       "  8,\n",
       "  34,\n",
       "  22,\n",
       "  31,\n",
       "  98,\n",
       "  69,\n",
       "  38,\n",
       "  37,\n",
       "  59,\n",
       "  52],\n",
       " [2,\n",
       "  78,\n",
       "  37,\n",
       "  35,\n",
       "  15,\n",
       "  67,\n",
       "  60,\n",
       "  31,\n",
       "  45,\n",
       "  42,\n",
       "  73,\n",
       "  55,\n",
       "  61,\n",
       "  22,\n",
       "  56,\n",
       "  30,\n",
       "  49,\n",
       "  54,\n",
       "  69,\n",
       "  38],\n",
       " [74,\n",
       "  60,\n",
       "  34,\n",
       "  18,\n",
       "  37,\n",
       "  15,\n",
       "  31,\n",
       "  78,\n",
       "  56,\n",
       "  62,\n",
       "  61,\n",
       "  38,\n",
       "  9,\n",
       "  69,\n",
       "  55,\n",
       "  98,\n",
       "  35,\n",
       "  22,\n",
       "  27,\n",
       "  76],\n",
       " [69,\n",
       "  67,\n",
       "  56,\n",
       "  18,\n",
       "  60,\n",
       "  30,\n",
       "  73,\n",
       "  2,\n",
       "  31,\n",
       "  34,\n",
       "  15,\n",
       "  63,\n",
       "  42,\n",
       "  61,\n",
       "  54,\n",
       "  62,\n",
       "  22,\n",
       "  85,\n",
       "  45,\n",
       "  49],\n",
       " [74,\n",
       "  18,\n",
       "  35,\n",
       "  34,\n",
       "  38,\n",
       "  62,\n",
       "  98,\n",
       "  2,\n",
       "  15,\n",
       "  31,\n",
       "  58,\n",
       "  60,\n",
       "  96,\n",
       "  78,\n",
       "  0,\n",
       "  57,\n",
       "  22,\n",
       "  73,\n",
       "  56,\n",
       "  65],\n",
       " [74,\n",
       "  2,\n",
       "  18,\n",
       "  69,\n",
       "  60,\n",
       "  34,\n",
       "  31,\n",
       "  98,\n",
       "  62,\n",
       "  15,\n",
       "  20,\n",
       "  78,\n",
       "  38,\n",
       "  73,\n",
       "  54,\n",
       "  67,\n",
       "  9,\n",
       "  59,\n",
       "  57,\n",
       "  58],\n",
       " [18,\n",
       "  69,\n",
       "  56,\n",
       "  60,\n",
       "  74,\n",
       "  37,\n",
       "  34,\n",
       "  15,\n",
       "  22,\n",
       "  49,\n",
       "  4,\n",
       "  31,\n",
       "  2,\n",
       "  38,\n",
       "  21,\n",
       "  99,\n",
       "  58,\n",
       "  54,\n",
       "  40,\n",
       "  51],\n",
       " [20,\n",
       "  34,\n",
       "  46,\n",
       "  99,\n",
       "  25,\n",
       "  60,\n",
       "  30,\n",
       "  54,\n",
       "  18,\n",
       "  59,\n",
       "  85,\n",
       "  1,\n",
       "  67,\n",
       "  27,\n",
       "  14,\n",
       "  7,\n",
       "  31,\n",
       "  82,\n",
       "  62,\n",
       "  69],\n",
       " [2, 35, 20, 73, 16, 18, 42, 78, 54, 6, 63, 8, 45, 52, 30, 82, 99, 77, 31, 59],\n",
       " [18,\n",
       "  35,\n",
       "  37,\n",
       "  94,\n",
       "  74,\n",
       "  0,\n",
       "  14,\n",
       "  78,\n",
       "  61,\n",
       "  15,\n",
       "  31,\n",
       "  34,\n",
       "  46,\n",
       "  54,\n",
       "  56,\n",
       "  60,\n",
       "  52,\n",
       "  45,\n",
       "  73,\n",
       "  6],\n",
       " [61,\n",
       "  18,\n",
       "  60,\n",
       "  26,\n",
       "  31,\n",
       "  56,\n",
       "  7,\n",
       "  72,\n",
       "  74,\n",
       "  34,\n",
       "  45,\n",
       "  30,\n",
       "  52,\n",
       "  78,\n",
       "  54,\n",
       "  59,\n",
       "  85,\n",
       "  14,\n",
       "  67,\n",
       "  12],\n",
       " [69, 74, 18, 60, 2, 15, 38, 31, 34, 37, 22, 49, 62, 9, 4, 98, 57, 35, 73, 93],\n",
       " [2,\n",
       "  18,\n",
       "  35,\n",
       "  22,\n",
       "  15,\n",
       "  78,\n",
       "  42,\n",
       "  20,\n",
       "  31,\n",
       "  73,\n",
       "  60,\n",
       "  62,\n",
       "  58,\n",
       "  96,\n",
       "  74,\n",
       "  16,\n",
       "  57,\n",
       "  6,\n",
       "  38,\n",
       "  49],\n",
       " [69, 67, 2, 18, 73, 74, 60, 31, 22, 56, 9, 45, 42, 8, 15, 35, 6, 28, 62, 49],\n",
       " [14, 11, 46, 42, 30, 67, 1, 62, 0, 66, 85, 18, 2, 73, 12, 92, 65, 21, 6, 20],\n",
       " [18,\n",
       "  74,\n",
       "  58,\n",
       "  21,\n",
       "  35,\n",
       "  34,\n",
       "  31,\n",
       "  2,\n",
       "  15,\n",
       "  40,\n",
       "  38,\n",
       "  60,\n",
       "  17,\n",
       "  0,\n",
       "  26,\n",
       "  62,\n",
       "  49,\n",
       "  37,\n",
       "  46,\n",
       "  29],\n",
       " [35,\n",
       "  18,\n",
       "  78,\n",
       "  45,\n",
       "  54,\n",
       "  94,\n",
       "  52,\n",
       "  15,\n",
       "  16,\n",
       "  59,\n",
       "  31,\n",
       "  2,\n",
       "  20,\n",
       "  49,\n",
       "  99,\n",
       "  67,\n",
       "  26,\n",
       "  30,\n",
       "  24,\n",
       "  42],\n",
       " [56,\n",
       "  4,\n",
       "  69,\n",
       "  14,\n",
       "  12,\n",
       "  1,\n",
       "  60,\n",
       "  22,\n",
       "  85,\n",
       "  34,\n",
       "  25,\n",
       "  37,\n",
       "  46,\n",
       "  30,\n",
       "  49,\n",
       "  41,\n",
       "  15,\n",
       "  18,\n",
       "  33,\n",
       "  51],\n",
       " [69,\n",
       "  2,\n",
       "  85,\n",
       "  18,\n",
       "  63,\n",
       "  74,\n",
       "  31,\n",
       "  45,\n",
       "  71,\n",
       "  62,\n",
       "  49,\n",
       "  7,\n",
       "  56,\n",
       "  15,\n",
       "  21,\n",
       "  26,\n",
       "  60,\n",
       "  50,\n",
       "  34,\n",
       "  35],\n",
       " [99,\n",
       "  54,\n",
       "  18,\n",
       "  56,\n",
       "  15,\n",
       "  49,\n",
       "  16,\n",
       "  20,\n",
       "  60,\n",
       "  45,\n",
       "  59,\n",
       "  69,\n",
       "  31,\n",
       "  34,\n",
       "  78,\n",
       "  7,\n",
       "  35,\n",
       "  51,\n",
       "  52,\n",
       "  8],\n",
       " [60,\n",
       "  78,\n",
       "  18,\n",
       "  99,\n",
       "  20,\n",
       "  37,\n",
       "  54,\n",
       "  16,\n",
       "  55,\n",
       "  34,\n",
       "  31,\n",
       "  22,\n",
       "  35,\n",
       "  2,\n",
       "  27,\n",
       "  59,\n",
       "  49,\n",
       "  56,\n",
       "  30,\n",
       "  94],\n",
       " [18,\n",
       "  99,\n",
       "  34,\n",
       "  60,\n",
       "  15,\n",
       "  54,\n",
       "  20,\n",
       "  46,\n",
       "  31,\n",
       "  2,\n",
       "  30,\n",
       "  74,\n",
       "  62,\n",
       "  22,\n",
       "  35,\n",
       "  69,\n",
       "  98,\n",
       "  12,\n",
       "  58,\n",
       "  57],\n",
       " [85, 59, 69, 20, 18, 24, 50, 74, 67, 34, 31, 2, 63, 54, 82, 7, 30, 72, 9, 78],\n",
       " [35,\n",
       "  18,\n",
       "  73,\n",
       "  54,\n",
       "  2,\n",
       "  67,\n",
       "  74,\n",
       "  46,\n",
       "  98,\n",
       "  34,\n",
       "  77,\n",
       "  0,\n",
       "  15,\n",
       "  22,\n",
       "  31,\n",
       "  20,\n",
       "  38,\n",
       "  59,\n",
       "  30,\n",
       "  37],\n",
       " [20,\n",
       "  35,\n",
       "  18,\n",
       "  98,\n",
       "  50,\n",
       "  59,\n",
       "  54,\n",
       "  78,\n",
       "  74,\n",
       "  58,\n",
       "  99,\n",
       "  34,\n",
       "  13,\n",
       "  60,\n",
       "  62,\n",
       "  22,\n",
       "  31,\n",
       "  69,\n",
       "  16,\n",
       "  15],\n",
       " [56,\n",
       "  18,\n",
       "  37,\n",
       "  38,\n",
       "  15,\n",
       "  69,\n",
       "  22,\n",
       "  2,\n",
       "  74,\n",
       "  62,\n",
       "  65,\n",
       "  73,\n",
       "  49,\n",
       "  4,\n",
       "  31,\n",
       "  34,\n",
       "  14,\n",
       "  61,\n",
       "  42,\n",
       "  55],\n",
       " [69, 18, 74, 56, 85, 31, 2, 92, 62, 1, 66, 12, 34, 49, 15, 6, 7, 40, 42, 63],\n",
       " [69, 56, 60, 1, 18, 34, 99, 12, 31, 26, 21, 49, 30, 15, 20, 74, 54, 92, 2, 4],\n",
       " [69, 60, 18, 56, 85, 67, 31, 61, 34, 6, 14, 36, 54, 15, 49, 37, 7, 72, 22, 9],\n",
       " [45, 56, 49, 18, 69, 54, 63, 42, 30, 52, 8, 67, 85, 15, 99, 7, 2, 16, 31, 6],\n",
       " [2,\n",
       "  35,\n",
       "  22,\n",
       "  18,\n",
       "  67,\n",
       "  20,\n",
       "  73,\n",
       "  78,\n",
       "  16,\n",
       "  31,\n",
       "  6,\n",
       "  60,\n",
       "  74,\n",
       "  63,\n",
       "  42,\n",
       "  82,\n",
       "  54,\n",
       "  15,\n",
       "  13,\n",
       "  96],\n",
       " [67,\n",
       "  24,\n",
       "  60,\n",
       "  18,\n",
       "  74,\n",
       "  31,\n",
       "  2,\n",
       "  69,\n",
       "  9,\n",
       "  78,\n",
       "  72,\n",
       "  63,\n",
       "  66,\n",
       "  61,\n",
       "  85,\n",
       "  82,\n",
       "  20,\n",
       "  42,\n",
       "  62,\n",
       "  34],\n",
       " [74,\n",
       "  18,\n",
       "  34,\n",
       "  35,\n",
       "  31,\n",
       "  60,\n",
       "  38,\n",
       "  0,\n",
       "  15,\n",
       "  2,\n",
       "  78,\n",
       "  46,\n",
       "  59,\n",
       "  67,\n",
       "  69,\n",
       "  56,\n",
       "  90,\n",
       "  37,\n",
       "  22,\n",
       "  17],\n",
       " [20,\n",
       "  2,\n",
       "  60,\n",
       "  18,\n",
       "  62,\n",
       "  34,\n",
       "  30,\n",
       "  31,\n",
       "  69,\n",
       "  99,\n",
       "  78,\n",
       "  67,\n",
       "  15,\n",
       "  54,\n",
       "  35,\n",
       "  59,\n",
       "  22,\n",
       "  50,\n",
       "  1,\n",
       "  46],\n",
       " [18,\n",
       "  74,\n",
       "  34,\n",
       "  35,\n",
       "  78,\n",
       "  60,\n",
       "  67,\n",
       "  31,\n",
       "  73,\n",
       "  24,\n",
       "  15,\n",
       "  98,\n",
       "  59,\n",
       "  0,\n",
       "  62,\n",
       "  54,\n",
       "  46,\n",
       "  27,\n",
       "  2,\n",
       "  55],\n",
       " [18, 60, 85, 69, 74, 31, 1, 34, 2, 67, 66, 30, 72, 62, 20, 7, 24, 26, 63, 59],\n",
       " [35,\n",
       "  18,\n",
       "  67,\n",
       "  73,\n",
       "  2,\n",
       "  54,\n",
       "  59,\n",
       "  45,\n",
       "  78,\n",
       "  77,\n",
       "  52,\n",
       "  31,\n",
       "  20,\n",
       "  30,\n",
       "  15,\n",
       "  42,\n",
       "  63,\n",
       "  46,\n",
       "  0,\n",
       "  34],\n",
       " [22,\n",
       "  2,\n",
       "  58,\n",
       "  18,\n",
       "  51,\n",
       "  4,\n",
       "  21,\n",
       "  99,\n",
       "  37,\n",
       "  20,\n",
       "  69,\n",
       "  62,\n",
       "  35,\n",
       "  16,\n",
       "  60,\n",
       "  15,\n",
       "  54,\n",
       "  40,\n",
       "  98,\n",
       "  96],\n",
       " [18,\n",
       "  62,\n",
       "  58,\n",
       "  35,\n",
       "  34,\n",
       "  2,\n",
       "  74,\n",
       "  20,\n",
       "  46,\n",
       "  98,\n",
       "  15,\n",
       "  31,\n",
       "  17,\n",
       "  78,\n",
       "  21,\n",
       "  96,\n",
       "  99,\n",
       "  22,\n",
       "  38,\n",
       "  30],\n",
       " [69,\n",
       "  2,\n",
       "  74,\n",
       "  62,\n",
       "  71,\n",
       "  34,\n",
       "  98,\n",
       "  8,\n",
       "  75,\n",
       "  25,\n",
       "  56,\n",
       "  63,\n",
       "  38,\n",
       "  73,\n",
       "  85,\n",
       "  18,\n",
       "  50,\n",
       "  49,\n",
       "  22,\n",
       "  54],\n",
       " [56,\n",
       "  15,\n",
       "  18,\n",
       "  69,\n",
       "  71,\n",
       "  49,\n",
       "  31,\n",
       "  2,\n",
       "  60,\n",
       "  62,\n",
       "  45,\n",
       "  34,\n",
       "  57,\n",
       "  38,\n",
       "  74,\n",
       "  9,\n",
       "  30,\n",
       "  54,\n",
       "  98,\n",
       "  40],\n",
       " [69,\n",
       "  60,\n",
       "  20,\n",
       "  85,\n",
       "  16,\n",
       "  2,\n",
       "  18,\n",
       "  99,\n",
       "  9,\n",
       "  78,\n",
       "  31,\n",
       "  30,\n",
       "  63,\n",
       "  15,\n",
       "  54,\n",
       "  13,\n",
       "  49,\n",
       "  23,\n",
       "  50,\n",
       "  67],\n",
       " [2,\n",
       "  18,\n",
       "  6,\n",
       "  16,\n",
       "  21,\n",
       "  22,\n",
       "  20,\n",
       "  42,\n",
       "  35,\n",
       "  51,\n",
       "  67,\n",
       "  99,\n",
       "  52,\n",
       "  92,\n",
       "  31,\n",
       "  54,\n",
       "  82,\n",
       "  85,\n",
       "  45,\n",
       "  49],\n",
       " [54,\n",
       "  99,\n",
       "  59,\n",
       "  20,\n",
       "  35,\n",
       "  16,\n",
       "  18,\n",
       "  78,\n",
       "  15,\n",
       "  26,\n",
       "  45,\n",
       "  49,\n",
       "  31,\n",
       "  2,\n",
       "  52,\n",
       "  30,\n",
       "  13,\n",
       "  60,\n",
       "  7,\n",
       "  98],\n",
       " [26, 7, 30, 85, 99, 16, 12, 45, 18, 56, 61, 54, 14, 52, 15, 42, 1, 49, 5, 20],\n",
       " [18,\n",
       "  69,\n",
       "  2,\n",
       "  60,\n",
       "  31,\n",
       "  56,\n",
       "  85,\n",
       "  45,\n",
       "  15,\n",
       "  49,\n",
       "  54,\n",
       "  42,\n",
       "  30,\n",
       "  99,\n",
       "  67,\n",
       "  20,\n",
       "  16,\n",
       "  63,\n",
       "  52,\n",
       "  6],\n",
       " [45, 26, 35, 52, 94, 18, 7, 16, 59, 30, 15, 42, 31, 78, 99, 63, 61, 6, 24, 2],\n",
       " [67, 30, 14, 73, 42, 18, 56, 46, 8, 65, 37, 61, 41, 60, 1, 2, 20, 62, 11, 55],\n",
       " [74,\n",
       "  18,\n",
       "  34,\n",
       "  60,\n",
       "  35,\n",
       "  31,\n",
       "  46,\n",
       "  2,\n",
       "  98,\n",
       "  59,\n",
       "  20,\n",
       "  22,\n",
       "  54,\n",
       "  15,\n",
       "  78,\n",
       "  0,\n",
       "  62,\n",
       "  37,\n",
       "  69,\n",
       "  17],\n",
       " [37,\n",
       "  18,\n",
       "  60,\n",
       "  99,\n",
       "  56,\n",
       "  15,\n",
       "  54,\n",
       "  22,\n",
       "  20,\n",
       "  55,\n",
       "  30,\n",
       "  34,\n",
       "  78,\n",
       "  14,\n",
       "  31,\n",
       "  12,\n",
       "  46,\n",
       "  49,\n",
       "  16,\n",
       "  2],\n",
       " [14, 18, 12, 37, 56, 61, 60, 1, 74, 34, 66, 6, 92, 69, 31, 67, 42, 5, 62, 85],\n",
       " [56,\n",
       "  69,\n",
       "  18,\n",
       "  49,\n",
       "  45,\n",
       "  74,\n",
       "  15,\n",
       "  31,\n",
       "  34,\n",
       "  38,\n",
       "  60,\n",
       "  54,\n",
       "  2,\n",
       "  7,\n",
       "  61,\n",
       "  12,\n",
       "  30,\n",
       "  35,\n",
       "  93,\n",
       "  98],\n",
       " [45, 54, 18, 35, 49, 15, 56, 52, 31, 94, 30, 99, 7, 8, 59, 61, 78, 42, 2, 67],\n",
       " [99,\n",
       "  46,\n",
       "  18,\n",
       "  22,\n",
       "  56,\n",
       "  58,\n",
       "  54,\n",
       "  2,\n",
       "  20,\n",
       "  34,\n",
       "  15,\n",
       "  62,\n",
       "  51,\n",
       "  4,\n",
       "  60,\n",
       "  30,\n",
       "  37,\n",
       "  49,\n",
       "  11,\n",
       "  40],\n",
       " [18,\n",
       "  35,\n",
       "  94,\n",
       "  15,\n",
       "  37,\n",
       "  78,\n",
       "  74,\n",
       "  54,\n",
       "  49,\n",
       "  21,\n",
       "  99,\n",
       "  31,\n",
       "  26,\n",
       "  34,\n",
       "  38,\n",
       "  40,\n",
       "  98,\n",
       "  0,\n",
       "  60,\n",
       "  57],\n",
       " [54,\n",
       "  45,\n",
       "  56,\n",
       "  15,\n",
       "  99,\n",
       "  49,\n",
       "  18,\n",
       "  8,\n",
       "  30,\n",
       "  35,\n",
       "  55,\n",
       "  31,\n",
       "  78,\n",
       "  2,\n",
       "  38,\n",
       "  69,\n",
       "  60,\n",
       "  52,\n",
       "  61,\n",
       "  20],\n",
       " [69,\n",
       "  18,\n",
       "  54,\n",
       "  56,\n",
       "  74,\n",
       "  15,\n",
       "  60,\n",
       "  38,\n",
       "  34,\n",
       "  22,\n",
       "  49,\n",
       "  35,\n",
       "  73,\n",
       "  98,\n",
       "  2,\n",
       "  31,\n",
       "  78,\n",
       "  28,\n",
       "  37,\n",
       "  45],\n",
       " [18,\n",
       "  60,\n",
       "  56,\n",
       "  15,\n",
       "  2,\n",
       "  42,\n",
       "  45,\n",
       "  69,\n",
       "  49,\n",
       "  37,\n",
       "  6,\n",
       "  61,\n",
       "  78,\n",
       "  30,\n",
       "  52,\n",
       "  54,\n",
       "  67,\n",
       "  35,\n",
       "  22,\n",
       "  12],\n",
       " [18,\n",
       "  35,\n",
       "  74,\n",
       "  59,\n",
       "  31,\n",
       "  7,\n",
       "  54,\n",
       "  34,\n",
       "  2,\n",
       "  26,\n",
       "  52,\n",
       "  20,\n",
       "  15,\n",
       "  45,\n",
       "  85,\n",
       "  69,\n",
       "  60,\n",
       "  30,\n",
       "  99,\n",
       "  98],\n",
       " [4,\n",
       "  22,\n",
       "  69,\n",
       "  51,\n",
       "  2,\n",
       "  21,\n",
       "  99,\n",
       "  58,\n",
       "  56,\n",
       "  71,\n",
       "  40,\n",
       "  18,\n",
       "  62,\n",
       "  49,\n",
       "  16,\n",
       "  92,\n",
       "  37,\n",
       "  33,\n",
       "  42,\n",
       "  85],\n",
       " [69, 71, 2, 56, 49, 85, 18, 15, 31, 62, 60, 45, 74, 57, 40, 63, 21, 9, 38, 4],\n",
       " [45, 56, 7, 49, 99, 18, 51, 15, 52, 30, 46, 35, 42, 8, 31, 69, 16, 26, 12, 2],\n",
       " [18,\n",
       "  34,\n",
       "  46,\n",
       "  54,\n",
       "  94,\n",
       "  78,\n",
       "  99,\n",
       "  60,\n",
       "  59,\n",
       "  15,\n",
       "  74,\n",
       "  14,\n",
       "  31,\n",
       "  20,\n",
       "  30,\n",
       "  37,\n",
       "  0,\n",
       "  55,\n",
       "  27,\n",
       "  7],\n",
       " [99,\n",
       "  60,\n",
       "  16,\n",
       "  13,\n",
       "  20,\n",
       "  54,\n",
       "  4,\n",
       "  58,\n",
       "  49,\n",
       "  22,\n",
       "  15,\n",
       "  2,\n",
       "  18,\n",
       "  51,\n",
       "  78,\n",
       "  56,\n",
       "  31,\n",
       "  50,\n",
       "  21,\n",
       "  34],\n",
       " [56,\n",
       "  18,\n",
       "  15,\n",
       "  49,\n",
       "  60,\n",
       "  99,\n",
       "  69,\n",
       "  54,\n",
       "  45,\n",
       "  31,\n",
       "  12,\n",
       "  34,\n",
       "  61,\n",
       "  7,\n",
       "  2,\n",
       "  42,\n",
       "  38,\n",
       "  74,\n",
       "  14,\n",
       "  57],\n",
       " [34,\n",
       "  74,\n",
       "  56,\n",
       "  69,\n",
       "  15,\n",
       "  60,\n",
       "  38,\n",
       "  98,\n",
       "  62,\n",
       "  2,\n",
       "  31,\n",
       "  54,\n",
       "  99,\n",
       "  49,\n",
       "  22,\n",
       "  35,\n",
       "  57,\n",
       "  20,\n",
       "  58,\n",
       "  9],\n",
       " [56,\n",
       "  54,\n",
       "  8,\n",
       "  69,\n",
       "  18,\n",
       "  34,\n",
       "  25,\n",
       "  49,\n",
       "  7,\n",
       "  67,\n",
       "  99,\n",
       "  15,\n",
       "  61,\n",
       "  30,\n",
       "  14,\n",
       "  31,\n",
       "  77,\n",
       "  52,\n",
       "  73,\n",
       "  59],\n",
       " [85,\n",
       "  56,\n",
       "  69,\n",
       "  2,\n",
       "  99,\n",
       "  71,\n",
       "  45,\n",
       "  30,\n",
       "  49,\n",
       "  18,\n",
       "  15,\n",
       "  7,\n",
       "  54,\n",
       "  20,\n",
       "  63,\n",
       "  16,\n",
       "  31,\n",
       "  51,\n",
       "  42,\n",
       "  62],\n",
       " [25,\n",
       "  14,\n",
       "  34,\n",
       "  46,\n",
       "  56,\n",
       "  69,\n",
       "  85,\n",
       "  7,\n",
       "  18,\n",
       "  11,\n",
       "  74,\n",
       "  1,\n",
       "  12,\n",
       "  99,\n",
       "  62,\n",
       "  30,\n",
       "  54,\n",
       "  22,\n",
       "  31,\n",
       "  67],\n",
       " [16,\n",
       "  26,\n",
       "  99,\n",
       "  45,\n",
       "  85,\n",
       "  54,\n",
       "  89,\n",
       "  49,\n",
       "  20,\n",
       "  13,\n",
       "  78,\n",
       "  52,\n",
       "  30,\n",
       "  7,\n",
       "  59,\n",
       "  24,\n",
       "  15,\n",
       "  79,\n",
       "  51,\n",
       "  87],\n",
       " [18,\n",
       "  35,\n",
       "  45,\n",
       "  2,\n",
       "  42,\n",
       "  16,\n",
       "  52,\n",
       "  78,\n",
       "  67,\n",
       "  31,\n",
       "  15,\n",
       "  54,\n",
       "  30,\n",
       "  49,\n",
       "  6,\n",
       "  20,\n",
       "  63,\n",
       "  99,\n",
       "  94,\n",
       "  59],\n",
       " [67,\n",
       "  30,\n",
       "  85,\n",
       "  62,\n",
       "  1,\n",
       "  20,\n",
       "  2,\n",
       "  46,\n",
       "  18,\n",
       "  25,\n",
       "  63,\n",
       "  69,\n",
       "  60,\n",
       "  14,\n",
       "  73,\n",
       "  82,\n",
       "  11,\n",
       "  41,\n",
       "  42,\n",
       "  34],\n",
       " [74,\n",
       "  67,\n",
       "  60,\n",
       "  34,\n",
       "  59,\n",
       "  20,\n",
       "  78,\n",
       "  35,\n",
       "  2,\n",
       "  31,\n",
       "  54,\n",
       "  27,\n",
       "  22,\n",
       "  73,\n",
       "  69,\n",
       "  82,\n",
       "  46,\n",
       "  6,\n",
       "  98,\n",
       "  99],\n",
       " [18,\n",
       "  56,\n",
       "  45,\n",
       "  15,\n",
       "  49,\n",
       "  35,\n",
       "  94,\n",
       "  37,\n",
       "  61,\n",
       "  31,\n",
       "  54,\n",
       "  42,\n",
       "  12,\n",
       "  40,\n",
       "  78,\n",
       "  99,\n",
       "  2,\n",
       "  55,\n",
       "  52,\n",
       "  57],\n",
       " [69, 2, 18, 60, 78, 31, 15, 35, 54, 74, 20, 22, 45, 49, 9, 73, 56, 63, 16, 6],\n",
       " [99,\n",
       "  16,\n",
       "  20,\n",
       "  30,\n",
       "  54,\n",
       "  18,\n",
       "  59,\n",
       "  35,\n",
       "  7,\n",
       "  46,\n",
       "  94,\n",
       "  52,\n",
       "  78,\n",
       "  26,\n",
       "  45,\n",
       "  15,\n",
       "  42,\n",
       "  31,\n",
       "  85,\n",
       "  2],\n",
       " [54,\n",
       "  77,\n",
       "  35,\n",
       "  78,\n",
       "  18,\n",
       "  74,\n",
       "  34,\n",
       "  59,\n",
       "  98,\n",
       "  60,\n",
       "  22,\n",
       "  99,\n",
       "  73,\n",
       "  15,\n",
       "  67,\n",
       "  8,\n",
       "  20,\n",
       "  28,\n",
       "  37,\n",
       "  27],\n",
       " [60,\n",
       "  99,\n",
       "  34,\n",
       "  18,\n",
       "  54,\n",
       "  20,\n",
       "  15,\n",
       "  69,\n",
       "  30,\n",
       "  37,\n",
       "  31,\n",
       "  78,\n",
       "  27,\n",
       "  22,\n",
       "  55,\n",
       "  12,\n",
       "  46,\n",
       "  49,\n",
       "  74,\n",
       "  59],\n",
       " [56, 4, 99, 49, 51, 45, 12, 33, 69, 85, 54, 7, 14, 8, 93, 30, 15, 16, 42, 40],\n",
       " [35,\n",
       "  18,\n",
       "  74,\n",
       "  58,\n",
       "  98,\n",
       "  38,\n",
       "  2,\n",
       "  15,\n",
       "  22,\n",
       "  37,\n",
       "  54,\n",
       "  94,\n",
       "  49,\n",
       "  78,\n",
       "  31,\n",
       "  34,\n",
       "  21,\n",
       "  96,\n",
       "  0,\n",
       "  60],\n",
       " [99,\n",
       "  54,\n",
       "  16,\n",
       "  7,\n",
       "  26,\n",
       "  49,\n",
       "  20,\n",
       "  60,\n",
       "  18,\n",
       "  15,\n",
       "  51,\n",
       "  30,\n",
       "  12,\n",
       "  69,\n",
       "  21,\n",
       "  4,\n",
       "  34,\n",
       "  59,\n",
       "  31,\n",
       "  13],\n",
       " [67,\n",
       "  69,\n",
       "  60,\n",
       "  18,\n",
       "  2,\n",
       "  73,\n",
       "  34,\n",
       "  22,\n",
       "  74,\n",
       "  20,\n",
       "  31,\n",
       "  62,\n",
       "  30,\n",
       "  82,\n",
       "  27,\n",
       "  8,\n",
       "  54,\n",
       "  85,\n",
       "  56,\n",
       "  63],\n",
       " [26,\n",
       "  18,\n",
       "  34,\n",
       "  59,\n",
       "  54,\n",
       "  99,\n",
       "  58,\n",
       "  35,\n",
       "  7,\n",
       "  21,\n",
       "  31,\n",
       "  15,\n",
       "  98,\n",
       "  17,\n",
       "  20,\n",
       "  49,\n",
       "  29,\n",
       "  78,\n",
       "  94,\n",
       "  46],\n",
       " [7,\n",
       "  85,\n",
       "  99,\n",
       "  54,\n",
       "  59,\n",
       "  30,\n",
       "  18,\n",
       "  25,\n",
       "  14,\n",
       "  16,\n",
       "  52,\n",
       "  20,\n",
       "  46,\n",
       "  56,\n",
       "  34,\n",
       "  87,\n",
       "  12,\n",
       "  27,\n",
       "  67,\n",
       "  31],\n",
       " [56,\n",
       "  54,\n",
       "  18,\n",
       "  99,\n",
       "  60,\n",
       "  15,\n",
       "  34,\n",
       "  49,\n",
       "  37,\n",
       "  69,\n",
       "  61,\n",
       "  31,\n",
       "  22,\n",
       "  78,\n",
       "  14,\n",
       "  45,\n",
       "  8,\n",
       "  74,\n",
       "  27,\n",
       "  46],\n",
       " [35,\n",
       "  18,\n",
       "  74,\n",
       "  67,\n",
       "  31,\n",
       "  34,\n",
       "  73,\n",
       "  59,\n",
       "  0,\n",
       "  98,\n",
       "  54,\n",
       "  78,\n",
       "  15,\n",
       "  52,\n",
       "  62,\n",
       "  45,\n",
       "  30,\n",
       "  20,\n",
       "  77,\n",
       "  60],\n",
       " [16, 99, 42, 30, 85, 20, 45, 2, 51, 54, 18, 63, 49, 56, 6, 52, 15, 4, 67, 7],\n",
       " [54, 59, 99, 67, 77, 7, 6, 26, 18, 16, 8, 87, 20, 25, 27, 45, 30, 78, 60, 69],\n",
       " [74,\n",
       "  18,\n",
       "  60,\n",
       "  78,\n",
       "  35,\n",
       "  69,\n",
       "  15,\n",
       "  31,\n",
       "  2,\n",
       "  98,\n",
       "  22,\n",
       "  37,\n",
       "  38,\n",
       "  54,\n",
       "  73,\n",
       "  55,\n",
       "  20,\n",
       "  62,\n",
       "  9,\n",
       "  67],\n",
       " [56, 22, 99, 18, 4, 46, 54, 37, 2, 15, 60, 30, 73, 20, 34, 51, 49, 42, 8, 11],\n",
       " [35,\n",
       "  18,\n",
       "  78,\n",
       "  54,\n",
       "  59,\n",
       "  94,\n",
       "  15,\n",
       "  45,\n",
       "  31,\n",
       "  52,\n",
       "  30,\n",
       "  34,\n",
       "  74,\n",
       "  20,\n",
       "  61,\n",
       "  60,\n",
       "  26,\n",
       "  99,\n",
       "  98,\n",
       "  49],\n",
       " [50,\n",
       "  71,\n",
       "  85,\n",
       "  26,\n",
       "  21,\n",
       "  2,\n",
       "  69,\n",
       "  7,\n",
       "  40,\n",
       "  63,\n",
       "  58,\n",
       "  17,\n",
       "  16,\n",
       "  62,\n",
       "  23,\n",
       "  51,\n",
       "  20,\n",
       "  49,\n",
       "  31,\n",
       "  59],\n",
       " [18,\n",
       "  7,\n",
       "  74,\n",
       "  26,\n",
       "  21,\n",
       "  56,\n",
       "  34,\n",
       "  40,\n",
       "  31,\n",
       "  49,\n",
       "  15,\n",
       "  46,\n",
       "  71,\n",
       "  35,\n",
       "  14,\n",
       "  54,\n",
       "  99,\n",
       "  2,\n",
       "  60,\n",
       "  62],\n",
       " [67, 18, 35, 73, 54, 78, 8, 74, 46, 2, 15, 34, 31, 22, 59, 30, 52, 0, 20, 45],\n",
       " [18,\n",
       "  45,\n",
       "  54,\n",
       "  16,\n",
       "  15,\n",
       "  99,\n",
       "  20,\n",
       "  49,\n",
       "  69,\n",
       "  30,\n",
       "  31,\n",
       "  35,\n",
       "  42,\n",
       "  60,\n",
       "  78,\n",
       "  52,\n",
       "  67,\n",
       "  63,\n",
       "  85,\n",
       "  22],\n",
       " [99, 54, 56, 16, 51, 49, 7, 69, 85, 4, 8, 18, 6, 45, 22, 20, 30, 87, 52, 15],\n",
       " [37, 18, 78, 60, 35, 22, 20, 55, 54, 15, 67, 31, 2, 6, 99, 94, 34, 16, 0, 74],\n",
       " [56, 54, 49, 4, 99, 22, 18, 37, 51, 69, 15, 33, 45, 38, 8, 6, 60, 93, 14, 34],\n",
       " [18,\n",
       "  60,\n",
       "  2,\n",
       "  69,\n",
       "  74,\n",
       "  15,\n",
       "  56,\n",
       "  34,\n",
       "  49,\n",
       "  99,\n",
       "  22,\n",
       "  54,\n",
       "  21,\n",
       "  20,\n",
       "  35,\n",
       "  37,\n",
       "  58,\n",
       "  38,\n",
       "  57,\n",
       "  51],\n",
       " [7,\n",
       "  85,\n",
       "  25,\n",
       "  99,\n",
       "  30,\n",
       "  26,\n",
       "  34,\n",
       "  46,\n",
       "  18,\n",
       "  54,\n",
       "  1,\n",
       "  56,\n",
       "  12,\n",
       "  59,\n",
       "  14,\n",
       "  20,\n",
       "  31,\n",
       "  60,\n",
       "  17,\n",
       "  15],\n",
       " [18,\n",
       "  2,\n",
       "  69,\n",
       "  15,\n",
       "  31,\n",
       "  56,\n",
       "  35,\n",
       "  67,\n",
       "  34,\n",
       "  73,\n",
       "  54,\n",
       "  22,\n",
       "  49,\n",
       "  62,\n",
       "  30,\n",
       "  38,\n",
       "  42,\n",
       "  98,\n",
       "  78,\n",
       "  20],\n",
       " [18,\n",
       "  60,\n",
       "  34,\n",
       "  74,\n",
       "  67,\n",
       "  2,\n",
       "  31,\n",
       "  20,\n",
       "  35,\n",
       "  54,\n",
       "  30,\n",
       "  73,\n",
       "  69,\n",
       "  15,\n",
       "  62,\n",
       "  22,\n",
       "  59,\n",
       "  27,\n",
       "  99,\n",
       "  56],\n",
       " [18,\n",
       "  78,\n",
       "  61,\n",
       "  94,\n",
       "  60,\n",
       "  26,\n",
       "  37,\n",
       "  54,\n",
       "  31,\n",
       "  35,\n",
       "  52,\n",
       "  59,\n",
       "  34,\n",
       "  99,\n",
       "  7,\n",
       "  30,\n",
       "  14,\n",
       "  24,\n",
       "  6,\n",
       "  56],\n",
       " [34,\n",
       "  56,\n",
       "  14,\n",
       "  1,\n",
       "  25,\n",
       "  60,\n",
       "  69,\n",
       "  30,\n",
       "  46,\n",
       "  62,\n",
       "  18,\n",
       "  85,\n",
       "  12,\n",
       "  74,\n",
       "  41,\n",
       "  67,\n",
       "  31,\n",
       "  99,\n",
       "  7,\n",
       "  11],\n",
       " [16, 45, 69, 49, 2, 54, 99, 18, 15, 20, 6, 4, 78, 42, 51, 56, 60, 22, 8, 28],\n",
       " [18,\n",
       "  60,\n",
       "  20,\n",
       "  2,\n",
       "  34,\n",
       "  62,\n",
       "  78,\n",
       "  37,\n",
       "  31,\n",
       "  22,\n",
       "  15,\n",
       "  99,\n",
       "  30,\n",
       "  35,\n",
       "  46,\n",
       "  1,\n",
       "  54,\n",
       "  67,\n",
       "  27,\n",
       "  42],\n",
       " [37,\n",
       "  78,\n",
       "  20,\n",
       "  99,\n",
       "  18,\n",
       "  55,\n",
       "  54,\n",
       "  22,\n",
       "  16,\n",
       "  46,\n",
       "  94,\n",
       "  60,\n",
       "  15,\n",
       "  30,\n",
       "  2,\n",
       "  27,\n",
       "  58,\n",
       "  0,\n",
       "  42,\n",
       "  73],\n",
       " [59, 18, 54, 67, 20, 7, 34, 46, 74, 35, 6, 99, 2, 85, 52, 77, 31, 82, 16, 22],\n",
       " [26,\n",
       "  69,\n",
       "  85,\n",
       "  45,\n",
       "  49,\n",
       "  56,\n",
       "  54,\n",
       "  31,\n",
       "  89,\n",
       "  60,\n",
       "  52,\n",
       "  6,\n",
       "  63,\n",
       "  59,\n",
       "  15,\n",
       "  16,\n",
       "  99,\n",
       "  61,\n",
       "  67,\n",
       "  30],\n",
       " [7,\n",
       "  26,\n",
       "  18,\n",
       "  45,\n",
       "  54,\n",
       "  52,\n",
       "  14,\n",
       "  56,\n",
       "  6,\n",
       "  49,\n",
       "  25,\n",
       "  59,\n",
       "  46,\n",
       "  85,\n",
       "  99,\n",
       "  12,\n",
       "  31,\n",
       "  94,\n",
       "  35,\n",
       "  30],\n",
       " [69, 60, 61, 18, 67, 54, 45, 8, 31, 15, 85, 30, 14, 6, 99, 42, 63, 52, 34, 7],\n",
       " [69,\n",
       "  56,\n",
       "  45,\n",
       "  18,\n",
       "  49,\n",
       "  15,\n",
       "  54,\n",
       "  60,\n",
       "  31,\n",
       "  61,\n",
       "  9,\n",
       "  74,\n",
       "  8,\n",
       "  34,\n",
       "  30,\n",
       "  24,\n",
       "  52,\n",
       "  67,\n",
       "  28,\n",
       "  89],\n",
       " [74,\n",
       "  18,\n",
       "  60,\n",
       "  21,\n",
       "  69,\n",
       "  2,\n",
       "  34,\n",
       "  62,\n",
       "  31,\n",
       "  58,\n",
       "  17,\n",
       "  22,\n",
       "  92,\n",
       "  37,\n",
       "  40,\n",
       "  15,\n",
       "  57,\n",
       "  38,\n",
       "  98,\n",
       "  56],\n",
       " [56,\n",
       "  12,\n",
       "  4,\n",
       "  99,\n",
       "  60,\n",
       "  49,\n",
       "  37,\n",
       "  15,\n",
       "  18,\n",
       "  61,\n",
       "  14,\n",
       "  1,\n",
       "  30,\n",
       "  54,\n",
       "  40,\n",
       "  31,\n",
       "  51,\n",
       "  85,\n",
       "  16,\n",
       "  55],\n",
       " [78,\n",
       "  35,\n",
       "  16,\n",
       "  18,\n",
       "  20,\n",
       "  59,\n",
       "  54,\n",
       "  94,\n",
       "  26,\n",
       "  99,\n",
       "  2,\n",
       "  60,\n",
       "  58,\n",
       "  31,\n",
       "  6,\n",
       "  15,\n",
       "  37,\n",
       "  52,\n",
       "  23,\n",
       "  22],\n",
       " [60,\n",
       "  56,\n",
       "  18,\n",
       "  15,\n",
       "  49,\n",
       "  31,\n",
       "  69,\n",
       "  85,\n",
       "  26,\n",
       "  61,\n",
       "  99,\n",
       "  30,\n",
       "  16,\n",
       "  45,\n",
       "  12,\n",
       "  78,\n",
       "  9,\n",
       "  1,\n",
       "  20,\n",
       "  57],\n",
       " [18,\n",
       "  60,\n",
       "  31,\n",
       "  15,\n",
       "  78,\n",
       "  61,\n",
       "  35,\n",
       "  42,\n",
       "  56,\n",
       "  45,\n",
       "  37,\n",
       "  26,\n",
       "  2,\n",
       "  49,\n",
       "  12,\n",
       "  52,\n",
       "  20,\n",
       "  54,\n",
       "  74,\n",
       "  16],\n",
       " [69, 60, 18, 56, 54, 85, 31, 2, 99, 15, 34, 49, 20, 67, 74, 22, 59, 8, 9, 45],\n",
       " [18,\n",
       "  74,\n",
       "  2,\n",
       "  35,\n",
       "  31,\n",
       "  59,\n",
       "  60,\n",
       "  67,\n",
       "  20,\n",
       "  24,\n",
       "  78,\n",
       "  69,\n",
       "  62,\n",
       "  54,\n",
       "  15,\n",
       "  98,\n",
       "  30,\n",
       "  52,\n",
       "  85,\n",
       "  63],\n",
       " [34,\n",
       "  54,\n",
       "  60,\n",
       "  74,\n",
       "  69,\n",
       "  18,\n",
       "  59,\n",
       "  99,\n",
       "  77,\n",
       "  36,\n",
       "  56,\n",
       "  98,\n",
       "  15,\n",
       "  78,\n",
       "  31,\n",
       "  22,\n",
       "  49,\n",
       "  28,\n",
       "  8,\n",
       "  27],\n",
       " [54,\n",
       "  59,\n",
       "  18,\n",
       "  26,\n",
       "  78,\n",
       "  94,\n",
       "  35,\n",
       "  99,\n",
       "  52,\n",
       "  7,\n",
       "  16,\n",
       "  20,\n",
       "  34,\n",
       "  31,\n",
       "  77,\n",
       "  60,\n",
       "  45,\n",
       "  30,\n",
       "  49,\n",
       "  27],\n",
       " [67,\n",
       "  62,\n",
       "  18,\n",
       "  2,\n",
       "  60,\n",
       "  73,\n",
       "  20,\n",
       "  30,\n",
       "  65,\n",
       "  34,\n",
       "  22,\n",
       "  31,\n",
       "  46,\n",
       "  42,\n",
       "  41,\n",
       "  78,\n",
       "  74,\n",
       "  1,\n",
       "  35,\n",
       "  15],\n",
       " [69, 8, 28, 67, 54, 2, 45, 22, 63, 16, 89, 18, 99, 6, 60, 4, 20, 85, 73, 13],\n",
       " [58,\n",
       "  60,\n",
       "  15,\n",
       "  99,\n",
       "  18,\n",
       "  56,\n",
       "  34,\n",
       "  22,\n",
       "  37,\n",
       "  4,\n",
       "  2,\n",
       "  69,\n",
       "  49,\n",
       "  98,\n",
       "  54,\n",
       "  20,\n",
       "  57,\n",
       "  31,\n",
       "  78,\n",
       "  62],\n",
       " [49, 45, 56, 54, 26, 69, 93, 7, 15, 51, 85, 89, 8, 16, 4, 28, 52, 18, 87, 13],\n",
       " [62, 71, 85, 2, 30, 46, 18, 11, 25, 99, 20, 7, 34, 56, 1, 69, 21, 31, 42, 40],\n",
       " [2,\n",
       "  35,\n",
       "  20,\n",
       "  54,\n",
       "  99,\n",
       "  98,\n",
       "  96,\n",
       "  62,\n",
       "  8,\n",
       "  73,\n",
       "  75,\n",
       "  15,\n",
       "  59,\n",
       "  34,\n",
       "  69,\n",
       "  22,\n",
       "  46,\n",
       "  58,\n",
       "  30,\n",
       "  38],\n",
       " [69, 56, 60, 34, 8, 54, 74, 22, 18, 99, 2, 4, 67, 25, 31, 49, 30, 38, 62, 20],\n",
       " [46,\n",
       "  20,\n",
       "  2,\n",
       "  18,\n",
       "  67,\n",
       "  99,\n",
       "  22,\n",
       "  11,\n",
       "  73,\n",
       "  62,\n",
       "  30,\n",
       "  25,\n",
       "  34,\n",
       "  54,\n",
       "  0,\n",
       "  82,\n",
       "  14,\n",
       "  59,\n",
       "  42,\n",
       "  85],\n",
       " [67,\n",
       "  18,\n",
       "  42,\n",
       "  6,\n",
       "  45,\n",
       "  63,\n",
       "  61,\n",
       "  2,\n",
       "  30,\n",
       "  78,\n",
       "  37,\n",
       "  52,\n",
       "  73,\n",
       "  31,\n",
       "  60,\n",
       "  16,\n",
       "  69,\n",
       "  66,\n",
       "  54,\n",
       "  22],\n",
       " [56,\n",
       "  7,\n",
       "  99,\n",
       "  45,\n",
       "  18,\n",
       "  15,\n",
       "  54,\n",
       "  12,\n",
       "  71,\n",
       "  30,\n",
       "  40,\n",
       "  51,\n",
       "  31,\n",
       "  52,\n",
       "  46,\n",
       "  42,\n",
       "  21,\n",
       "  16,\n",
       "  14,\n",
       "  57],\n",
       " [67, 18, 73, 74, 2, 54, 6, 34, 31, 77, 46, 69, 59, 60, 20, 52, 30, 0, 42, 82],\n",
       " [18,\n",
       "  56,\n",
       "  60,\n",
       "  69,\n",
       "  34,\n",
       "  74,\n",
       "  54,\n",
       "  31,\n",
       "  15,\n",
       "  61,\n",
       "  67,\n",
       "  14,\n",
       "  49,\n",
       "  37,\n",
       "  22,\n",
       "  99,\n",
       "  30,\n",
       "  73,\n",
       "  27,\n",
       "  8],\n",
       " [69, 60, 18, 15, 54, 99, 31, 34, 9, 49, 20, 30, 2, 74, 59, 45, 8, 24, 7, 78],\n",
       " [18,\n",
       "  20,\n",
       "  59,\n",
       "  34,\n",
       "  54,\n",
       "  74,\n",
       "  99,\n",
       "  60,\n",
       "  78,\n",
       "  31,\n",
       "  2,\n",
       "  98,\n",
       "  15,\n",
       "  58,\n",
       "  50,\n",
       "  26,\n",
       "  16,\n",
       "  17,\n",
       "  49,\n",
       "  46],\n",
       " [67,\n",
       "  60,\n",
       "  24,\n",
       "  69,\n",
       "  9,\n",
       "  18,\n",
       "  74,\n",
       "  2,\n",
       "  31,\n",
       "  63,\n",
       "  73,\n",
       "  34,\n",
       "  20,\n",
       "  78,\n",
       "  30,\n",
       "  72,\n",
       "  62,\n",
       "  59,\n",
       "  27,\n",
       "  82],\n",
       " [35,\n",
       "  18,\n",
       "  15,\n",
       "  2,\n",
       "  54,\n",
       "  45,\n",
       "  78,\n",
       "  31,\n",
       "  30,\n",
       "  99,\n",
       "  73,\n",
       "  49,\n",
       "  38,\n",
       "  98,\n",
       "  56,\n",
       "  67,\n",
       "  42,\n",
       "  60,\n",
       "  55,\n",
       "  8],\n",
       " [74,\n",
       "  18,\n",
       "  35,\n",
       "  2,\n",
       "  34,\n",
       "  98,\n",
       "  31,\n",
       "  38,\n",
       "  62,\n",
       "  69,\n",
       "  15,\n",
       "  73,\n",
       "  22,\n",
       "  78,\n",
       "  96,\n",
       "  58,\n",
       "  59,\n",
       "  54,\n",
       "  20,\n",
       "  67],\n",
       " [16, 85, 26, 99, 18, 7, 54, 20, 21, 6, 52, 2, 49, 69, 51, 31, 30, 60, 42, 15],\n",
       " [69, 8, 22, 60, 56, 28, 18, 67, 4, 74, 6, 2, 73, 31, 99, 15, 34, 36, 87, 45],\n",
       " [60,\n",
       "  69,\n",
       "  74,\n",
       "  62,\n",
       "  18,\n",
       "  56,\n",
       "  34,\n",
       "  1,\n",
       "  37,\n",
       "  2,\n",
       "  21,\n",
       "  31,\n",
       "  12,\n",
       "  15,\n",
       "  40,\n",
       "  57,\n",
       "  92,\n",
       "  14,\n",
       "  71,\n",
       "  58],\n",
       " [37,\n",
       "  18,\n",
       "  55,\n",
       "  22,\n",
       "  15,\n",
       "  60,\n",
       "  2,\n",
       "  62,\n",
       "  65,\n",
       "  73,\n",
       "  14,\n",
       "  42,\n",
       "  38,\n",
       "  78,\n",
       "  30,\n",
       "  61,\n",
       "  31,\n",
       "  0,\n",
       "  35,\n",
       "  57],\n",
       " [60, 18, 37, 78, 34, 31, 15, 35, 22, 2, 55, 61, 27, 20, 69, 36, 23, 24, 6, 9],\n",
       " [26,\n",
       "  18,\n",
       "  54,\n",
       "  58,\n",
       "  49,\n",
       "  59,\n",
       "  21,\n",
       "  74,\n",
       "  34,\n",
       "  60,\n",
       "  69,\n",
       "  15,\n",
       "  13,\n",
       "  7,\n",
       "  31,\n",
       "  29,\n",
       "  98,\n",
       "  35,\n",
       "  20,\n",
       "  51],\n",
       " [69,\n",
       "  60,\n",
       "  34,\n",
       "  85,\n",
       "  2,\n",
       "  22,\n",
       "  50,\n",
       "  99,\n",
       "  62,\n",
       "  82,\n",
       "  1,\n",
       "  74,\n",
       "  21,\n",
       "  18,\n",
       "  13,\n",
       "  25,\n",
       "  59,\n",
       "  67,\n",
       "  58,\n",
       "  54],\n",
       " [54,\n",
       "  59,\n",
       "  67,\n",
       "  8,\n",
       "  77,\n",
       "  35,\n",
       "  20,\n",
       "  99,\n",
       "  18,\n",
       "  25,\n",
       "  73,\n",
       "  30,\n",
       "  52,\n",
       "  2,\n",
       "  47,\n",
       "  63,\n",
       "  45,\n",
       "  28,\n",
       "  98,\n",
       "  31],\n",
       " [2,\n",
       "  45,\n",
       "  16,\n",
       "  35,\n",
       "  51,\n",
       "  49,\n",
       "  18,\n",
       "  99,\n",
       "  54,\n",
       "  15,\n",
       "  42,\n",
       "  20,\n",
       "  58,\n",
       "  96,\n",
       "  21,\n",
       "  22,\n",
       "  31,\n",
       "  4,\n",
       "  52,\n",
       "  57],\n",
       " [14, 18, 85, 1, 67, 6, 60, 92, 66, 21, 82, 69, 22, 2, 42, 20, 37, 30, 34, 12],\n",
       " [35,\n",
       "  78,\n",
       "  94,\n",
       "  18,\n",
       "  54,\n",
       "  59,\n",
       "  77,\n",
       "  37,\n",
       "  20,\n",
       "  55,\n",
       "  74,\n",
       "  98,\n",
       "  15,\n",
       "  31,\n",
       "  67,\n",
       "  52,\n",
       "  2,\n",
       "  24,\n",
       "  27,\n",
       "  73],\n",
       " [2,\n",
       "  18,\n",
       "  35,\n",
       "  78,\n",
       "  31,\n",
       "  74,\n",
       "  15,\n",
       "  60,\n",
       "  45,\n",
       "  69,\n",
       "  20,\n",
       "  67,\n",
       "  42,\n",
       "  23,\n",
       "  22,\n",
       "  73,\n",
       "  62,\n",
       "  49,\n",
       "  57,\n",
       "  63],\n",
       " [2,\n",
       "  20,\n",
       "  69,\n",
       "  18,\n",
       "  22,\n",
       "  60,\n",
       "  62,\n",
       "  34,\n",
       "  99,\n",
       "  74,\n",
       "  54,\n",
       "  67,\n",
       "  35,\n",
       "  73,\n",
       "  98,\n",
       "  31,\n",
       "  59,\n",
       "  50,\n",
       "  13,\n",
       "  15],\n",
       " [35,\n",
       "  94,\n",
       "  18,\n",
       "  45,\n",
       "  78,\n",
       "  52,\n",
       "  15,\n",
       "  31,\n",
       "  26,\n",
       "  54,\n",
       "  49,\n",
       "  61,\n",
       "  59,\n",
       "  2,\n",
       "  42,\n",
       "  38,\n",
       "  24,\n",
       "  98,\n",
       "  0,\n",
       "  37],\n",
       " [16,\n",
       "  99,\n",
       "  20,\n",
       "  18,\n",
       "  2,\n",
       "  54,\n",
       "  78,\n",
       "  15,\n",
       "  60,\n",
       "  30,\n",
       "  22,\n",
       "  49,\n",
       "  31,\n",
       "  45,\n",
       "  42,\n",
       "  69,\n",
       "  35,\n",
       "  37,\n",
       "  67,\n",
       "  55],\n",
       " [74,\n",
       "  34,\n",
       "  59,\n",
       "  18,\n",
       "  60,\n",
       "  98,\n",
       "  50,\n",
       "  54,\n",
       "  35,\n",
       "  26,\n",
       "  58,\n",
       "  20,\n",
       "  99,\n",
       "  31,\n",
       "  78,\n",
       "  17,\n",
       "  15,\n",
       "  13,\n",
       "  69,\n",
       "  2],\n",
       " [35,\n",
       "  78,\n",
       "  18,\n",
       "  54,\n",
       "  45,\n",
       "  94,\n",
       "  15,\n",
       "  59,\n",
       "  52,\n",
       "  67,\n",
       "  31,\n",
       "  20,\n",
       "  30,\n",
       "  24,\n",
       "  99,\n",
       "  77,\n",
       "  2,\n",
       "  55,\n",
       "  16,\n",
       "  49],\n",
       " [26,\n",
       "  18,\n",
       "  94,\n",
       "  7,\n",
       "  99,\n",
       "  54,\n",
       "  49,\n",
       "  15,\n",
       "  56,\n",
       "  12,\n",
       "  16,\n",
       "  78,\n",
       "  60,\n",
       "  52,\n",
       "  59,\n",
       "  31,\n",
       "  21,\n",
       "  37,\n",
       "  61,\n",
       "  29],\n",
       " [56, 8, 99, 30, 45, 54, 49, 85, 7, 25, 15, 71, 51, 63, 4, 18, 12, 42, 2, 75],\n",
       " [38,\n",
       "  15,\n",
       "  18,\n",
       "  74,\n",
       "  34,\n",
       "  60,\n",
       "  69,\n",
       "  55,\n",
       "  98,\n",
       "  37,\n",
       "  62,\n",
       "  31,\n",
       "  73,\n",
       "  54,\n",
       "  22,\n",
       "  65,\n",
       "  35,\n",
       "  49,\n",
       "  61,\n",
       "  78],\n",
       " [2,\n",
       "  18,\n",
       "  45,\n",
       "  69,\n",
       "  15,\n",
       "  49,\n",
       "  31,\n",
       "  74,\n",
       "  54,\n",
       "  56,\n",
       "  38,\n",
       "  98,\n",
       "  63,\n",
       "  62,\n",
       "  20,\n",
       "  99,\n",
       "  57,\n",
       "  42,\n",
       "  30,\n",
       "  52],\n",
       " [16,\n",
       "  20,\n",
       "  2,\n",
       "  99,\n",
       "  35,\n",
       "  54,\n",
       "  30,\n",
       "  45,\n",
       "  18,\n",
       "  63,\n",
       "  85,\n",
       "  59,\n",
       "  42,\n",
       "  52,\n",
       "  50,\n",
       "  51,\n",
       "  15,\n",
       "  7,\n",
       "  13,\n",
       "  31],\n",
       " [37,\n",
       "  18,\n",
       "  74,\n",
       "  34,\n",
       "  60,\n",
       "  36,\n",
       "  94,\n",
       "  0,\n",
       "  6,\n",
       "  14,\n",
       "  29,\n",
       "  22,\n",
       "  26,\n",
       "  35,\n",
       "  31,\n",
       "  58,\n",
       "  21,\n",
       "  27,\n",
       "  15,\n",
       "  54],\n",
       " [35,\n",
       "  2,\n",
       "  20,\n",
       "  59,\n",
       "  67,\n",
       "  18,\n",
       "  54,\n",
       "  78,\n",
       "  73,\n",
       "  74,\n",
       "  98,\n",
       "  16,\n",
       "  77,\n",
       "  22,\n",
       "  31,\n",
       "  99,\n",
       "  60,\n",
       "  34,\n",
       "  13,\n",
       "  69],\n",
       " [2,\n",
       "  62,\n",
       "  22,\n",
       "  46,\n",
       "  99,\n",
       "  20,\n",
       "  73,\n",
       "  18,\n",
       "  54,\n",
       "  69,\n",
       "  98,\n",
       "  34,\n",
       "  35,\n",
       "  75,\n",
       "  11,\n",
       "  58,\n",
       "  74,\n",
       "  25,\n",
       "  51,\n",
       "  8],\n",
       " [22, 69, 67, 60, 18, 20, 2, 54, 99, 73, 8, 37, 34, 27, 6, 31, 30, 15, 56, 82],\n",
       " [18,\n",
       "  31,\n",
       "  74,\n",
       "  35,\n",
       "  15,\n",
       "  2,\n",
       "  60,\n",
       "  45,\n",
       "  69,\n",
       "  24,\n",
       "  78,\n",
       "  34,\n",
       "  49,\n",
       "  54,\n",
       "  56,\n",
       "  59,\n",
       "  9,\n",
       "  30,\n",
       "  52,\n",
       "  26],\n",
       " [46,\n",
       "  18,\n",
       "  54,\n",
       "  25,\n",
       "  56,\n",
       "  34,\n",
       "  75,\n",
       "  35,\n",
       "  30,\n",
       "  11,\n",
       "  15,\n",
       "  95,\n",
       "  99,\n",
       "  98,\n",
       "  7,\n",
       "  45,\n",
       "  38,\n",
       "  62,\n",
       "  73,\n",
       "  74],\n",
       " [99,\n",
       "  54,\n",
       "  26,\n",
       "  50,\n",
       "  58,\n",
       "  13,\n",
       "  49,\n",
       "  69,\n",
       "  59,\n",
       "  15,\n",
       "  20,\n",
       "  18,\n",
       "  34,\n",
       "  16,\n",
       "  98,\n",
       "  60,\n",
       "  51,\n",
       "  2,\n",
       "  31,\n",
       "  21],\n",
       " [69,\n",
       "  54,\n",
       "  2,\n",
       "  56,\n",
       "  18,\n",
       "  49,\n",
       "  15,\n",
       "  22,\n",
       "  45,\n",
       "  73,\n",
       "  99,\n",
       "  35,\n",
       "  67,\n",
       "  38,\n",
       "  31,\n",
       "  74,\n",
       "  60,\n",
       "  28,\n",
       "  98,\n",
       "  34],\n",
       " [2,\n",
       "  20,\n",
       "  18,\n",
       "  60,\n",
       "  69,\n",
       "  62,\n",
       "  30,\n",
       "  16,\n",
       "  15,\n",
       "  31,\n",
       "  42,\n",
       "  99,\n",
       "  22,\n",
       "  67,\n",
       "  78,\n",
       "  35,\n",
       "  54,\n",
       "  57,\n",
       "  63,\n",
       "  73],\n",
       " [74,\n",
       "  34,\n",
       "  60,\n",
       "  18,\n",
       "  69,\n",
       "  31,\n",
       "  9,\n",
       "  24,\n",
       "  78,\n",
       "  15,\n",
       "  59,\n",
       "  36,\n",
       "  62,\n",
       "  98,\n",
       "  56,\n",
       "  27,\n",
       "  1,\n",
       "  61,\n",
       "  20,\n",
       "  72],\n",
       " [74, 34, 69, 60, 62, 31, 56, 98, 2, 17, 15, 46, 38, 7, 59, 21, 1, 85, 22, 12],\n",
       " [18,\n",
       "  54,\n",
       "  35,\n",
       "  34,\n",
       "  74,\n",
       "  59,\n",
       "  98,\n",
       "  99,\n",
       "  46,\n",
       "  77,\n",
       "  15,\n",
       "  31,\n",
       "  94,\n",
       "  49,\n",
       "  22,\n",
       "  7,\n",
       "  78,\n",
       "  52,\n",
       "  60,\n",
       "  2],\n",
       " [2,\n",
       "  98,\n",
       "  69,\n",
       "  34,\n",
       "  74,\n",
       "  38,\n",
       "  22,\n",
       "  62,\n",
       "  58,\n",
       "  15,\n",
       "  60,\n",
       "  54,\n",
       "  20,\n",
       "  18,\n",
       "  99,\n",
       "  35,\n",
       "  13,\n",
       "  73,\n",
       "  56,\n",
       "  31],\n",
       " [18,\n",
       "  56,\n",
       "  69,\n",
       "  74,\n",
       "  31,\n",
       "  34,\n",
       "  60,\n",
       "  7,\n",
       "  15,\n",
       "  14,\n",
       "  49,\n",
       "  2,\n",
       "  12,\n",
       "  30,\n",
       "  45,\n",
       "  85,\n",
       "  54,\n",
       "  42,\n",
       "  67,\n",
       "  62],\n",
       " [18,\n",
       "  62,\n",
       "  30,\n",
       "  2,\n",
       "  67,\n",
       "  20,\n",
       "  60,\n",
       "  46,\n",
       "  73,\n",
       "  34,\n",
       "  42,\n",
       "  31,\n",
       "  1,\n",
       "  15,\n",
       "  14,\n",
       "  65,\n",
       "  22,\n",
       "  69,\n",
       "  56,\n",
       "  99],\n",
       " [56, 45, 18, 49, 2, 60, 9, 61, 93, 31, 15, 63, 4, 42, 74, 67, 38, 28, 6, 37],\n",
       " [18,\n",
       "  2,\n",
       "  15,\n",
       "  30,\n",
       "  60,\n",
       "  56,\n",
       "  31,\n",
       "  69,\n",
       "  54,\n",
       "  99,\n",
       "  62,\n",
       "  67,\n",
       "  42,\n",
       "  78,\n",
       "  45,\n",
       "  34,\n",
       "  49,\n",
       "  73,\n",
       "  9,\n",
       "  57],\n",
       " [18,\n",
       "  2,\n",
       "  54,\n",
       "  20,\n",
       "  59,\n",
       "  7,\n",
       "  69,\n",
       "  30,\n",
       "  99,\n",
       "  35,\n",
       "  31,\n",
       "  63,\n",
       "  45,\n",
       "  67,\n",
       "  34,\n",
       "  52,\n",
       "  15,\n",
       "  25,\n",
       "  74,\n",
       "  16],\n",
       " [18,\n",
       "  78,\n",
       "  60,\n",
       "  22,\n",
       "  37,\n",
       "  2,\n",
       "  54,\n",
       "  35,\n",
       "  6,\n",
       "  31,\n",
       "  49,\n",
       "  15,\n",
       "  20,\n",
       "  67,\n",
       "  28,\n",
       "  16,\n",
       "  59,\n",
       "  99,\n",
       "  13,\n",
       "  73],\n",
       " [67,\n",
       "  18,\n",
       "  74,\n",
       "  59,\n",
       "  35,\n",
       "  2,\n",
       "  73,\n",
       "  20,\n",
       "  54,\n",
       "  31,\n",
       "  30,\n",
       "  25,\n",
       "  62,\n",
       "  46,\n",
       "  98,\n",
       "  63,\n",
       "  60,\n",
       "  69,\n",
       "  24,\n",
       "  52],\n",
       " [60,\n",
       "  74,\n",
       "  34,\n",
       "  18,\n",
       "  58,\n",
       "  62,\n",
       "  2,\n",
       "  50,\n",
       "  17,\n",
       "  22,\n",
       "  31,\n",
       "  23,\n",
       "  21,\n",
       "  78,\n",
       "  1,\n",
       "  37,\n",
       "  98,\n",
       "  15,\n",
       "  57,\n",
       "  13],\n",
       " [18,\n",
       "  67,\n",
       "  78,\n",
       "  54,\n",
       "  59,\n",
       "  35,\n",
       "  24,\n",
       "  52,\n",
       "  60,\n",
       "  31,\n",
       "  20,\n",
       "  6,\n",
       "  30,\n",
       "  61,\n",
       "  45,\n",
       "  27,\n",
       "  16,\n",
       "  15,\n",
       "  26,\n",
       "  77],\n",
       " [45, 54, 18, 35, 49, 52, 2, 7, 31, 15, 67, 59, 26, 69, 99, 63, 56, 6, 42, 28],\n",
       " [34, 56, 60, 74, 12, 18, 15, 7, 61, 25, 54, 99, 69, 31, 1, 32, 26, 30, 14, 9],\n",
       " [2, 35, 67, 18, 59, 20, 54, 69, 73, 22, 74, 77, 16, 31, 99, 78, 6, 52, 8, 82],\n",
       " [74,\n",
       "  18,\n",
       "  35,\n",
       "  34,\n",
       "  31,\n",
       "  15,\n",
       "  56,\n",
       "  38,\n",
       "  98,\n",
       "  60,\n",
       "  49,\n",
       "  54,\n",
       "  45,\n",
       "  69,\n",
       "  61,\n",
       "  52,\n",
       "  2,\n",
       "  59,\n",
       "  73,\n",
       "  7],\n",
       " [69, 56, 2, 49, 4, 18, 51, 22, 6, 42, 31, 21, 54, 93, 33, 99, 73, 63, 40, 85],\n",
       " [18,\n",
       "  26,\n",
       "  7,\n",
       "  21,\n",
       "  99,\n",
       "  46,\n",
       "  35,\n",
       "  59,\n",
       "  34,\n",
       "  58,\n",
       "  74,\n",
       "  52,\n",
       "  94,\n",
       "  31,\n",
       "  0,\n",
       "  40,\n",
       "  16,\n",
       "  49,\n",
       "  17,\n",
       "  20],\n",
       " [18,\n",
       "  74,\n",
       "  34,\n",
       "  35,\n",
       "  60,\n",
       "  78,\n",
       "  20,\n",
       "  31,\n",
       "  15,\n",
       "  62,\n",
       "  59,\n",
       "  98,\n",
       "  54,\n",
       "  30,\n",
       "  46,\n",
       "  99,\n",
       "  17,\n",
       "  57,\n",
       "  24,\n",
       "  27],\n",
       " [18,\n",
       "  2,\n",
       "  67,\n",
       "  74,\n",
       "  31,\n",
       "  60,\n",
       "  69,\n",
       "  6,\n",
       "  42,\n",
       "  63,\n",
       "  20,\n",
       "  66,\n",
       "  35,\n",
       "  62,\n",
       "  82,\n",
       "  92,\n",
       "  21,\n",
       "  73,\n",
       "  78,\n",
       "  22],\n",
       " [99, 37, 22, 16, 20, 46, 14, 54, 18, 60, 58, 6, 56, 12, 34, 4, 51, 1, 30, 27],\n",
       " [45, 49, 99, 26, 16, 51, 7, 54, 85, 4, 6, 69, 52, 18, 15, 93, 42, 89, 30, 33],\n",
       " [99,\n",
       "  54,\n",
       "  34,\n",
       "  7,\n",
       "  18,\n",
       "  26,\n",
       "  56,\n",
       "  59,\n",
       "  15,\n",
       "  20,\n",
       "  49,\n",
       "  46,\n",
       "  60,\n",
       "  30,\n",
       "  31,\n",
       "  85,\n",
       "  25,\n",
       "  98,\n",
       "  16,\n",
       "  50],\n",
       " [18,\n",
       "  56,\n",
       "  49,\n",
       "  45,\n",
       "  15,\n",
       "  31,\n",
       "  69,\n",
       "  60,\n",
       "  12,\n",
       "  7,\n",
       "  14,\n",
       "  54,\n",
       "  42,\n",
       "  6,\n",
       "  74,\n",
       "  99,\n",
       "  52,\n",
       "  37,\n",
       "  26,\n",
       "  30],\n",
       " [69, 60, 56, 4, 49, 18, 15, 21, 31, 26, 74, 34, 12, 99, 9, 1, 40, 57, 92, 51],\n",
       " [18,\n",
       "  2,\n",
       "  15,\n",
       "  56,\n",
       "  69,\n",
       "  74,\n",
       "  49,\n",
       "  60,\n",
       "  31,\n",
       "  38,\n",
       "  62,\n",
       "  57,\n",
       "  71,\n",
       "  40,\n",
       "  21,\n",
       "  58,\n",
       "  35,\n",
       "  34,\n",
       "  4,\n",
       "  45],\n",
       " [2, 18, 62, 74, 60, 69, 20, 31, 34, 67, 21, 1, 35, 82, 92, 73, 42, 23, 37, 0],\n",
       " [56,\n",
       "  74,\n",
       "  18,\n",
       "  69,\n",
       "  67,\n",
       "  73,\n",
       "  14,\n",
       "  31,\n",
       "  45,\n",
       "  34,\n",
       "  15,\n",
       "  60,\n",
       "  38,\n",
       "  65,\n",
       "  63,\n",
       "  62,\n",
       "  30,\n",
       "  42,\n",
       "  49,\n",
       "  9],\n",
       " [20,\n",
       "  2,\n",
       "  35,\n",
       "  54,\n",
       "  59,\n",
       "  67,\n",
       "  78,\n",
       "  99,\n",
       "  16,\n",
       "  18,\n",
       "  50,\n",
       "  73,\n",
       "  62,\n",
       "  98,\n",
       "  30,\n",
       "  96,\n",
       "  13,\n",
       "  8,\n",
       "  22,\n",
       "  31],\n",
       " [69, 67, 56, 18, 60, 61, 74, 31, 8, 15, 54, 37, 34, 22, 45, 30, 14, 2, 49, 6],\n",
       " [56, 71, 99, 51, 49, 85, 4, 40, 7, 21, 69, 45, 12, 15, 16, 18, 54, 26, 30, 2],\n",
       " [30, 8, 99, 42, 16, 85, 67, 56, 63, 20, 45, 54, 69, 2, 4, 15, 1, 60, 18, 14],\n",
       " [18,\n",
       "  15,\n",
       "  2,\n",
       "  98,\n",
       "  35,\n",
       "  54,\n",
       "  49,\n",
       "  74,\n",
       "  38,\n",
       "  99,\n",
       "  34,\n",
       "  56,\n",
       "  71,\n",
       "  58,\n",
       "  69,\n",
       "  45,\n",
       "  62,\n",
       "  20,\n",
       "  40,\n",
       "  96],\n",
       " [2,\n",
       "  20,\n",
       "  69,\n",
       "  18,\n",
       "  16,\n",
       "  99,\n",
       "  54,\n",
       "  60,\n",
       "  22,\n",
       "  13,\n",
       "  59,\n",
       "  50,\n",
       "  35,\n",
       "  31,\n",
       "  85,\n",
       "  21,\n",
       "  74,\n",
       "  58,\n",
       "  34,\n",
       "  15],\n",
       " [18,\n",
       "  54,\n",
       "  69,\n",
       "  35,\n",
       "  2,\n",
       "  22,\n",
       "  6,\n",
       "  67,\n",
       "  49,\n",
       "  74,\n",
       "  59,\n",
       "  45,\n",
       "  73,\n",
       "  99,\n",
       "  52,\n",
       "  31,\n",
       "  51,\n",
       "  16,\n",
       "  15,\n",
       "  20],\n",
       " [2, 63, 69, 45, 18, 35, 67, 31, 85, 54, 59, 52, 49, 74, 20, 73, 8, 42, 7, 15],\n",
       " [99,\n",
       "  54,\n",
       "  56,\n",
       "  34,\n",
       "  60,\n",
       "  37,\n",
       "  22,\n",
       "  4,\n",
       "  15,\n",
       "  49,\n",
       "  46,\n",
       "  18,\n",
       "  69,\n",
       "  20,\n",
       "  8,\n",
       "  77,\n",
       "  14,\n",
       "  25,\n",
       "  27,\n",
       "  51],\n",
       " [18,\n",
       "  60,\n",
       "  34,\n",
       "  54,\n",
       "  78,\n",
       "  74,\n",
       "  59,\n",
       "  31,\n",
       "  37,\n",
       "  14,\n",
       "  99,\n",
       "  27,\n",
       "  56,\n",
       "  15,\n",
       "  26,\n",
       "  7,\n",
       "  46,\n",
       "  20,\n",
       "  30,\n",
       "  52],\n",
       " [2,\n",
       "  18,\n",
       "  31,\n",
       "  69,\n",
       "  60,\n",
       "  35,\n",
       "  74,\n",
       "  78,\n",
       "  67,\n",
       "  6,\n",
       "  20,\n",
       "  23,\n",
       "  21,\n",
       "  42,\n",
       "  16,\n",
       "  63,\n",
       "  15,\n",
       "  22,\n",
       "  92,\n",
       "  82],\n",
       " [49, 45, 99, 56, 51, 16, 15, 18, 54, 26, 40, 4, 21, 7, 93, 42, 52, 12, 2, 71],\n",
       " [58,\n",
       "  18,\n",
       "  60,\n",
       "  74,\n",
       "  69,\n",
       "  15,\n",
       "  34,\n",
       "  21,\n",
       "  2,\n",
       "  49,\n",
       "  31,\n",
       "  99,\n",
       "  98,\n",
       "  56,\n",
       "  26,\n",
       "  38,\n",
       "  50,\n",
       "  57,\n",
       "  20,\n",
       "  13],\n",
       " [69,\n",
       "  67,\n",
       "  73,\n",
       "  2,\n",
       "  54,\n",
       "  28,\n",
       "  63,\n",
       "  18,\n",
       "  45,\n",
       "  22,\n",
       "  15,\n",
       "  31,\n",
       "  30,\n",
       "  35,\n",
       "  60,\n",
       "  49,\n",
       "  74,\n",
       "  20,\n",
       "  9,\n",
       "  42],\n",
       " [26,\n",
       "  85,\n",
       "  7,\n",
       "  16,\n",
       "  59,\n",
       "  18,\n",
       "  20,\n",
       "  52,\n",
       "  54,\n",
       "  6,\n",
       "  99,\n",
       "  31,\n",
       "  60,\n",
       "  21,\n",
       "  67,\n",
       "  30,\n",
       "  82,\n",
       "  63,\n",
       "  69,\n",
       "  66],\n",
       " [60, 18, 2, 69, 74, 20, 67, 34, 62, 1, 85, 78, 82, 22, 30, 9, 23, 59, 15, 27],\n",
       " [56, 69, 49, 18, 4, 51, 7, 45, 12, 15, 40, 85, 21, 99, 31, 14, 60, 6, 2, 22],\n",
       " [18,\n",
       "  35,\n",
       "  2,\n",
       "  78,\n",
       "  67,\n",
       "  31,\n",
       "  15,\n",
       "  60,\n",
       "  45,\n",
       "  74,\n",
       "  73,\n",
       "  30,\n",
       "  61,\n",
       "  42,\n",
       "  54,\n",
       "  37,\n",
       "  24,\n",
       "  20,\n",
       "  69,\n",
       "  49],\n",
       " [18, 28, 69, 54, 45, 78, 67, 49, 6, 37, 60, 22, 35, 15, 8, 2, 31, 56, 52, 73],\n",
       " [69,\n",
       "  60,\n",
       "  85,\n",
       "  67,\n",
       "  18,\n",
       "  20,\n",
       "  59,\n",
       "  31,\n",
       "  34,\n",
       "  74,\n",
       "  2,\n",
       "  82,\n",
       "  54,\n",
       "  24,\n",
       "  30,\n",
       "  63,\n",
       "  72,\n",
       "  7,\n",
       "  78,\n",
       "  27],\n",
       " [69, 56, 85, 4, 99, 50, 2, 13, 49, 34, 54, 8, 15, 20, 9, 51, 74, 31, 22, 71],\n",
       " [62, 18, 2, 20, 46, 34, 35, 30, 74, 31, 67, 60, 0, 65, 1, 11, 73, 17, 59, 98],\n",
       " [18,\n",
       "  45,\n",
       "  15,\n",
       "  56,\n",
       "  31,\n",
       "  69,\n",
       "  60,\n",
       "  24,\n",
       "  9,\n",
       "  30,\n",
       "  2,\n",
       "  49,\n",
       "  61,\n",
       "  74,\n",
       "  35,\n",
       "  63,\n",
       "  34,\n",
       "  78,\n",
       "  42,\n",
       "  52],\n",
       " [69, 56, 4, 60, 85, 49, 99, 1, 15, 18, 12, 31, 2, 51, 21, 71, 9, 92, 74, 54],\n",
       " [35,\n",
       "  54,\n",
       "  59,\n",
       "  18,\n",
       "  94,\n",
       "  99,\n",
       "  26,\n",
       "  78,\n",
       "  45,\n",
       "  20,\n",
       "  30,\n",
       "  16,\n",
       "  7,\n",
       "  15,\n",
       "  52,\n",
       "  31,\n",
       "  24,\n",
       "  49,\n",
       "  98,\n",
       "  34],\n",
       " [99,\n",
       "  20,\n",
       "  16,\n",
       "  85,\n",
       "  30,\n",
       "  69,\n",
       "  60,\n",
       "  8,\n",
       "  59,\n",
       "  2,\n",
       "  34,\n",
       "  18,\n",
       "  13,\n",
       "  25,\n",
       "  50,\n",
       "  15,\n",
       "  22,\n",
       "  46,\n",
       "  67,\n",
       "  56],\n",
       " [56,\n",
       "  54,\n",
       "  18,\n",
       "  15,\n",
       "  30,\n",
       "  7,\n",
       "  12,\n",
       "  60,\n",
       "  49,\n",
       "  61,\n",
       "  34,\n",
       "  14,\n",
       "  26,\n",
       "  85,\n",
       "  31,\n",
       "  45,\n",
       "  16,\n",
       "  20,\n",
       "  46,\n",
       "  1],\n",
       " [2,\n",
       "  16,\n",
       "  20,\n",
       "  18,\n",
       "  45,\n",
       "  42,\n",
       "  30,\n",
       "  63,\n",
       "  99,\n",
       "  35,\n",
       "  85,\n",
       "  54,\n",
       "  15,\n",
       "  67,\n",
       "  31,\n",
       "  49,\n",
       "  69,\n",
       "  52,\n",
       "  57,\n",
       "  62],\n",
       " [2,\n",
       "  35,\n",
       "  20,\n",
       "  69,\n",
       "  18,\n",
       "  16,\n",
       "  67,\n",
       "  54,\n",
       "  31,\n",
       "  78,\n",
       "  13,\n",
       "  73,\n",
       "  63,\n",
       "  15,\n",
       "  45,\n",
       "  99,\n",
       "  98,\n",
       "  59,\n",
       "  96,\n",
       "  60],\n",
       " [2,\n",
       "  74,\n",
       "  69,\n",
       "  18,\n",
       "  60,\n",
       "  62,\n",
       "  31,\n",
       "  34,\n",
       "  67,\n",
       "  9,\n",
       "  20,\n",
       "  35,\n",
       "  15,\n",
       "  73,\n",
       "  78,\n",
       "  98,\n",
       "  22,\n",
       "  63,\n",
       "  30,\n",
       "  38],\n",
       " [74, 69, 21, 26, 6, 18, 36, 22, 49, 60, 51, 2, 92, 29, 34, 31, 58, 7, 59, 4],\n",
       " [69,\n",
       "  74,\n",
       "  18,\n",
       "  60,\n",
       "  2,\n",
       "  31,\n",
       "  22,\n",
       "  37,\n",
       "  15,\n",
       "  49,\n",
       "  21,\n",
       "  38,\n",
       "  56,\n",
       "  34,\n",
       "  35,\n",
       "  4,\n",
       "  58,\n",
       "  78,\n",
       "  57,\n",
       "  98],\n",
       " [2,\n",
       "  16,\n",
       "  20,\n",
       "  45,\n",
       "  63,\n",
       "  18,\n",
       "  85,\n",
       "  99,\n",
       "  35,\n",
       "  30,\n",
       "  42,\n",
       "  54,\n",
       "  31,\n",
       "  52,\n",
       "  15,\n",
       "  69,\n",
       "  67,\n",
       "  51,\n",
       "  59,\n",
       "  7],\n",
       " [18,\n",
       "  74,\n",
       "  2,\n",
       "  60,\n",
       "  69,\n",
       "  15,\n",
       "  35,\n",
       "  49,\n",
       "  34,\n",
       "  78,\n",
       "  98,\n",
       "  38,\n",
       "  58,\n",
       "  20,\n",
       "  57,\n",
       "  22,\n",
       "  54,\n",
       "  56,\n",
       "  9,\n",
       "  62],\n",
       " [56,\n",
       "  45,\n",
       "  69,\n",
       "  7,\n",
       "  18,\n",
       "  49,\n",
       "  15,\n",
       "  30,\n",
       "  85,\n",
       "  31,\n",
       "  12,\n",
       "  54,\n",
       "  34,\n",
       "  61,\n",
       "  63,\n",
       "  71,\n",
       "  74,\n",
       "  60,\n",
       "  25,\n",
       "  99],\n",
       " [2,\n",
       "  18,\n",
       "  69,\n",
       "  60,\n",
       "  6,\n",
       "  16,\n",
       "  67,\n",
       "  31,\n",
       "  20,\n",
       "  78,\n",
       "  85,\n",
       "  42,\n",
       "  63,\n",
       "  92,\n",
       "  82,\n",
       "  49,\n",
       "  37,\n",
       "  45,\n",
       "  52,\n",
       "  15],\n",
       " [18,\n",
       "  60,\n",
       "  37,\n",
       "  2,\n",
       "  22,\n",
       "  69,\n",
       "  15,\n",
       "  31,\n",
       "  74,\n",
       "  56,\n",
       "  20,\n",
       "  34,\n",
       "  62,\n",
       "  55,\n",
       "  35,\n",
       "  73,\n",
       "  42,\n",
       "  99,\n",
       "  57,\n",
       "  54],\n",
       " [34,\n",
       "  56,\n",
       "  18,\n",
       "  60,\n",
       "  15,\n",
       "  74,\n",
       "  31,\n",
       "  54,\n",
       "  99,\n",
       "  69,\n",
       "  30,\n",
       "  49,\n",
       "  12,\n",
       "  7,\n",
       "  98,\n",
       "  59,\n",
       "  62,\n",
       "  38,\n",
       "  61,\n",
       "  20],\n",
       " [67, 54, 77, 8, 73, 35, 20, 18, 78, 99, 30, 46, 59, 16, 52, 27, 22, 6, 2, 42],\n",
       " [56, 69, 85, 60, 30, 15, 49, 61, 12, 8, 45, 99, 9, 18, 4, 31, 34, 54, 1, 7],\n",
       " [18, 56, 51, 22, 2, 40, 96, 46, 71, 15, 58, 99, 37, 11, 35, 4, 62, 0, 49, 42],\n",
       " [85, 18, 69, 21, 26, 60, 31, 92, 7, 2, 6, 1, 66, 74, 16, 20, 42, 56, 12, 49],\n",
       " [69, 60, 99, 4, 56, 85, 20, 18, 16, 2, 22, 15, 49, 1, 54, 34, 21, 51, 31, 13],\n",
       " [69, 2, 18, 49, 56, 45, 54, 15, 31, 99, 51, 85, 74, 60, 7, 22, 35, 4, 21, 16],\n",
       " [37,\n",
       "  60,\n",
       "  56,\n",
       "  18,\n",
       "  74,\n",
       "  34,\n",
       "  69,\n",
       "  15,\n",
       "  31,\n",
       "  38,\n",
       "  61,\n",
       "  49,\n",
       "  12,\n",
       "  4,\n",
       "  55,\n",
       "  14,\n",
       "  78,\n",
       "  2,\n",
       "  62,\n",
       "  57],\n",
       " [56,\n",
       "  61,\n",
       "  45,\n",
       "  18,\n",
       "  12,\n",
       "  14,\n",
       "  69,\n",
       "  30,\n",
       "  15,\n",
       "  49,\n",
       "  42,\n",
       "  31,\n",
       "  63,\n",
       "  60,\n",
       "  85,\n",
       "  7,\n",
       "  66,\n",
       "  32,\n",
       "  67,\n",
       "  52],\n",
       " [56, 15, 18, 69, 45, 60, 31, 2, 93, 38, 57, 9, 74, 71, 4, 61, 42, 40, 30, 99],\n",
       " [99, 54, 16, 45, 49, 51, 56, 18, 15, 8, 52, 4, 20, 69, 7, 26, 6, 85, 30, 42],\n",
       " [56,\n",
       "  18,\n",
       "  37,\n",
       "  15,\n",
       "  99,\n",
       "  30,\n",
       "  60,\n",
       "  54,\n",
       "  55,\n",
       "  22,\n",
       "  42,\n",
       "  49,\n",
       "  2,\n",
       "  45,\n",
       "  31,\n",
       "  78,\n",
       "  20,\n",
       "  61,\n",
       "  14,\n",
       "  12],\n",
       " [69,\n",
       "  99,\n",
       "  2,\n",
       "  54,\n",
       "  22,\n",
       "  85,\n",
       "  4,\n",
       "  51,\n",
       "  25,\n",
       "  13,\n",
       "  56,\n",
       "  34,\n",
       "  16,\n",
       "  87,\n",
       "  73,\n",
       "  49,\n",
       "  67,\n",
       "  50,\n",
       "  18,\n",
       "  46],\n",
       " [38,\n",
       "  15,\n",
       "  2,\n",
       "  56,\n",
       "  18,\n",
       "  60,\n",
       "  74,\n",
       "  62,\n",
       "  69,\n",
       "  34,\n",
       "  98,\n",
       "  55,\n",
       "  9,\n",
       "  65,\n",
       "  35,\n",
       "  22,\n",
       "  31,\n",
       "  57,\n",
       "  96,\n",
       "  73],\n",
       " [99, 54, 20, 16, 59, 18, 30, 85, 60, 7, 15, 69, 2, 8, 49, 31, 34, 52, 45, 56],\n",
       " [74,\n",
       "  18,\n",
       "  34,\n",
       "  35,\n",
       "  62,\n",
       "  31,\n",
       "  2,\n",
       "  98,\n",
       "  38,\n",
       "  0,\n",
       "  15,\n",
       "  46,\n",
       "  73,\n",
       "  60,\n",
       "  69,\n",
       "  56,\n",
       "  67,\n",
       "  59,\n",
       "  17,\n",
       "  65],\n",
       " [26, 7, 18, 85, 21, 45, 52, 6, 31, 49, 66, 92, 42, 2, 16, 63, 40, 12, 56, 59],\n",
       " [54,\n",
       "  99,\n",
       "  7,\n",
       "  59,\n",
       "  18,\n",
       "  30,\n",
       "  20,\n",
       "  52,\n",
       "  16,\n",
       "  46,\n",
       "  14,\n",
       "  25,\n",
       "  26,\n",
       "  85,\n",
       "  67,\n",
       "  34,\n",
       "  60,\n",
       "  77,\n",
       "  56,\n",
       "  27],\n",
       " [20,\n",
       "  18,\n",
       "  2,\n",
       "  78,\n",
       "  35,\n",
       "  54,\n",
       "  60,\n",
       "  99,\n",
       "  16,\n",
       "  59,\n",
       "  31,\n",
       "  15,\n",
       "  13,\n",
       "  74,\n",
       "  69,\n",
       "  34,\n",
       "  58,\n",
       "  50,\n",
       "  49,\n",
       "  26],\n",
       " [69,\n",
       "  2,\n",
       "  85,\n",
       "  63,\n",
       "  4,\n",
       "  49,\n",
       "  51,\n",
       "  45,\n",
       "  18,\n",
       "  31,\n",
       "  20,\n",
       "  56,\n",
       "  60,\n",
       "  13,\n",
       "  15,\n",
       "  16,\n",
       "  99,\n",
       "  22,\n",
       "  54,\n",
       "  93],\n",
       " [99,\n",
       "  49,\n",
       "  45,\n",
       "  54,\n",
       "  51,\n",
       "  18,\n",
       "  15,\n",
       "  56,\n",
       "  35,\n",
       "  2,\n",
       "  52,\n",
       "  26,\n",
       "  22,\n",
       "  21,\n",
       "  42,\n",
       "  7,\n",
       "  94,\n",
       "  20,\n",
       "  31,\n",
       "  40],\n",
       " [37, 18, 20, 22, 2, 0, 78, 46, 16, 42, 73, 99, 55, 67, 94, 30, 54, 6, 15, 14],\n",
       " [18,\n",
       "  56,\n",
       "  37,\n",
       "  61,\n",
       "  67,\n",
       "  2,\n",
       "  69,\n",
       "  42,\n",
       "  73,\n",
       "  15,\n",
       "  60,\n",
       "  31,\n",
       "  14,\n",
       "  74,\n",
       "  45,\n",
       "  65,\n",
       "  22,\n",
       "  62,\n",
       "  30,\n",
       "  38],\n",
       " [18,\n",
       "  74,\n",
       "  34,\n",
       "  56,\n",
       "  31,\n",
       "  21,\n",
       "  15,\n",
       "  12,\n",
       "  62,\n",
       "  69,\n",
       "  26,\n",
       "  17,\n",
       "  57,\n",
       "  40,\n",
       "  2,\n",
       "  49,\n",
       "  58,\n",
       "  37,\n",
       "  71,\n",
       "  85],\n",
       " [56, 69, 85, 45, 49, 7, 99, 54, 18, 30, 15, 8, 2, 63, 51, 31, 42, 52, 16, 60],\n",
       " [60, 18, 26, 69, 31, 85, 78, 6, 74, 59, 16, 20, 24, 2, 15, 49, 54, 52, 34, 7],\n",
       " [18,\n",
       "  34,\n",
       "  54,\n",
       "  46,\n",
       "  99,\n",
       "  74,\n",
       "  35,\n",
       "  59,\n",
       "  14,\n",
       "  77,\n",
       "  25,\n",
       "  7,\n",
       "  60,\n",
       "  0,\n",
       "  15,\n",
       "  98,\n",
       "  56,\n",
       "  31,\n",
       "  20,\n",
       "  94],\n",
       " [35,\n",
       "  78,\n",
       "  59,\n",
       "  77,\n",
       "  54,\n",
       "  20,\n",
       "  67,\n",
       "  18,\n",
       "  24,\n",
       "  98,\n",
       "  2,\n",
       "  73,\n",
       "  74,\n",
       "  94,\n",
       "  34,\n",
       "  31,\n",
       "  27,\n",
       "  52,\n",
       "  15,\n",
       "  8],\n",
       " [18, 35, 59, 20, 6, 26, 31, 74, 78, 52, 67, 2, 16, 7, 60, 34, 99, 21, 46, 22],\n",
       " [18,\n",
       "  35,\n",
       "  59,\n",
       "  34,\n",
       "  54,\n",
       "  74,\n",
       "  20,\n",
       "  99,\n",
       "  26,\n",
       "  98,\n",
       "  31,\n",
       "  58,\n",
       "  15,\n",
       "  78,\n",
       "  46,\n",
       "  60,\n",
       "  7,\n",
       "  2,\n",
       "  17,\n",
       "  16],\n",
       " [85,\n",
       "  60,\n",
       "  56,\n",
       "  1,\n",
       "  69,\n",
       "  12,\n",
       "  14,\n",
       "  7,\n",
       "  34,\n",
       "  99,\n",
       "  18,\n",
       "  26,\n",
       "  30,\n",
       "  61,\n",
       "  31,\n",
       "  54,\n",
       "  25,\n",
       "  15,\n",
       "  49,\n",
       "  20],\n",
       " [74,\n",
       "  18,\n",
       "  34,\n",
       "  2,\n",
       "  35,\n",
       "  62,\n",
       "  60,\n",
       "  67,\n",
       "  69,\n",
       "  73,\n",
       "  98,\n",
       "  59,\n",
       "  15,\n",
       "  20,\n",
       "  0,\n",
       "  78,\n",
       "  30,\n",
       "  17,\n",
       "  38,\n",
       "  54],\n",
       " [14,\n",
       "  56,\n",
       "  61,\n",
       "  67,\n",
       "  18,\n",
       "  30,\n",
       "  45,\n",
       "  74,\n",
       "  12,\n",
       "  34,\n",
       "  69,\n",
       "  31,\n",
       "  25,\n",
       "  46,\n",
       "  7,\n",
       "  42,\n",
       "  15,\n",
       "  8,\n",
       "  63,\n",
       "  65],\n",
       " [99, 54, 51, 56, 20, 49, 22, 30, 69, 15, 2, 18, 85, 42, 45, 60, 8, 21, 7, 13],\n",
       " [74,\n",
       "  18,\n",
       "  60,\n",
       "  34,\n",
       "  69,\n",
       "  2,\n",
       "  15,\n",
       "  31,\n",
       "  62,\n",
       "  56,\n",
       "  38,\n",
       "  98,\n",
       "  35,\n",
       "  9,\n",
       "  78,\n",
       "  54,\n",
       "  49,\n",
       "  22,\n",
       "  20,\n",
       "  57],\n",
       " [60, 37, 22, 4, 18, 20, 99, 56, 15, 55, 78, 2, 34, 31, 54, 27, 9, 74, 57, 62],\n",
       " [2, 67, 20, 35, 69, 18, 73, 63, 16, 22, 54, 59, 82, 8, 31, 78, 6, 13, 42, 52],\n",
       " [18,\n",
       "  78,\n",
       "  37,\n",
       "  67,\n",
       "  35,\n",
       "  2,\n",
       "  60,\n",
       "  31,\n",
       "  61,\n",
       "  6,\n",
       "  42,\n",
       "  15,\n",
       "  74,\n",
       "  24,\n",
       "  22,\n",
       "  45,\n",
       "  55,\n",
       "  20,\n",
       "  52,\n",
       "  27],\n",
       " [34,\n",
       "  74,\n",
       "  62,\n",
       "  46,\n",
       "  25,\n",
       "  18,\n",
       "  60,\n",
       "  98,\n",
       "  69,\n",
       "  31,\n",
       "  17,\n",
       "  54,\n",
       "  1,\n",
       "  20,\n",
       "  15,\n",
       "  59,\n",
       "  11,\n",
       "  99,\n",
       "  14,\n",
       "  30],\n",
       " [20,\n",
       "  58,\n",
       "  16,\n",
       "  54,\n",
       "  2,\n",
       "  35,\n",
       "  18,\n",
       "  13,\n",
       "  50,\n",
       "  15,\n",
       "  59,\n",
       "  98,\n",
       "  49,\n",
       "  51,\n",
       "  60,\n",
       "  21,\n",
       "  31,\n",
       "  57,\n",
       "  22,\n",
       "  34],\n",
       " [30,\n",
       "  54,\n",
       "  99,\n",
       "  18,\n",
       "  67,\n",
       "  85,\n",
       "  8,\n",
       "  7,\n",
       "  56,\n",
       "  20,\n",
       "  25,\n",
       "  46,\n",
       "  59,\n",
       "  60,\n",
       "  45,\n",
       "  31,\n",
       "  34,\n",
       "  52,\n",
       "  15,\n",
       "  69],\n",
       " [54,\n",
       "  99,\n",
       "  98,\n",
       "  15,\n",
       "  34,\n",
       "  50,\n",
       "  35,\n",
       "  59,\n",
       "  49,\n",
       "  18,\n",
       "  20,\n",
       "  69,\n",
       "  38,\n",
       "  2,\n",
       "  56,\n",
       "  13,\n",
       "  31,\n",
       "  60,\n",
       "  58,\n",
       "  45],\n",
       " [74,\n",
       "  34,\n",
       "  60,\n",
       "  69,\n",
       "  18,\n",
       "  98,\n",
       "  36,\n",
       "  31,\n",
       "  59,\n",
       "  38,\n",
       "  17,\n",
       "  9,\n",
       "  15,\n",
       "  62,\n",
       "  56,\n",
       "  35,\n",
       "  22,\n",
       "  54,\n",
       "  2,\n",
       "  78],\n",
       " [18,\n",
       "  78,\n",
       "  35,\n",
       "  37,\n",
       "  54,\n",
       "  20,\n",
       "  16,\n",
       "  15,\n",
       "  2,\n",
       "  99,\n",
       "  31,\n",
       "  22,\n",
       "  60,\n",
       "  42,\n",
       "  6,\n",
       "  67,\n",
       "  52,\n",
       "  30,\n",
       "  45,\n",
       "  27]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_algo1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bf154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1_000\n",
    "start_memory_usage = get_memory_usage()\n",
    "algo5 = LinTS(hyperparameters={'alpha': 0.00005})\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "algo5.train(train_10k_50k_250k, contexts=algo5_contexts)\n",
    "memory_used_algo5 = get_memory_usage() - start_memory_usage\n",
    "\n",
    "recommendations_ids5 = np.zeros((test_10k_50k_250k.shape[0], src.TOP_N), dtype=int)\n",
    "recommendations_scores5 = np.zeros((test_10k_50k_250k.shape[0], src.TOP_N), dtype=float)\n",
    "\n",
    "for j in range(0, len(test_10k_50k_250k), BATCH_SIZE):\n",
    "    batch_df = test_10k_50k_250k.iloc[j:j + BATCH_SIZE]\n",
    "    batch_contexts = algo5_contexts_rec[j:j + BATCH_SIZE]\n",
    "    results_algo5 = algo5.recommend(users_ids=batch_df[src.COLUMN_USER_ID], contexts=batch_contexts)\n",
    "    recommendations_ids5[j:j + BATCH_SIZE] = results_algo5[0]\n",
    "    recommendations_scores5[j:j + BATCH_SIZE] = results_algo5[1]\n",
    "    break\n",
    "\n",
    "total_time_algo5 = time.time() - start_time\n",
    "\n",
    "print('\\n\\n\\n-----------------------------------------------------------\\n')\n",
    "print(f\"Time taken by 10k users, 50k items and 250k interactions: {total_time_algo5:.2f} seconds\")\n",
    "print(f\"Memory used by 10k users, 50k items and 250k interactions: {memory_used_algo5:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8fdefc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.040284156799316406 segundos\n",
      "passar para cuda demorou 0.024843931198120117 segundos\n",
      "A add demorou 0.0010445117950439453 segundos\n",
      "Xty demorou 8.702278137207031e-05 segundos\n",
      "beta demorou 0.0658121109008789 segundos\n",
      "beta demorou 0.009865760803222656 segundos\n",
      "beta demorou 0.009853363037109375 segundos\n",
      "beta demorou 0.009863853454589844 segundos\n",
      "beta demorou 0.009884357452392578 segundos\n",
      "beta demorou 0.009868383407592773 segundos\n",
      "beta demorou 0.009876012802124023 segundos\n",
      "beta demorou 0.009832143783569336 segundos\n",
      "beta demorou 0.009868860244750977 segundos\n",
      "beta demorou 0.009895563125610352 segundos\n",
      "beta demorou 0.009325027465820312 segundos\n",
      "beta demorou 0.009111642837524414 segundos\n",
      "beta demorou 0.009108543395996094 segundos\n",
      "beta demorou 0.00915384292602539 segundos\n",
      "beta demorou 0.0091094970703125 segundos\n",
      "beta demorou 0.009120702743530273 segundos\n",
      "beta demorou 0.009132623672485352 segundos\n",
      "beta demorou 0.009146690368652344 segundos\n",
      "beta demorou 0.009175300598144531 segundos\n",
      "beta demorou 0.009135961532592773 segundos\n",
      "beta demorou 0.009129524230957031 segundos\n",
      "beta demorou 0.00914144515991211 segundos\n",
      "beta demorou 0.009119749069213867 segundos\n",
      "beta demorou 0.009170770645141602 segundos\n",
      "beta demorou 0.009143352508544922 segundos\n",
      "beta demorou 0.00913381576538086 segundos\n",
      "beta demorou 0.009127140045166016 segundos\n",
      "beta demorou 0.009145736694335938 segundos\n",
      "beta demorou 0.009125471115112305 segundos\n",
      "beta demorou 0.009144306182861328 segundos\n",
      "beta demorou 0.009146690368652344 segundos\n",
      "beta demorou 0.009143590927124023 segundos\n",
      "beta demorou 0.009099721908569336 segundos\n",
      "beta demorou 0.009125232696533203 segundos\n",
      "beta demorou 0.009109973907470703 segundos\n",
      "beta demorou 0.00909280776977539 segundos\n",
      "beta demorou 0.009118795394897461 segundos\n",
      "beta demorou 0.00911569595336914 segundos\n",
      "beta demorou 0.00914144515991211 segundos\n",
      "beta demorou 0.008952856063842773 segundos\n",
      "beta demorou 0.008980989456176758 segundos\n",
      "beta demorou 0.008903026580810547 segundos\n",
      "beta demorou 0.008824586868286133 segundos\n",
      "beta demorou 0.008805990219116211 segundos\n",
      "beta demorou 0.00878143310546875 segundos\n",
      "beta demorou 0.008705854415893555 segundos\n",
      "beta demorou 0.008724451065063477 segundos\n",
      "beta demorou 0.008708000183105469 segundos\n",
      "beta demorou 0.008690834045410156 segundos\n",
      "beta demorou 0.008731603622436523 segundos\n",
      "parallel_fit demorou 0.5439019203186035 segundos\n",
      "passar para cuda demorou 0.007254362106323242 segundos\n",
      "A add demorou 0.00023508071899414062 segundos\n",
      "Xty demorou 6.389617919921875e-05 segundos\n",
      "beta demorou 0.040834903717041016 segundos\n",
      "beta demorou 0.008908510208129883 segundos\n",
      "beta demorou 0.008842229843139648 segundos\n",
      "beta demorou 0.008794784545898438 segundos\n",
      "beta demorou 0.008748292922973633 segundos\n",
      "beta demorou 0.00874018669128418 segundos\n",
      "beta demorou 0.008804798126220703 segundos\n",
      "beta demorou 0.008749008178710938 segundos\n",
      "beta demorou 0.008745431900024414 segundos\n",
      "beta demorou 0.008737564086914062 segundos\n",
      "beta demorou 0.00877237319946289 segundos\n",
      "beta demorou 0.008751153945922852 segundos\n",
      "beta demorou 0.008809089660644531 segundos\n",
      "beta demorou 0.008742570877075195 segundos\n",
      "beta demorou 0.008778572082519531 segundos\n",
      "beta demorou 0.008813858032226562 segundos\n",
      "beta demorou 0.008735418319702148 segundos\n",
      "beta demorou 0.008709192276000977 segundos\n",
      "beta demorou 0.008762121200561523 segundos\n",
      "beta demorou 0.008794307708740234 segundos\n",
      "beta demorou 0.008786439895629883 segundos\n",
      "beta demorou 0.008716583251953125 segundos\n",
      "beta demorou 0.008748769760131836 segundos\n",
      "beta demorou 0.008733034133911133 segundos\n",
      "beta demorou 0.008769512176513672 segundos\n",
      "beta demorou 0.008774757385253906 segundos\n",
      "beta demorou 0.008783578872680664 segundos\n",
      "beta demorou 0.008776664733886719 segundos\n",
      "beta demorou 0.008747100830078125 segundos\n",
      "beta demorou 0.008767127990722656 segundos\n",
      "beta demorou 0.008786201477050781 segundos\n",
      "beta demorou 0.008794546127319336 segundos\n",
      "beta demorou 0.008725166320800781 segundos\n",
      "beta demorou 0.008736133575439453 segundos\n",
      "beta demorou 0.008755207061767578 segundos\n",
      "beta demorou 0.008760452270507812 segundos\n",
      "beta demorou 0.00876927375793457 segundos\n",
      "beta demorou 0.00872945785522461 segundos\n",
      "beta demorou 0.008871793746948242 segundos\n",
      "beta demorou 0.008738040924072266 segundos\n",
      "beta demorou 0.008754253387451172 segundos\n",
      "beta demorou 0.008733034133911133 segundos\n",
      "beta demorou 0.008705377578735352 segundos\n",
      "beta demorou 0.00874018669128418 segundos\n",
      "beta demorou 0.008749723434448242 segundos\n",
      "beta demorou 0.008739471435546875 segundos\n",
      "beta demorou 0.008743762969970703 segundos\n",
      "beta demorou 0.008732080459594727 segundos\n",
      "beta demorou 0.008768796920776367 segundos\n",
      "beta demorou 0.008828401565551758 segundos\n",
      "parallel_fit demorou 0.47905397415161133 segundos\n",
      "passar para cuda demorou 0.0037794113159179688 segundos\n",
      "A add demorou 0.0001647472381591797 segundos\n",
      "Xty demorou 5.984306335449219e-05 segundos\n",
      "beta demorou 0.024740219116210938 segundos\n",
      "beta demorou 0.008799076080322266 segundos\n",
      "beta demorou 0.008762598037719727 segundos\n",
      "beta demorou 0.008789300918579102 segundos\n",
      "beta demorou 0.0088043212890625 segundos\n",
      "beta demorou 0.008727312088012695 segundos\n",
      "beta demorou 0.00872349739074707 segundos\n",
      "beta demorou 0.008774280548095703 segundos\n",
      "beta demorou 0.008744239807128906 segundos\n",
      "beta demorou 0.008783340454101562 segundos\n",
      "beta demorou 0.008774757385253906 segundos\n",
      "beta demorou 0.008738040924072266 segundos\n",
      "beta demorou 0.008755922317504883 segundos\n",
      "beta demorou 0.008766412734985352 segundos\n",
      "beta demorou 0.008722066879272461 segundos\n",
      "beta demorou 0.009709358215332031 segundos\n",
      "beta demorou 0.008784294128417969 segundos\n",
      "beta demorou 0.008795499801635742 segundos\n",
      "beta demorou 0.008821487426757812 segundos\n",
      "beta demorou 0.008744478225708008 segundos\n",
      "beta demorou 0.008763790130615234 segundos\n",
      "beta demorou 0.008750438690185547 segundos\n",
      "beta demorou 0.008829116821289062 segundos\n",
      "beta demorou 0.008790969848632812 segundos\n",
      "beta demorou 0.008743524551391602 segundos\n",
      "beta demorou 0.008774042129516602 segundos\n",
      "beta demorou 0.008754730224609375 segundos\n",
      "beta demorou 0.008804559707641602 segundos\n",
      "beta demorou 0.008725881576538086 segundos\n",
      "beta demorou 0.008758783340454102 segundos\n",
      "beta demorou 0.008739233016967773 segundos\n",
      "beta demorou 0.008768558502197266 segundos\n",
      "beta demorou 0.008761405944824219 segundos\n",
      "beta demorou 0.008745908737182617 segundos\n",
      "beta demorou 0.008742570877075195 segundos\n",
      "beta demorou 0.00871729850769043 segundos\n",
      "beta demorou 0.00874185562133789 segundos\n",
      "beta demorou 0.008776664733886719 segundos\n",
      "beta demorou 0.00874471664428711 segundos\n",
      "beta demorou 0.008767366409301758 segundos\n",
      "beta demorou 0.00878596305847168 segundos\n",
      "beta demorou 0.008737325668334961 segundos\n",
      "beta demorou 0.00877690315246582 segundos\n",
      "beta demorou 0.008813858032226562 segundos\n",
      "beta demorou 0.008740663528442383 segundos\n",
      "beta demorou 0.008738279342651367 segundos\n",
      "beta demorou 0.008759021759033203 segundos\n",
      "beta demorou 0.008813619613647461 segundos\n",
      "beta demorou 0.008758068084716797 segundos\n",
      "beta demorou 0.008756160736083984 segundos\n",
      "parallel_fit demorou 0.4602952003479004 segundos\n",
      "predict_expectations demorou 0.43991899490356445 segundos\n",
      "Exclude mask demorou 0.005211591720581055 segundos\n",
      "Ordenação top-K demorou 0.0005567073822021484 segundos\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 50_000\n",
    "RECOMMEND_BATCH_SIZE = 50\n",
    "ITEMS_PER_BATCH = 1_000\n",
    "start_memory_usage = get_memory_usage()\n",
    "algo5_optimized = LinTSOptimized(train_10k_50k_250k[src.COLUMN_USER_ID].nunique(), train_10k_50k_250k[src.COLUMN_ITEM_ID].nunique(), algo5_contexts.shape[1], hyperparameters={'alpha': 0.00005})\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "for j in range(0, len(train_10k_50k_250k), TRAIN_BATCH_SIZE):\n",
    "    batch_df = train_10k_50k_250k.iloc[j:j + TRAIN_BATCH_SIZE]\n",
    "    batch_contexts = algo5_contexts[j:j + TRAIN_BATCH_SIZE]\n",
    "    algo5_optimized.train(batch_df, contexts=batch_contexts)\n",
    "#algo5_optimized.train(train_10k_50k_250k, contexts=algo5_contexts)\n",
    "memory_used_algo5_optimized = get_memory_usage() - start_memory_usage\n",
    "\n",
    "recommendations_ids5_optimized = np.zeros((test_10k_50k_250k.shape[0], src.TOP_N), dtype=int)\n",
    "recommendations_scores5_optimized = np.zeros((test_10k_50k_250k.shape[0], src.TOP_N), dtype=float)\n",
    "\n",
    "for j in range(0, len(test_10k_50k_250k), RECOMMEND_BATCH_SIZE):\n",
    "    batch_df = test_10k_50k_250k.iloc[j:j + RECOMMEND_BATCH_SIZE]\n",
    "    batch_contexts = algo5_contexts_rec[j:j + RECOMMEND_BATCH_SIZE]\n",
    "    results_algo5_optimized = algo5_optimized.recommend(users_ids=batch_df[src.COLUMN_USER_ID], contexts=batch_contexts)\n",
    "    recommendations_ids5_optimized[j:j + RECOMMEND_BATCH_SIZE] = results_algo5_optimized[0]\n",
    "    recommendations_scores5_optimized[j:j + RECOMMEND_BATCH_SIZE] = results_algo5_optimized[1]\n",
    "    break\n",
    "\n",
    "total_time_algo5_optimized = time.time() - start_time\n",
    "\n",
    "#print('recs iguais ?')\n",
    "#print(np.array_equal(recommendations_ids5, recommendations_ids5_optimized))\n",
    "#print(np.allclose(recommendations_scores5, recommendations_scores5_optimized))\n",
    "\n",
    "#print('\\n\\n\\n-----------------------------------------------------------\\n')\n",
    "#print(f\"Time taken by 10k users, 50k items and 250k interactions: {total_time_algo5_optimized:.2f} seconds ({total_time_algo5 / total_time_algo5_optimized:.2f}x mais rápido)\")\n",
    "#print(f\"Memory used by 10k users, 50k items and 250k interactions: {memory_used_algo5_optimized:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe121f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.0003666877746582031 segundos\n",
      "passar para cuda demorou 0.026379108428955078 segundos\n",
      "A add demorou 0.0002143383026123047 segundos\n",
      "Xty demorou 6.29425048828125e-05 segundos\n",
      "beta demorou 0.041290998458862305 segundos\n",
      "beta demorou 0.00945591926574707 segundos\n",
      "beta demorou 0.00946950912475586 segundos\n",
      "beta demorou 0.009478330612182617 segundos\n",
      "beta demorou 0.009142160415649414 segundos\n",
      "beta demorou 0.008839130401611328 segundos\n",
      "beta demorou 0.008839607238769531 segundos\n",
      "beta demorou 0.008753776550292969 segundos\n",
      "beta demorou 0.008795022964477539 segundos\n",
      "beta demorou 0.008748769760131836 segundos\n",
      "beta demorou 0.008763313293457031 segundos\n",
      "beta demorou 0.008742809295654297 segundos\n",
      "beta demorou 0.008740663528442383 segundos\n",
      "beta demorou 0.008862972259521484 segundos\n",
      "beta demorou 0.008753776550292969 segundos\n",
      "beta demorou 0.008743762969970703 segundos\n",
      "beta demorou 0.008727073669433594 segundos\n",
      "beta demorou 0.008786916732788086 segundos\n",
      "beta demorou 0.00874638557434082 segundos\n",
      "beta demorou 0.008736371994018555 segundos\n",
      "beta demorou 0.008750438690185547 segundos\n",
      "beta demorou 0.008762598037719727 segundos\n",
      "beta demorou 0.008864879608154297 segundos\n",
      "beta demorou 0.00873422622680664 segundos\n",
      "beta demorou 0.00884866714477539 segundos\n",
      "beta demorou 0.008748054504394531 segundos\n",
      "beta demorou 0.008783102035522461 segundos\n",
      "beta demorou 0.008716106414794922 segundos\n",
      "beta demorou 0.008735418319702148 segundos\n",
      "beta demorou 0.008744955062866211 segundos\n",
      "beta demorou 0.008763313293457031 segundos\n",
      "beta demorou 0.00873565673828125 segundos\n",
      "beta demorou 0.008739948272705078 segundos\n",
      "beta demorou 0.008780241012573242 segundos\n",
      "beta demorou 0.008764982223510742 segundos\n",
      "beta demorou 0.008732795715332031 segundos\n",
      "beta demorou 0.008741140365600586 segundos\n",
      "beta demorou 0.008735418319702148 segundos\n",
      "beta demorou 0.009383678436279297 segundos\n",
      "beta demorou 0.008780956268310547 segundos\n",
      "beta demorou 0.008764266967773438 segundos\n",
      "beta demorou 0.008722066879272461 segundos\n",
      "beta demorou 0.00874018669128418 segundos\n",
      "beta demorou 0.008738994598388672 segundos\n",
      "beta demorou 0.008781909942626953 segundos\n",
      "beta demorou 0.008736848831176758 segundos\n",
      "beta demorou 0.008710145950317383 segundos\n",
      "beta demorou 0.008727788925170898 segundos\n",
      "beta demorou 0.008747100830078125 segundos\n",
      "beta demorou 0.008769035339355469 segundos\n",
      "parallel_fit demorou 0.5016019344329834 segundos\n",
      "passar para cuda demorou 0.0069887638092041016 segundos\n",
      "A add demorou 0.0002193450927734375 segundos\n",
      "Xty demorou 6.580352783203125e-05 segundos\n",
      "beta demorou 0.040964365005493164 segundos\n",
      "beta demorou 0.008836507797241211 segundos\n",
      "beta demorou 0.008768320083618164 segundos\n",
      "beta demorou 0.00879526138305664 segundos\n",
      "beta demorou 0.008727312088012695 segundos\n",
      "beta demorou 0.008770227432250977 segundos\n",
      "beta demorou 0.008794784545898438 segundos\n",
      "beta demorou 0.008758544921875 segundos\n",
      "beta demorou 0.008767843246459961 segundos\n",
      "beta demorou 0.008736610412597656 segundos\n",
      "beta demorou 0.008734464645385742 segundos\n",
      "beta demorou 0.008789777755737305 segundos\n",
      "beta demorou 0.008740663528442383 segundos\n",
      "beta demorou 0.008726119995117188 segundos\n",
      "beta demorou 0.008764982223510742 segundos\n",
      "beta demorou 0.008769750595092773 segundos\n",
      "beta demorou 0.008752107620239258 segundos\n",
      "beta demorou 0.008728504180908203 segundos\n",
      "beta demorou 0.00872802734375 segundos\n",
      "beta demorou 0.00874471664428711 segundos\n",
      "beta demorou 0.008718013763427734 segundos\n",
      "beta demorou 0.008758068084716797 segundos\n",
      "beta demorou 0.008753299713134766 segundos\n",
      "beta demorou 0.008742094039916992 segundos\n",
      "beta demorou 0.00872492790222168 segundos\n",
      "beta demorou 0.008748292922973633 segundos\n",
      "beta demorou 0.008742094039916992 segundos\n",
      "beta demorou 0.008764028549194336 segundos\n",
      "beta demorou 0.008738040924072266 segundos\n",
      "beta demorou 0.008748769760131836 segundos\n",
      "beta demorou 0.008764266967773438 segundos\n",
      "beta demorou 0.008777379989624023 segundos\n",
      "beta demorou 0.008711576461791992 segundos\n",
      "beta demorou 0.008705377578735352 segundos\n",
      "beta demorou 0.008837461471557617 segundos\n",
      "beta demorou 0.008758544921875 segundos\n",
      "beta demorou 0.008738279342651367 segundos\n",
      "beta demorou 0.00883173942565918 segundos\n",
      "beta demorou 0.00872945785522461 segundos\n",
      "beta demorou 0.008744955062866211 segundos\n",
      "beta demorou 0.00872945785522461 segundos\n",
      "beta demorou 0.008781194686889648 segundos\n",
      "beta demorou 0.008712291717529297 segundos\n",
      "beta demorou 0.008777618408203125 segundos\n",
      "beta demorou 0.00876474380493164 segundos\n",
      "beta demorou 0.00874018669128418 segundos\n",
      "beta demorou 0.0087432861328125 segundos\n",
      "beta demorou 0.00871133804321289 segundos\n",
      "beta demorou 0.008739471435546875 segundos\n",
      "beta demorou 0.008713483810424805 segundos\n",
      "parallel_fit demorou 0.47828006744384766 segundos\n",
      "passar para cuda demorou 0.0035169124603271484 segundos\n",
      "A add demorou 0.00015997886657714844 segundos\n",
      "Xty demorou 5.9604644775390625e-05 segundos\n",
      "beta demorou 0.024643659591674805 segundos\n",
      "beta demorou 0.00877833366394043 segundos\n",
      "beta demorou 0.008806705474853516 segundos\n",
      "beta demorou 0.008839607238769531 segundos\n",
      "beta demorou 0.008748769760131836 segundos\n",
      "beta demorou 0.008742809295654297 segundos\n",
      "beta demorou 0.008758068084716797 segundos\n",
      "beta demorou 0.008742332458496094 segundos\n",
      "beta demorou 0.008729219436645508 segundos\n",
      "beta demorou 0.008736371994018555 segundos\n",
      "beta demorou 0.008778572082519531 segundos\n",
      "beta demorou 0.008789539337158203 segundos\n",
      "beta demorou 0.008770227432250977 segundos\n",
      "beta demorou 0.008747339248657227 segundos\n",
      "beta demorou 0.008768081665039062 segundos\n",
      "beta demorou 0.008727073669433594 segundos\n",
      "beta demorou 0.008727550506591797 segundos\n",
      "beta demorou 0.00876474380493164 segundos\n",
      "beta demorou 0.008703470230102539 segundos\n",
      "beta demorou 0.008765459060668945 segundos\n",
      "beta demorou 0.009711027145385742 segundos\n",
      "beta demorou 0.008795022964477539 segundos\n",
      "beta demorou 0.008748769760131836 segundos\n",
      "beta demorou 0.00875997543334961 segundos\n",
      "beta demorou 0.008738040924072266 segundos\n",
      "beta demorou 0.008777856826782227 segundos\n",
      "beta demorou 0.008792638778686523 segundos\n",
      "beta demorou 0.008753061294555664 segundos\n",
      "beta demorou 0.008733510971069336 segundos\n",
      "beta demorou 0.008775949478149414 segundos\n",
      "beta demorou 0.008790016174316406 segundos\n",
      "beta demorou 0.008765220642089844 segundos\n",
      "beta demorou 0.00874781608581543 segundos\n",
      "beta demorou 0.008732318878173828 segundos\n",
      "beta demorou 0.0087738037109375 segundos\n",
      "beta demorou 0.008720636367797852 segundos\n",
      "beta demorou 0.008745193481445312 segundos\n",
      "beta demorou 0.008758306503295898 segundos\n",
      "beta demorou 0.00873422622680664 segundos\n",
      "beta demorou 0.008765935897827148 segundos\n",
      "beta demorou 0.008752107620239258 segundos\n",
      "beta demorou 0.008771181106567383 segundos\n",
      "beta demorou 0.008759260177612305 segundos\n",
      "beta demorou 0.008767127990722656 segundos\n",
      "beta demorou 0.008763551712036133 segundos\n",
      "beta demorou 0.0087432861328125 segundos\n",
      "beta demorou 0.00878286361694336 segundos\n",
      "beta demorou 0.008728265762329102 segundos\n",
      "beta demorou 0.00880742073059082 segundos\n",
      "beta demorou 0.008728742599487305 segundos\n",
      "parallel_fit demorou 0.4597346782684326 segundos\n",
      "predict_expectations demorou 0.44037914276123047 segundos\n",
      "Exclude mask demorou 0.005233049392700195 segundos\n",
      "Ordenação top-K demorou 0.00046634674072265625 segundos\n"
     ]
    }
   ],
   "source": [
    "TRAIN_BATCH_SIZE = 50_000\n",
    "RECOMMEND_BATCH_SIZE = 50\n",
    "ITEMS_PER_BATCH = 2_000\n",
    "start_memory_usage = get_memory_usage()\n",
    "algo5_optimized = LinTSOptimized(train_10k_50k_250k[src.COLUMN_USER_ID].nunique(), train_10k_50k_250k[src.COLUMN_ITEM_ID].nunique(), algo5_contexts.shape[1], hyperparameters={'alpha': 0.00005})\n",
    "\n",
    "# Treinando o modelo com o dataset de 100 usuários e 100 itens\n",
    "start_time = time.time()\n",
    "for j in range(0, len(train_10k_50k_250k), TRAIN_BATCH_SIZE):\n",
    "    batch_df = train_10k_50k_250k.iloc[j:j + TRAIN_BATCH_SIZE]\n",
    "    batch_contexts = algo5_contexts[j:j + TRAIN_BATCH_SIZE]\n",
    "    algo5_optimized.train(batch_df, contexts=batch_contexts)\n",
    "#algo5_optimized.train(train_10k_50k_250k, contexts=algo5_contexts)\n",
    "memory_used_algo5_optimized = get_memory_usage() - start_memory_usage\n",
    "\n",
    "recommendations_ids5_optimized = np.zeros((test_10k_50k_250k.shape[0], src.TOP_N), dtype=int)\n",
    "recommendations_scores5_optimized = np.zeros((test_10k_50k_250k.shape[0], src.TOP_N), dtype=float)\n",
    "\n",
    "for j in range(0, len(test_10k_50k_250k), RECOMMEND_BATCH_SIZE):\n",
    "    batch_df = test_10k_50k_250k.iloc[j:j + RECOMMEND_BATCH_SIZE]\n",
    "    batch_contexts = algo5_contexts_rec[j:j + RECOMMEND_BATCH_SIZE]\n",
    "    results_algo5_optimized2 = algo5_optimized.recommend(users_ids=batch_df[src.COLUMN_USER_ID], contexts=batch_contexts)\n",
    "    recommendations_ids5_optimized[j:j + RECOMMEND_BATCH_SIZE] = results_algo5_optimized[0]\n",
    "    recommendations_scores5_optimized[j:j + RECOMMEND_BATCH_SIZE] = results_algo5_optimized[1]\n",
    "    break\n",
    "\n",
    "total_time_algo5_optimized = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0038839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(results_algo5_optimized[0] == results_algo5_optimized2[0]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2d3a6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30555, 38986,   215, ...,   891, 29288, 24339],\n",
       "       [23236, 35837, 23776, ...,  8175, 32133, 41982],\n",
       "       [21231, 32512,  3650, ..., 39024,  2994, 28461],\n",
       "       ...,\n",
       "       [32512, 34953, 24339, ...,  3650, 36448, 12308],\n",
       "       [32512,  6081, 37859, ..., 33491, 25172, 34763],\n",
       "       [29865, 32512,   215, ..., 11034, 23776, 33491]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(results_algo5[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea922da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30555, 38986,   215, ..., 24339, 17700, 40985],\n",
       "       [23236, 35837, 23776, ...,  1293, 32133,  3754],\n",
       "       [21231, 32512,  3650, ..., 41786,  6890,  2994],\n",
       "       ...,\n",
       "       [32512, 34953, 46052, ..., 29288, 44618, 41982],\n",
       "       [32512,  6081, 37859, ...,  1293, 47550, 25172],\n",
       "       [29865, 32512,   215, ..., 33491, 11769, 23776]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_algo5_optimized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0901c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_rng = torch.Generator(device='cuda').manual_seed(src.RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a8373487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0360e+00, -1.9756e+00, -1.0678e+00, -7.3533e-01,  7.0253e-01],\n",
       "        [-9.6031e-01, -9.1739e-01,  4.0556e-01,  8.0946e-01, -4.8576e-01],\n",
       "        [ 7.8499e-01, -2.4007e-01, -6.9460e-01, -1.9525e+00,  3.9284e-01],\n",
       "        [-1.0636e+00,  2.1251e-01, -1.6480e+00, -5.1468e-01,  1.1583e+00],\n",
       "        [ 3.4949e-01,  7.4801e-01, -5.5509e-01,  8.3432e-01,  1.0950e+00],\n",
       "        [ 2.4906e-01, -1.0698e+00,  7.8971e-02, -8.3719e-02,  3.2338e-02],\n",
       "        [ 5.1603e-01, -6.8338e-01,  1.0061e+00,  5.0865e-01, -1.2080e+00],\n",
       "        [ 1.3042e+00,  4.4752e-01, -1.2036e+00,  2.0147e+00, -4.4517e-01],\n",
       "        [-1.6052e+00, -8.0793e-01, -3.3069e-01,  1.4150e-01,  4.5848e-01],\n",
       "        [ 7.0441e-01, -2.3455e-01, -1.7963e+00,  7.0716e-04,  9.7392e-01]],\n",
       "       device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((10, 5), generator=torch_rng, device='cuda', dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09c636ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_rng = torch.Generator(device='cuda').manual_seed(src.RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26246c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0360, -1.9756, -1.0678, -0.7353,  0.7025],\n",
       "        [-0.9603, -0.9174,  0.4056,  0.8095, -0.4858],\n",
       "        [ 0.7850, -0.2401, -0.6946, -1.9525,  0.3928],\n",
       "        [-1.0636,  0.2125, -1.6480, -0.5147,  1.1583],\n",
       "        [ 0.3495,  0.7480, -0.5551,  0.8343,  1.0950]], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((5, 5), generator=torch_rng, device='cuda', dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c9b7c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4233,  0.1292, -0.4278,  2.0776, -1.3249],\n",
       "        [ 0.1959, -1.0691,  1.2437, -1.5044,  0.7260],\n",
       "        [-0.1422,  0.9751, -0.6168,  0.7498, -0.6927],\n",
       "        [ 0.7814, -0.4943,  1.1254,  0.2509,  0.8883],\n",
       "        [-0.7887, -0.4151,  1.0460,  0.6586, -1.3972]], device='cuda:0',\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn((5, 5), generator=torch_rng, device='cuda', dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "602fb893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch_rng = torch.Generator(device='cuda')\n",
    "torch_rng.manual_seed(src.RANDOM_STATE)\n",
    "\n",
    "# Then in your loop, just keep drawing in chunks\n",
    "z1 = torch.randn((5, 5), generator=torch_rng, device='cuda', dtype=torch.double)\n",
    "z2 = torch.randn((5, 5), generator=torch_rng, device='cuda', dtype=torch.double)\n",
    "z3 = torch.randn((5, 5), generator=torch_rng, device='cuda', dtype=torch.double)\n",
    "\n",
    "# This gives the same result as:\n",
    "torch_rng = torch.Generator(device='cuda').manual_seed(src.RANDOM_STATE)\n",
    "z_full = torch.randn((15, 5), generator=torch_rng, device='cuda', dtype=torch.double)\n",
    "z1_expected = z_full[:5]\n",
    "z2_expected = z_full[5:10]\n",
    "z3_expected = z_full[10:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94dfaad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(z1 == z1_expected).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7088fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(z2 == z2_expected).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc01458a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(False)\n",
      "tensor(False)\n",
      "tensor(False)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "RANDOM_STATE = 1234  # Fixed seed\n",
    "\n",
    "# Generator setup\n",
    "torch_rng = torch.Generator(device='cpu').manual_seed(RANDOM_STATE)\n",
    "\n",
    "# Draw in chunks\n",
    "z1 = torch.randn((5, 5), generator=torch_rng, device='cpu', dtype=torch.double)\n",
    "z2 = torch.randn((5, 5), generator=torch_rng, device='cpu', dtype=torch.double)\n",
    "z3 = torch.randn((5, 5), generator=torch_rng, device='cpu', dtype=torch.double)\n",
    "\n",
    "# Draw all at once\n",
    "torch_rng = torch.Generator(device='cpu').manual_seed(RANDOM_STATE)\n",
    "z_full = torch.randn((15, 5), generator=torch_rng, device='cpu', dtype=torch.double)\n",
    "z1_expected = z_full[:5]\n",
    "z2_expected = z_full[5:10]\n",
    "z3_expected = z_full[10:15]\n",
    "\n",
    "# Compare\n",
    "print((z1 == z1_expected).all())\n",
    "print((z2 == z2_expected).all())\n",
    "print((z3 == z3_expected).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c8eae66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0883,  0.3420,  0.4112,  1.0051, -0.1117],\n",
       "        [-0.5988, -0.0982, -0.3511,  0.7209, -0.1952],\n",
       "        [-0.5215,  0.8718,  1.7656,  0.7725, -2.6852],\n",
       "        [-0.1483, -1.7856,  0.0853,  1.0006, -0.0962],\n",
       "        [ 1.0628,  0.8112, -0.0721,  0.8024,  0.1205]], dtype=torch.float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d6c9cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05b8f942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0883,  0.3420,  0.4112,  1.0051, -0.1117],\n",
       "        [-0.5988, -0.0982, -0.3511,  0.7209, -0.2169],\n",
       "        [-1.0427,  0.2448, -0.9887, -0.5196,  0.6585],\n",
       "        [ 0.6406,  0.7839,  0.6573, -1.6348, -0.6108],\n",
       "        [ 0.6003, -0.8769,  0.9649, -0.1926,  0.3745]], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33ed1b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(seed=src.RANDOM_STATE)\n",
    "\n",
    "z1 = rng.standard_normal((5, 5))\n",
    "z2 = rng.standard_normal((5, 5))\n",
    "z3 = rng.standard_normal((5, 5))\n",
    "\n",
    "rng = np.random.default_rng(seed=src.RANDOM_STATE)\n",
    "\n",
    "z_full = rng.standard_normal((15, 5))\n",
    "z1_expected = z_full[:5]\n",
    "z2_expected = z_full[5:10]\n",
    "z3_expected = z_full[10:15]\n",
    "\n",
    "print((z1 == z1_expected).all())\n",
    "print((z2 == z2_expected).all())\n",
    "print((z3 == z3_expected).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b45fefbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Iterable, List, Tuple\n",
    "\n",
    "class _NumpyRNG:\n",
    "    \n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.default_rng(self.seed)\n",
    "\n",
    "    def rand(self, size=None):\n",
    "        return self.rng.random(size)\n",
    "\n",
    "    def randint(self, low: int, high: int = None, size: int = None):\n",
    "        return self.rng.integers(low=low, high=high, size=size)\n",
    "\n",
    "    def choice(self, a: Union[int, Iterable[int]], size: Union[int, Tuple[int]] = None, p: Iterable[float] = None):\n",
    "        return self.rng.choice(a=a, size=size, p=p)\n",
    "\n",
    "    def beta(self, num_success: int, num_failure: int, size=None):\n",
    "        return self.rng.beta(num_success, num_failure, size)\n",
    "\n",
    "    def standard_normal(self, size=None):\n",
    "        return self.rng.standard_normal(size)\n",
    "\n",
    "    def multivariate_normal(self, mean: Union[np.ndarray, List[float]],\n",
    "                            covariance: Union[np.ndarray, List[List[float]]], size=None):\n",
    "        return np.squeeze(self.rng.multivariate_normal(mean, covariance, size=size, method='cholesky'))\n",
    "\n",
    "    def dirichlet(self, alpha: List[float], size=None):\n",
    "        return self.rng.dirichlet(alpha, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e251e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class _RidgeRegression:\n",
    "\n",
    "    def __init__(self, rng, alpha = 1.0, l2_lambda = 1.0, scale: bool = False):\n",
    "\n",
    "        # Ridge Regression: https://onlinecourses.science.psu.edu/stat857/node/155/\n",
    "        self.rng = rng                      # random number generator\n",
    "        self.alpha = alpha                  # exploration parameter\n",
    "        self.l2_lambda = l2_lambda          # regularization parameter\n",
    "        self.scale = scale                  # scale contexts\n",
    "\n",
    "        self.beta = None                    # (XtX + l2_lambda * I_d)^-1 * Xty = A^-1 * Xty\n",
    "        self.A = None                       # (XtX + l2_lambda * I_d)\n",
    "        self.A_inv = None                   # (XtX + l2_lambda * I_d)^-1\n",
    "        self.Xty = None\n",
    "        self.scaler = None\n",
    "\n",
    "    def init(self, num_features: int):\n",
    "        # By default, assume that\n",
    "        # A is the identity matrix and Xty is set to 0\n",
    "        self.Xty = np.zeros(num_features)\n",
    "        self.A = self.l2_lambda * np.identity(num_features)\n",
    "        self.A_inv = self.A.copy()\n",
    "        self.beta = np.dot(self.A_inv, self.Xty)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "\n",
    "        # Scale\n",
    "        if self.scaler is not None:\n",
    "            X = X.astype('float64')\n",
    "            if not hasattr(self.scaler, 'scale_'):\n",
    "                self.scaler.fit(X)\n",
    "            else:\n",
    "                self.scaler.partial_fit(X)\n",
    "            X = self.scaler.transform(X)\n",
    "\n",
    "        # X transpose\n",
    "        Xt = X.T\n",
    "\n",
    "        # Update A\n",
    "        self.A = self.A + np.dot(Xt, X)\n",
    "        self.A_inv = np.linalg.inv(self.A)\n",
    "\n",
    "        # Add new Xty values to old\n",
    "        self.Xty = self.Xty + np.dot(Xt, y)\n",
    "\n",
    "        # Recalculate beta coefficients\n",
    "        self.beta = np.dot(self.A_inv, self.Xty)\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "\n",
    "        # Scale\n",
    "        if self.scaler is not None:\n",
    "            x = self._scale_predict_context(x)\n",
    "\n",
    "        # Calculate default expectation y = x * b\n",
    "        return np.dot(x, self.beta)\n",
    "\n",
    "    def _scale_predict_context(self, x: np.ndarray):\n",
    "        if not hasattr(self.scaler, 'scale_'):\n",
    "            return x\n",
    "\n",
    "        # Transform and return to previous shape. Convert to float64 to suppress any type warnings.\n",
    "        return self.scaler.transform(x.astype('float64'))\n",
    "\n",
    "\n",
    "class _LinTS(_RidgeRegression):\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "        # Scale\n",
    "        if self.scaler is not None:\n",
    "            x = self._scale_predict_context(x)\n",
    "\n",
    "        # Randomly sample coefficients from multivariate normal distribution\n",
    "        # Covariance is enhanced with the exploration factor\n",
    "        # Generates  random samples for all contexts in one single go. type(beta_sampled): np.ndarray\n",
    "        beta_sampled = self.rng.multivariate_normal(self.beta, np.square(self.alpha) * self.A_inv, size=x.shape[0])\n",
    "\n",
    "        # Calculate expectation y = x * beta_sampled\n",
    "        return np.sum(x * beta_sampled, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a62d3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multivariate Normal (mean): [0.00156743 1.00226364]\n",
      "Manual Sampling (mean):      [-0.00221187  0.999497  ]\n",
      "Multivariate Normal (cov):\n",
      " [[0.99977376 0.4947176 ]\n",
      " [0.4947176  1.00322467]]\n",
      "Manual Sampling (cov):\n",
      " [[0.99747788 0.49701348]\n",
      " [0.49701348 1.00781644]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean = np.array([0.0, 1.0])\n",
    "cov = np.array([[1.0, 0.5], [0.5, 1.0]])\n",
    "size = 100_000\n",
    "\n",
    "# Method 1: NumPy default_rng().multivariate_normal\n",
    "rng = np.random.default_rng(seed=42)\n",
    "samples_mv = rng.multivariate_normal(mean, cov, size=size)\n",
    "\n",
    "# Method 2: Manual sampling\n",
    "rng = np.random.default_rng(seed=42)\n",
    "eps = rng.standard_normal(size=(size, 2))\n",
    "L = np.linalg.cholesky(cov)\n",
    "samples_manual = mean + eps @ L.T\n",
    "\n",
    "# Compare sample mean and covariance\n",
    "print(\"Multivariate Normal (mean):\", samples_mv.mean(axis=0))\n",
    "print(\"Manual Sampling (mean):     \", samples_manual.mean(axis=0))\n",
    "\n",
    "print(\"Multivariate Normal (cov):\\n\", np.cov(samples_mv, rowvar=False))\n",
    "print(\"Manual Sampling (cov):\\n\", np.cov(samples_manual, rowvar=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e52e7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(samples_mv.mean(axis=0), mean, atol=1e-2)\n",
    "np.allclose(np.cov(samples_mv, rowvar=False), cov, atol=1e-2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weighted-sims",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

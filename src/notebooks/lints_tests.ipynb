{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3073f85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multivariate Normal (mean): [0.00156743 1.00226364]\n",
      "Manual Sampling (mean):      [-0.00221187  0.999497  ]\n",
      "Multivariate Normal (cov):\n",
      " [[0.99977376 0.4947176 ]\n",
      " [0.4947176  1.00322467]]\n",
      "Manual Sampling (cov):\n",
      " [[0.99747788 0.49701348]\n",
      " [0.49701348 1.00781644]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mean = np.array([0.0, 1.0])\n",
    "cov = np.array([[1.0, 0.5], [0.5, 1.0]])\n",
    "size = 100_000\n",
    "\n",
    "# Method 1: NumPy default_rng().multivariate_normal\n",
    "rng = np.random.default_rng(seed=42)\n",
    "samples_mv = rng.multivariate_normal(mean, cov, size=size)\n",
    "\n",
    "# Method 2: Manual sampling\n",
    "rng = np.random.default_rng(seed=42)\n",
    "eps = rng.standard_normal(size=(size, 2))\n",
    "L = np.linalg.cholesky(cov)\n",
    "samples_manual = mean + eps @ L.T\n",
    "\n",
    "# Compare sample mean and covariance\n",
    "print(\"Multivariate Normal (mean):\", samples_mv.mean(axis=0))\n",
    "print(\"Manual Sampling (mean):     \", samples_manual.mean(axis=0))\n",
    "\n",
    "print(\"Multivariate Normal (cov):\\n\", np.cov(samples_mv, rowvar=False))\n",
    "print(\"Manual Sampling (cov):\\n\", np.cov(samples_manual, rowvar=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99c80700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(samples_mv.mean(axis=0), mean, atol=1e-2))\n",
    "print(np.allclose(np.cov(samples_mv, rowvar=False), cov, atol=1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306124f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Iterable, List, Tuple\n",
    "\n",
    "class _NumpyRNG:\n",
    "    \n",
    "    def __init__(self, seed):\n",
    "        self.seed = seed\n",
    "        self.rng = np.random.default_rng(self.seed)\n",
    "\n",
    "    def rand(self, size=None):\n",
    "        return self.rng.random(size)\n",
    "\n",
    "    def randint(self, low: int, high: int = None, size: int = None):\n",
    "        return self.rng.integers(low=low, high=high, size=size)\n",
    "\n",
    "    def choice(self, a: Union[int, Iterable[int]], size: Union[int, Tuple[int]] = None, p: Iterable[float] = None):\n",
    "        return self.rng.choice(a=a, size=size, p=p)\n",
    "\n",
    "    def beta(self, num_success: int, num_failure: int, size=None):\n",
    "        return self.rng.beta(num_success, num_failure, size)\n",
    "\n",
    "    def standard_normal(self, size=None):\n",
    "        return self.rng.standard_normal(size)\n",
    "\n",
    "    def multivariate_normal(self, mean: Union[np.ndarray, List[float]],\n",
    "                            covariance: Union[np.ndarray, List[List[float]]], size=None):\n",
    "        return np.squeeze(self.rng.multivariate_normal(mean, covariance, size=size, method='cholesky'))\n",
    "\n",
    "    def dirichlet(self, alpha: List[float], size=None):\n",
    "        return self.rng.dirichlet(alpha, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2b6f179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class _RidgeRegression:\n",
    "\n",
    "    def __init__(self, rng, alpha = 1.0, l2_lambda = 1.0, scale: bool = False):\n",
    "\n",
    "        # Ridge Regression: https://onlinecourses.science.psu.edu/stat857/node/155/\n",
    "        self.rng = rng                      # random number generator\n",
    "        self.alpha = alpha                  # exploration parameter\n",
    "        self.l2_lambda = l2_lambda          # regularization parameter\n",
    "        self.scale = scale                  # scale contexts\n",
    "\n",
    "        self.beta = None                    # (XtX + l2_lambda * I_d)^-1 * Xty = A^-1 * Xty\n",
    "        self.A = None                       # (XtX + l2_lambda * I_d)\n",
    "        self.A_inv = None                   # (XtX + l2_lambda * I_d)^-1\n",
    "        self.Xty = None\n",
    "        self.scaler = None\n",
    "\n",
    "    def init(self, num_features: int):\n",
    "        # By default, assume that\n",
    "        # A is the identity matrix and Xty is set to 0\n",
    "        self.Xty = np.zeros(num_features)\n",
    "        self.A = self.l2_lambda * np.identity(num_features)\n",
    "        self.A_inv = self.A.copy()\n",
    "        self.beta = np.dot(self.A_inv, self.Xty)\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "\n",
    "        # Scale\n",
    "        if self.scaler is not None:\n",
    "            X = X.astype('float64')\n",
    "            if not hasattr(self.scaler, 'scale_'):\n",
    "                self.scaler.fit(X)\n",
    "            else:\n",
    "                self.scaler.partial_fit(X)\n",
    "            X = self.scaler.transform(X)\n",
    "\n",
    "        # X transpose\n",
    "        Xt = X.T\n",
    "\n",
    "        # Update A\n",
    "        self.A = self.A + np.dot(Xt, X)\n",
    "        self.A_inv = np.linalg.inv(self.A)\n",
    "\n",
    "        # Add new Xty values to old\n",
    "        self.Xty = self.Xty + np.dot(Xt, y)\n",
    "\n",
    "        # Recalculate beta coefficients\n",
    "        self.beta = np.dot(self.A_inv, self.Xty)\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "\n",
    "        # Scale\n",
    "        if self.scaler is not None:\n",
    "            x = self._scale_predict_context(x)\n",
    "\n",
    "        # Calculate default expectation y = x * b\n",
    "        return np.dot(x, self.beta)\n",
    "\n",
    "    def _scale_predict_context(self, x: np.ndarray):\n",
    "        if not hasattr(self.scaler, 'scale_'):\n",
    "            return x\n",
    "\n",
    "        # Transform and return to previous shape. Convert to float64 to suppress any type warnings.\n",
    "        return self.scaler.transform(x.astype('float64'))\n",
    "\n",
    "\n",
    "class _LinTS(_RidgeRegression):\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "        # Scale\n",
    "        if self.scaler is not None:\n",
    "            x = self._scale_predict_context(x)\n",
    "\n",
    "        # Randomly sample coefficients from multivariate normal distribution\n",
    "        # Covariance is enhanced with the exploration factor\n",
    "        # Generates  random samples for all contexts in one single go. type(beta_sampled): np.ndarray\n",
    "        beta_sampled = self.rng.multivariate_normal(self.beta, np.square(self.alpha) * self.A_inv, size=x.shape[0])\n",
    "        \n",
    "        print(beta_sampled[0])\n",
    "\n",
    "        # Calculate expectation y = x * beta_sampled\n",
    "        return np.sum(x * beta_sampled, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c717639",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _LinTS_modified(_RidgeRegression):\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "        # Scale\n",
    "        if self.scaler is not None:\n",
    "            x = self._scale_predict_context(x)\n",
    "\n",
    "        # Randomly sample coefficients from multivariate normal distribution\n",
    "        # Covariance is enhanced with the exploration factor\n",
    "        # Generates  random samples for all contexts in one single go. type(beta_sampled): np.ndarray\n",
    "        eps = rng.standard_normal(size=(x.shape[0], self.beta.shape[0]))\n",
    "        L = np.linalg.cholesky(np.square(self.alpha) * self.A_inv)\n",
    "        mean = self.beta\n",
    "        beta_sampled = mean + eps @ L.T\n",
    "\n",
    "        print(self.beta)\n",
    "\n",
    "        # Calculate expectation y = x * beta_sampled\n",
    "        return np.sum(x * beta_sampled, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21e9d539",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_INTERACTIONS_SIZE = 10_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d160f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[0, 1, 0, 0, 0], [1, 0, 0, 0, 0]])\n",
    "y_train = np.array([1, 0])\n",
    "\n",
    "X_test = np.array([[0, 1, 0, 0, 0], [1, 0, 0, 0, 0]])\n",
    "num_contexts = X_test.shape[0]\n",
    "X_test = np.repeat(X_test, TEST_INTERACTIONS_SIZE, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b830815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.21546751 -0.23537981  0.7504512   0.94056472 -1.95103519]\n"
     ]
    }
   ],
   "source": [
    "lin_ts1 = _LinTS(rng=_NumpyRNG(seed=42), alpha=1.0, l2_lambda=1.0, scale=False)\n",
    "lin_ts1.init(num_features=5)\n",
    "\n",
    "\n",
    "lin_ts1.fit(X_train, y_train)\n",
    "\n",
    "scores1 = lin_ts1.predict(X_test).reshape((num_contexts, TEST_INTERACTIONS_SIZE)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73e58fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23537981, -0.12584549],\n",
       "       [ 0.59039682, -1.31946612],\n",
       "       [ 1.04998195, -0.39989925],\n",
       "       ...,\n",
       "       [-0.1319411 ,  0.3742013 ],\n",
       "       [ 0.88785896,  0.85114717],\n",
       "       [ 0.50719944, -0.48063021]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f804066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.5 0.  0.  0. ]\n",
      "[0.  0.5 0.  0.  0. ]\n"
     ]
    }
   ],
   "source": [
    "lin_ts2 = _LinTS_modified(rng=np.random.default_rng(42), alpha=1.0, l2_lambda=1.0, scale=False)\n",
    "lin_ts2.init(num_features=5)\n",
    "\n",
    "lin_ts2.fit(X_train, y_train)\n",
    "\n",
    "lin_ts2.predict(X_test)\n",
    "\n",
    "scores2 = lin_ts2.predict(X_test).reshape((num_contexts, TEST_INTERACTIONS_SIZE)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83774342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinTS original mean:  [ 4.99974308e-01 -8.29050524e-06]\n",
      "LinTS modified mean:  [4.99541534e-01 7.57056431e-05]\n",
      "True\n",
      "True\n",
      "LinTS covariance:  [[5.00187258e-01 9.74367480e-05]\n",
      " [9.74367480e-05 5.00391862e-01]]\n",
      "LinTS modified covariance:  [[4.99720409e-01 2.45057695e-04]\n",
      " [2.45057695e-04 4.99938372e-01]]\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('LinTS original mean: ', scores1.mean(axis=0))\n",
    "print('LinTS modified mean: ', scores2.mean(axis=0))\n",
    "\n",
    "# Check if the means are close enough\n",
    "print(np.allclose(scores1.mean(axis=0), scores2.mean(axis=0), atol=1e-2))\n",
    "print(np.allclose(scores1.mean(axis=0), scores2.mean(axis=0), atol=1e-2))\n",
    "\n",
    "print(\"LinTS covariance: \", np.cov(scores1, rowvar=False))\n",
    "print(\"LinTS modified covariance: \", np.cov(scores2, rowvar=False))\n",
    "print(np.allclose(np.cov(scores1, rowvar=False), np.cov(scores2, rowvar=False), atol=1e-2))\n",
    "print(np.allclose(np.cov(scores1, rowvar=False), np.cov(scores2, rowvar=False), atol=1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6ef2486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class _LinTS_modified2(_RidgeRegression):\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "        # Scale\n",
    "        if self.scaler is not None:\n",
    "            x = self._scale_predict_context(x)\n",
    "\n",
    "        # Randomly sample coefficients from multivariate normal distribution\n",
    "        # Covariance is enhanced with the exploration factor\n",
    "        # Generates  random samples for all contexts in one single go. type(beta_sampled): np.ndarray\n",
    "        eps = torch.from_numpy(self.rng.standard_normal(size=(x.shape[0], self.beta.shape[0]))).cuda()\n",
    "        L = torch.linalg.cholesky(torch.from_numpy(np.square(self.alpha) * self.A_inv).cuda())\n",
    "        mean = torch.from_numpy(self.beta).cuda()\n",
    "        beta_sampled = mean + eps @ L\n",
    "\n",
    "        # Calculate expectation y = x * beta_sampled\n",
    "        return torch.sum(torch.from_numpy(x).cuda() * beta_sampled, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbc7158f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_ts3 = _LinTS_modified2(rng=np.random.default_rng(42), alpha=1.0, l2_lambda=1.0, scale=False)\n",
    "lin_ts3.init(num_features=5)\n",
    "\n",
    "lin_ts3.fit(X_train, y_train)\n",
    "\n",
    "scores3 = lin_ts3.predict(X_test).reshape((num_contexts, TEST_INTERACTIONS_SIZE)).T.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55c7f5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinTS original mean:  [ 4.99974308e-01 -8.29050524e-06]\n",
      "LinTS modified mean:  [ 4.99974308e-01 -8.29050524e-06]\n",
      "LinTS covariance:  [[5.00187258e-01 9.74367480e-05]\n",
      " [9.74367480e-05 5.00391862e-01]]\n",
      "LinTS modified covariance:  [[5.00187258e-01 9.74367480e-05]\n",
      " [9.74367480e-05 5.00391862e-01]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print('LinTS original mean: ', scores1.mean(axis=0))\n",
    "print('LinTS modified mean: ', scores3.mean(axis=0))\n",
    "\n",
    "# Check if the means are close enough\n",
    "np.allclose(scores1.mean(axis=0), scores3.mean(axis=0), atol=1e-2)\n",
    "np.allclose(scores1.mean(axis=0), scores3.mean(axis=0), atol=1e-2)\n",
    "\n",
    "print(\"LinTS covariance: \", np.cov(scores1, rowvar=False))\n",
    "print(\"LinTS modified covariance: \", np.cov(scores3, rowvar=False))\n",
    "np.allclose(np.cov(scores1, rowvar=False), np.cov(scores3, rowvar=False), atol=1e-2)\n",
    "np.allclose(np.cov(scores1, rowvar=False), np.cov(scores3, rowvar=False), atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cb4fa5",
   "metadata": {},
   "source": [
    "## Testes com mais de um item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "630339a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Callable, Dict, List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from mabwiser.base_mab import BaseMAB\n",
    "from mabwiser.utils import Arm, Num, _BaseRNG\n",
    "\n",
    "class _Linear(BaseMAB):\n",
    "    factory = {\"ts\": _LinTS,  \"ridge\": _RidgeRegression}\n",
    "\n",
    "    def __init__(self, rng: _BaseRNG, arms: List[Arm], n_jobs: int, backend: Optional[str],\n",
    "                 alpha: Num, epsilon: Num, l2_lambda: Num, regression: str, scale: bool):\n",
    "        super().__init__(rng, arms, n_jobs, backend)\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.regression = regression\n",
    "        self.scale = scale\n",
    "        self.num_features = None\n",
    "\n",
    "        # Create regression model for each arm\n",
    "        self.arm_to_model = dict((arm, _Linear.factory.get(regression)(rng, alpha, l2_lambda, scale)) for arm in arms)\n",
    "\n",
    "    def fit(self, decisions: np.ndarray, rewards: np.ndarray, contexts: np.ndarray = None) -> None:\n",
    "\n",
    "        # Initialize each model by arm\n",
    "        self.num_features = contexts.shape[1]\n",
    "        for arm in self.arms:\n",
    "            self.arm_to_model[arm].init(num_features=self.num_features)\n",
    "\n",
    "        # Reset warm started arms\n",
    "        self._reset_arm_to_status()\n",
    "\n",
    "        # Perform parallel fit\n",
    "        self._parallel_fit(decisions, rewards, contexts)\n",
    "\n",
    "        # Update trained arms\n",
    "        self._set_arms_as_trained(decisions=decisions, is_partial=False)\n",
    "\n",
    "    def partial_fit(self, decisions: np.ndarray, rewards: np.ndarray, contexts: np.ndarray = None) -> None:\n",
    "        # Perform parallel fit\n",
    "        self._parallel_fit(decisions, rewards, contexts)\n",
    "\n",
    "        # Update trained arms\n",
    "        self._set_arms_as_trained(decisions=decisions, is_partial=True)\n",
    "\n",
    "    def predict(self, contexts: np.ndarray = None) -> Union[Arm, List[Arm]]:\n",
    "        # Return predict for the given context\n",
    "        return self._vectorized_predict_context(contexts, is_predict=True)\n",
    "\n",
    "    def predict_expectations(self, contexts: np.ndarray = None) -> Union[Dict[Arm, Num], List[Dict[Arm, Num]]]:\n",
    "        # Return predict expectations for the given context\n",
    "        return self._vectorized_predict_context(contexts, is_predict=False)\n",
    "\n",
    "    def warm_start(self, arm_to_features: Dict[Arm, List[Num]], distance_quantile: float):\n",
    "        self._warm_start(arm_to_features, distance_quantile)\n",
    "\n",
    "    def _copy_arms(self, cold_arm_to_warm_arm):\n",
    "        for cold_arm, warm_arm in cold_arm_to_warm_arm.items():\n",
    "            self.arm_to_model[cold_arm] = deepcopy(self.arm_to_model[warm_arm])\n",
    "\n",
    "    def _uptake_new_arm(self, arm: Arm, binarizer: Callable = None):\n",
    "\n",
    "        # Add to untrained_arms arms\n",
    "        self.arm_to_model[arm] = _Linear.factory.get(self.regression)(self.rng, self.alpha, self.l2_lambda, self.scale)\n",
    "\n",
    "        # If fit happened, initialize the new arm to defaults\n",
    "        is_fitted = self.num_features is not None\n",
    "        if is_fitted:\n",
    "            self.arm_to_model[arm].init(num_features=self.num_features)\n",
    "\n",
    "    def _fit_arm(self, arm: Arm, decisions: np.ndarray, rewards: np.ndarray, contexts: Optional[np.ndarray] = None):\n",
    "\n",
    "        # Get local copy of model to minimize communication overhead\n",
    "        # between arms (processes) using shared object\n",
    "        lr = deepcopy(self.arm_to_model[arm])\n",
    "\n",
    "        # Skip the arms with no data\n",
    "        indices = np.where(decisions == arm)\n",
    "        if indices[0].size == 0:\n",
    "            return lr\n",
    "\n",
    "        # Fit the regression\n",
    "        X = contexts[indices]\n",
    "        y = rewards[indices]\n",
    "        lr.fit(X, y)\n",
    "\n",
    "        self.arm_to_model[arm] = lr\n",
    "\n",
    "    def _predict_contexts(self, contexts: np.ndarray, is_predict: bool,\n",
    "                          seeds: Optional[np.ndarray] = None, start_index: Optional[int] = None) -> List:\n",
    "        pass\n",
    "\n",
    "    def _vectorized_predict_context(self, contexts: np.ndarray, is_predict: bool) -> List:\n",
    "\n",
    "        # Converting the arms list to numpy array\n",
    "        arms = deepcopy(self.arms)\n",
    "        arms = np.array(arms)\n",
    "\n",
    "        # Initializing array with expectations for each arm\n",
    "        num_contexts = contexts.shape[0]\n",
    "        arm_expectations = np.empty((num_contexts, len(arms)), dtype=float)\n",
    "\n",
    "        # With epsilon probability, assign random flag to context\n",
    "        random_values = self.rng.rand(num_contexts)\n",
    "        random_mask = np.array(random_values < self.epsilon)\n",
    "        random_indices = random_mask.nonzero()[0]\n",
    "\n",
    "        # For random indices, generate random expectations\n",
    "        arm_expectations[random_indices] = self.rng.rand((random_indices.shape[0], len(arms)))\n",
    "\n",
    "        # For non-random indices, get expectations for each arm\n",
    "        nonrandom_indices = np.where(~random_mask)[0]\n",
    "        nonrandom_context = contexts[nonrandom_indices]\n",
    "        arm_expectations[nonrandom_indices] = np.array([self.arm_to_model[arm].predict(nonrandom_context)\n",
    "                                                        for arm in arms]).T\n",
    "\n",
    "        if is_predict:\n",
    "            predictions = arms[np.argmax(arm_expectations, axis=1)].tolist()\n",
    "        else:\n",
    "            predictions = [dict(zip(self.arms, value)) for value in arm_expectations]\n",
    "\n",
    "        return predictions if len(predictions) > 1 else predictions[0]\n",
    "\n",
    "    def _drop_existing_arm(self, arm: Arm) -> None:\n",
    "        self.arm_to_model.pop(arm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d9ebb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arms = [0, 1, 2, 3]\n",
    "lints_more_arms_1 = _Linear(\n",
    "    rng=_NumpyRNG(seed=42),\n",
    "    arms=arms,\n",
    "    n_jobs=1,\n",
    "    alpha=1.0,\n",
    "    epsilon=0.0,\n",
    "    backend=None,\n",
    "    l2_lambda=1.0,\n",
    "    regression='ts',\n",
    "    scale=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "245e641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1, 0, 0, 0, 0], [0, 1, 0, 0, 0],\n",
    "                    [0, 0, 1, 0, 0], [0, 0, 0, 1, 0],\n",
    "                    [0, 0, 0, 0, 1], [0, 0, 1, 0, 0],\n",
    "                    [0, 0, 1, 0, 0], [0, 0, 0, 0, 1]], dtype=float)\n",
    "decisions = np.array([0, 0, 1, 1, 2, 2, 3, 3])\n",
    "rewards = np.array([1, 0, 1, 0, 1, 0, 1, 0])\n",
    "\n",
    "X_test = np.array([[0, 0, 1, 0, 0]])\n",
    "num_contexts = X_test.shape[0]\n",
    "X_test = np.repeat(X_test, TEST_INTERACTIONS_SIZE, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70f47feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lints_more_arms_1.fit(decisions, rewards, contexts=X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dde7d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.71546751 -0.73537981  0.7504512   0.94056472 -1.95103519]\n",
      "[ 0.30471708 -1.03998411  1.03064913  0.66507969 -1.95103519]\n",
      "[ 0.30471708 -1.03998411  0.53064913  0.94056472 -0.87959021]\n",
      "[ 0.30471708 -1.03998411  1.03064913  0.94056472 -1.37959021]\n"
     ]
    }
   ],
   "source": [
    "scores_more_1_dict = lints_more_arms_1.predict_expectations(X_test)\n",
    "\n",
    "scores_more_1 = np.empty((TEST_INTERACTIONS_SIZE, len(arms)), dtype=np.double)\n",
    "for arm in arms:\n",
    "    for i in range(TEST_INTERACTIONS_SIZE):\n",
    "        scores_more_1[i, arm] = scores_more_1_dict[i][arm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c96a037a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinTS original mean:  [1.56298524e-04 5.00110520e-01 1.10519746e-04 5.00110520e-01]\n",
      "LinTS covariance:  [[1.00002563 0.70712491 0.70712491 0.70712491]\n",
      " [0.70712491 0.50001282 0.50001282 0.50001282]\n",
      " [0.70712491 0.50001282 0.50001282 0.50001282]\n",
      " [0.70712491 0.50001282 0.50001282 0.50001282]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('LinTS original mean: ', scores_more_1.mean(axis=0))\n",
    "\n",
    "print(\"LinTS covariance: \", np.cov(scores_more_1, rowvar=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19e7b11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adapted from: https://github.com/fidelity/mabwiser/blob/master/mabwiser/linear.py\n",
    "# The difference is that the original code accept different types of arms (strings, integers, etc) and the modified code only accept sequential integers as arms. With this modification, the code can be optimized.\n",
    "\n",
    "ITEMS_PER_BATCH = 10_000\n",
    "INTERACTIONS_PER_BATCH_LINTS = 1_000\n",
    "\n",
    "from mabwiser.linear import _Linear\n",
    "from mabwiser.utils import Arm, Num, _BaseRNG\n",
    "from typing import Callable, Dict, List, Optional, Union\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "\n",
    "class _RidgeRegressionOptimized:\n",
    "\n",
    "    def __init__(self, rng: _BaseRNG, alpha: Num = 1.0, l2_lambda: Num = 1.0, scale: bool = False):\n",
    "\n",
    "        # Ridge Regression: https://onlinecourses.science.psu.edu/stat857/node/155/\n",
    "        self.rng = rng                      # random number generator\n",
    "        self.alpha = alpha                  # exploration parameter\n",
    "        self.l2_lambda = l2_lambda          # regularization parameter\n",
    "        self.scale = scale                  # scale contexts\n",
    "\n",
    "        self.beta = None                    # (XtX + l2_lambda * I_d)^-1 * Xty = A^-1 * Xty\n",
    "        self.A = None                       # (XtX + l2_lambda * I_d)\n",
    "        self.A_inv = None                   # (XtX + l2_lambda * I_d)^-1\n",
    "        self.Xty = None\n",
    "        self.scaler = None\n",
    "\n",
    "    def init(self, num_features: int, num_arms: int):\n",
    "        # By default, assume that\n",
    "        # A is the identity matrix and Xty is set to 0\n",
    "        start_time = time.time()\n",
    "        self.Xty = torch.zeros((num_arms, num_features), device='cuda', dtype=torch.double)\n",
    "        self.A = torch.eye(num_features, device='cuda', dtype=torch.double).unsqueeze(0).repeat(num_arms, 1, 1) * self.l2_lambda\n",
    "        #self.A_inv = self.A.clone()\n",
    "        self.beta = torch.zeros((num_arms, num_features), device='cuda', dtype=torch.double)\n",
    "        print(f'init demorou {time.time() - start_time} segundos')\n",
    "        #self.scaler = StandardScaler() if self.scale else None\n",
    "\n",
    "    def fit(self, decisions: np.ndarray, X: np.ndarray, y: np.ndarray):\n",
    "\n",
    "        # Scale\n",
    "        #if self.scaler is not None:\n",
    "        #    X = X.astype('float64')\n",
    "        #    if not hasattr(self.scaler, 'scale_'):\n",
    "        #        self.scaler.fit(X)\n",
    "        #    else:\n",
    "        #        self.scaler.partial_fit(X)\n",
    "        #    fix_small_variance(self.scaler)\n",
    "        #    X = self.scaler.transform(X)\n",
    "        start_time = time.time()\n",
    "        X_device = torch.tensor(X, device='cuda')\n",
    "        y_device = torch.tensor(y, device='cuda')\n",
    "        decisions_device = torch.tensor(decisions, device='cuda')\n",
    "        print(f'passar para cuda demorou {time.time() - start_time} segundos')\n",
    "        # Update A\n",
    "        #start_time = time.time()\n",
    "        #outer = torch.einsum('ni,nj->nij', X_device, X_device)  # (n, d, d)\n",
    "        #print(f'outer demorou {time.time() - start_time} segundos')\n",
    "\n",
    "        # Scatter add outer products into self.A based on decisions\n",
    "        start_time = time.time()\n",
    "        self.A.index_add_(0, decisions_device, torch.einsum('ni,nj->nij', X_device, X_device))\n",
    "        print(f'A add demorou {time.time() - start_time} segundos')\n",
    "\n",
    "        # Add X * y to Xty\n",
    "        start_time = time.time()\n",
    "        self.Xty.index_add_(0, decisions_device, X_device * y_device.view(-1, 1))\n",
    "        print(f'Xty demorou {time.time() - start_time} segundos')\n",
    "\n",
    "        # Invert each A matrix\n",
    "        for j in range(0, self.beta.shape[0], ITEMS_PER_BATCH):            \n",
    "            start_time = time.time()\n",
    "            self.beta[j:j+ITEMS_PER_BATCH] = torch.linalg.solve(\n",
    "                self.A[j:j+ITEMS_PER_BATCH],\n",
    "                self.Xty[j:j+ITEMS_PER_BATCH]\n",
    "            )\n",
    "            print(f'beta demorou {time.time() - start_time} segundos')\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "\n",
    "        # Calculate default expectation y = x * b\n",
    "        return torch.matmul(torch.tensor(x, device='cuda', dtype=torch.double), self.beta.T)\n",
    "\n",
    "class _LinTSOptimized(_RidgeRegressionOptimized):\n",
    "\n",
    "    def __init__(self, rng: _BaseRNG, alpha: Num = 1.0, l2_lambda: Num = 1.0, scale: bool = False):\n",
    "        super().__init__(rng, alpha, l2_lambda, scale)\n",
    "        self.torch_rng = torch.Generator(device='cuda').manual_seed(42)\n",
    "    \n",
    "    def predict(self, x: np.ndarray):\n",
    "        x_torch = torch.tensor(x, device='cuda', dtype=torch.double)  # [B, D]\n",
    "\n",
    "        num_arms, d = self.beta.shape\n",
    "        B = x.shape[0]\n",
    "\n",
    "        scores = torch.empty((B, num_arms), device='cuda', dtype=torch.double)\n",
    "\n",
    "        # z_all = torch.randn((num_arms, B, d), generator=self.torch_rng, device='cuda', dtype=torch.double)\n",
    "\n",
    "        for start in range(0, num_arms, ITEMS_PER_BATCH):\n",
    "            end = min(start + ITEMS_PER_BATCH, num_arms)\n",
    "            chunk_size = end - start\n",
    "\n",
    "            beta_chunk = self.beta[start:end]            # [chunk_size, D]\n",
    "            A_chunk = self.A[start:end]                  # [chunk_size, D, D]\n",
    "\n",
    "            # Cholesky decomposition\n",
    "            L_chunk = torch.linalg.cholesky((self.alpha ** 2) * A_chunk)  # [chunk_size, D, D]\n",
    "\n",
    "            z = torch.randn((chunk_size, B, d), generator=self.torch_rng, device='cuda', dtype=torch.double)\n",
    "\n",
    "            # beta_sampled: [chunk_size, B, D]\n",
    "            beta_sampled = beta_chunk[:, None, :] + torch.matmul(L_chunk, z.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "            # Compute scores for this chunk: [B, chunk_size]\n",
    "            scores_chunk = torch.einsum('mbd,bd->bm', beta_sampled, x_torch)\n",
    "\n",
    "            # Assign to output\n",
    "            scores[:, start:end] = scores_chunk\n",
    "\n",
    "        return scores  # shape: [B, M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "810e9ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lints_more_arms_2 = _LinTSOptimized(\n",
    "    rng=_NumpyRNG(seed=42),\n",
    "    alpha=1.0,\n",
    "    l2_lambda=1.0,\n",
    "    scale=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8033611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.029323577880859375 segundos\n",
      "passar para cuda demorou 0.0004322528839111328 segundos\n",
      "A add demorou 0.0965576171875 segundos\n",
      "Xty demorou 0.0003063678741455078 segundos\n",
      "beta demorou 0.024651527404785156 segundos\n"
     ]
    }
   ],
   "source": [
    "lints_more_arms_2.init(num_features=5, num_arms=len(arms))\n",
    "\n",
    "lints_more_arms_2.fit(decisions, X_train, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24199ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_more_2 = lints_more_arms_2.predict(X_test).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9811404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinTS original mean:  [1.56298524e-04 5.00110520e-01 1.10519746e-04 5.00110520e-01]\n",
      "LinTS modified mean:  [-5.18344829e-04  4.99309120e-01  1.69111780e-04  4.99577570e-01]\n",
      "True\n",
      "True\n",
      "LinTS covariance:  [[1.00002563 0.70712491 0.70712491 0.70712491]\n",
      " [0.70712491 0.50001282 0.50001282 0.50001282]\n",
      " [0.70712491 0.50001282 0.50001282 0.50001282]\n",
      " [0.70712491 0.50001282 0.50001282 0.50001282]]\n",
      "LinTS modified covariance:  [[ 9.99865659e-01 -5.71655624e-05  6.57783780e-04  4.19714414e-04]\n",
      " [-5.71655624e-05  1.99881928e+00  5.44876873e-05  4.93168290e-04]\n",
      " [ 6.57783780e-04  5.44876873e-05  2.00084172e+00 -1.57671602e-03]\n",
      " [ 4.19714414e-04  4.93168290e-04 -1.57671602e-03  2.00003048e+00]]\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('LinTS original mean: ', scores_more_1.mean(axis=0))\n",
    "print('LinTS modified mean: ', scores_more_2.mean(axis=0))\n",
    "\n",
    "# Check if the means are close enough\n",
    "print(np.allclose(scores_more_1.mean(axis=0), scores_more_2.mean(axis=0), atol=1e-2))\n",
    "print(np.allclose(scores_more_1.mean(axis=0), scores_more_2.mean(axis=0), atol=1e-2))\n",
    "\n",
    "print(\"LinTS covariance: \", np.cov(scores_more_1, rowvar=False))\n",
    "print(\"LinTS modified covariance: \", np.cov(scores_more_2, rowvar=False))\n",
    "print(np.allclose(np.cov(scores_more_1, rowvar=False), np.cov(scores_more_2, rowvar=False), atol=1e-2))\n",
    "print(np.allclose(np.cov(scores_more_1, rowvar=False), np.cov(scores_more_2, rowvar=False), atol=1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acdc264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class _LinTS_modified2(_RidgeRegression):\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "        # Scale\n",
    "        if self.scaler is not None:\n",
    "            x = self._scale_predict_context(x)\n",
    "\n",
    "        # Randomly sample coefficients from multivariate normal distribution\n",
    "        # Covariance is enhanced with the exploration factor\n",
    "        # Generates  random samples for all contexts in one single go. type(beta_sampled): np.ndarray\n",
    "        eps = torch.from_numpy(self.rng.standard_normal(size=(x.shape[0], self.beta.shape[0]))).cuda()\n",
    "        L = torch.linalg.cholesky(torch.from_numpy(np.square(self.alpha) * self.A_inv).cuda())\n",
    "        mean = torch.from_numpy(self.beta).cuda()\n",
    "        beta_sampled = mean + eps @ L.T\n",
    "\n",
    "        # Calculate expectation y = x * beta_sampled\n",
    "        return torch.sum(torch.from_numpy(x).cuda() * beta_sampled, axis=1)\n",
    "    \n",
    "\n",
    "class _LinTSOptimized2(_RidgeRegressionOptimized):\n",
    "\n",
    "    def __init__(self, rng: _BaseRNG, alpha: Num = 1.0, l2_lambda: Num = 1.0, scale: bool = False):\n",
    "        super().__init__(rng, alpha, l2_lambda, scale)\n",
    "    \n",
    "    def predict(self, x: np.ndarray):\n",
    "        x_torch = torch.tensor(x, device='cuda', dtype=torch.double)\n",
    "\n",
    "        num_arms, num_features = self.beta.shape\n",
    "        num_contexts = x.shape[0]\n",
    "\n",
    "        scores = torch.zeros((num_contexts, num_arms), device='cuda', dtype=torch.double)\n",
    "\n",
    "        eps = torch.from_numpy(self.rng.standard_normal(size=(num_contexts, num_features))).cuda()\n",
    "\n",
    "        for start in range(0, num_arms, ITEMS_PER_BATCH):\n",
    "            end = min(start + ITEMS_PER_BATCH, num_arms)  \n",
    "\n",
    "            beta_chunk = self.beta[start:end]\n",
    "            A_chunk = self.A[start:end]\n",
    "            A_inv_chunk = torch.linalg.inv(A_chunk)\n",
    "\n",
    "            L_chunk = torch.linalg.cholesky((self.alpha ** 2) * A_inv_chunk)\n",
    "            beta_sampled = torch.einsum('bd,add->bad', eps, L_chunk) + beta_chunk\n",
    "\n",
    "            scores[:, start:end] = torch.einsum('bd,bad->ba', x_torch, beta_sampled)\n",
    "\n",
    "        return scores  # shape: [B, M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91709d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lints_more_arms_3 = _LinTSOptimized2(\n",
    "    rng=np.random.default_rng(42),\n",
    "    alpha=1.0,\n",
    "    l2_lambda=1.0,\n",
    "    scale=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c21d9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.0017380714416503906 segundos\n",
      "passar para cuda demorou 0.0007092952728271484 segundos\n",
      "A add demorou 0.0004999637603759766 segundos\n",
      "Xty demorou 0.00016927719116210938 segundos\n",
      "beta demorou 0.0011057853698730469 segundos\n"
     ]
    }
   ],
   "source": [
    "lints_more_arms_3.init(num_features=5, num_arms=len(arms))\n",
    "\n",
    "lints_more_arms_3.fit(decisions, X_train, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ec94edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_more_3 = lints_more_arms_3.predict(X_test).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36b6a040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinTS original mean:  [1.56298524e-04 5.00110520e-01 1.10519746e-04 5.00110520e-01]\n",
      "LinTS modified mean:  [1.56298524e-04 5.00110520e-01 1.10519746e-04 5.00110520e-01]\n",
      "True\n",
      "LinTS covariance:  [[1.00002563 0.70712491 0.70712491 0.70712491]\n",
      " [0.70712491 0.50001282 0.50001282 0.50001282]\n",
      " [0.70712491 0.50001282 0.50001282 0.50001282]\n",
      " [0.70712491 0.50001282 0.50001282 0.50001282]]\n",
      "LinTS modified covariance:  [[1.00002563 0.70712491 0.70712491 0.70712491]\n",
      " [0.70712491 0.50001282 0.50001282 0.50001282]\n",
      " [0.70712491 0.50001282 0.50001282 0.50001282]\n",
      " [0.70712491 0.50001282 0.50001282 0.50001282]]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('LinTS original mean: ', scores_more_1.mean(axis=0))\n",
    "print('LinTS modified mean: ', scores_more_3.mean(axis=0))\n",
    "\n",
    "# Check if the means are close enough\n",
    "print(np.allclose(scores_more_1.mean(axis=0), scores_more_3.mean(axis=0), atol=1e-2))\n",
    "\n",
    "print(\"LinTS covariance: \", np.cov(scores_more_1, rowvar=False))\n",
    "print(\"LinTS modified covariance: \", np.cov(scores_more_3, rowvar=False))\n",
    "print(np.allclose(np.cov(scores_more_1, rowvar=False), np.cov(scores_more_3, rowvar=False), atol=1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89510837",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _LinTS_modified2(_RidgeRegression):\n",
    "\n",
    "    def predict(self, x: np.ndarray):\n",
    "        # Scale\n",
    "        if self.scaler is not None:\n",
    "            x = self._scale_predict_context(x)\n",
    "\n",
    "        # Randomly sample coefficients from multivariate normal distribution\n",
    "        # Covariance is enhanced with the exploration factor\n",
    "        # Generates  random samples for all contexts in one single go. type(beta_sampled): np.ndarray\n",
    "        eps = torch.from_numpy(self.rng.standard_normal(size=(x.shape[0], self.beta.shape[0]))).cuda()\n",
    "        L = torch.linalg.cholesky(torch.from_numpy(np.square(self.alpha) * self.A_inv).cuda())\n",
    "        mean = torch.from_numpy(self.beta).cuda()\n",
    "        beta_sampled = mean + eps @ L\n",
    "\n",
    "        # Calculate expectation y = x * beta_sampled\n",
    "        return torch.sum(torch.from_numpy(x).cuda() * beta_sampled, axis=1)\n",
    "\n",
    "class _LinTSOptimized3(_RidgeRegressionOptimized):\n",
    "\n",
    "    def __init__(self, rng: _BaseRNG, alpha: Num = 1.0, l2_lambda: Num = 1.0, scale: bool = False):\n",
    "        super().__init__(rng, alpha, l2_lambda, scale)\n",
    "    \n",
    "    def predict(self, x: np.ndarray):\n",
    "        x_torch = torch.tensor(x, device='cuda', dtype=torch.double)  # [B, D]\n",
    "\n",
    "        num_arms, num_features = self.beta.shape\n",
    "        num_contexts = x.shape[0]\n",
    "\n",
    "        scores = torch.zeros((num_contexts, num_arms), device='cuda', dtype=torch.double)\n",
    "\n",
    "        print(self.beta)\n",
    "\n",
    "        for i in range(num_arms):\n",
    "            eps = torch.from_numpy(self.rng.standard_normal(size=(num_contexts, num_features))).cuda()\n",
    "\n",
    "            # Cholesky decomposition\n",
    "            L = torch.linalg.cholesky((self.alpha ** 2) * torch.linalg.inv(self.A[i]))\n",
    "\n",
    "            beta_sampled = self.beta[i] + eps @ L.T\n",
    "\n",
    "            scores[:, i] = torch.sum(x_torch * beta_sampled, axis=1)\n",
    "\n",
    "        return scores  # shape: [B, M]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "160e60de",
   "metadata": {},
   "outputs": [],
   "source": [
    "lints_more_arms_4 = _LinTSOptimized3(\n",
    "    rng=np.random.default_rng(42),\n",
    "    alpha=1.0,\n",
    "    l2_lambda=1.0,\n",
    "    scale=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dec1f592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init demorou 0.0014488697052001953 segundos\n",
      "passar para cuda demorou 0.0005772113800048828 segundos\n",
      "A add demorou 0.0004382133483886719 segundos\n",
      "Xty demorou 0.00015354156494140625 segundos\n",
      "beta demorou 0.0009043216705322266 segundos\n"
     ]
    }
   ],
   "source": [
    "lints_more_arms_4.init(num_features=5, num_arms=len(arms))\n",
    "\n",
    "lints_more_arms_4.fit(decisions, X_train, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "635d158c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.5000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.5000],\n",
      "        [0.0000, 0.0000, 0.5000, 0.0000, 0.0000]], device='cuda:0',\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "scores_more_4 = lints_more_arms_4.predict(X_test).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b363658b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinTS original mean:  [1.56298524e-04 5.00110520e-01 1.10519746e-04 5.00110520e-01]\n",
      "LinTS modified mean:  [1.56298524e-04 5.00027918e-01 2.47897748e-04 4.99704299e-01]\n",
      "True\n",
      "True\n",
      "LinTS covariance:  [[1.00002563 0.70712491 0.70712491 0.70712491]\n",
      " [0.70712491 0.50001282 0.50001282 0.50001282]\n",
      " [0.70712491 0.50001282 0.50001282 0.50001282]\n",
      " [0.70712491 0.50001282 0.50001282 0.50001282]]\n",
      "LinTS modified covariance:  [[ 1.00002563e+00 -1.61562468e-04 -6.02630559e-05  1.14322518e-04]\n",
      " [-1.61562468e-04  5.00001575e-01 -2.24885063e-04  1.63212409e-04]\n",
      " [-6.02630559e-05 -2.24885063e-04  5.00162124e-01 -1.48153346e-04]\n",
      " [ 1.14322518e-04  1.63212409e-04 -1.48153346e-04  4.99540355e-01]]\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('LinTS original mean: ', scores_more_1.mean(axis=0))\n",
    "print('LinTS modified mean: ', scores_more_4.mean(axis=0))\n",
    "\n",
    "# Check if the means are close enough\n",
    "print(np.allclose(scores_more_1.mean(axis=0), scores_more_4.mean(axis=0), atol=1e-2))\n",
    "print(np.allclose(scores_more_1.mean(axis=0), scores_more_4.mean(axis=0), atol=1e-2))\n",
    "\n",
    "print(\"LinTS covariance: \", np.cov(scores_more_1, rowvar=False))\n",
    "print(\"LinTS modified covariance: \", np.cov(scores_more_4, rowvar=False))\n",
    "print(np.allclose(np.cov(scores_more_1, rowvar=False), np.cov(scores_more_4, rowvar=False), atol=1e-2))\n",
    "print(np.allclose(np.cov(scores_more_1, rowvar=False), np.cov(scores_more_4, rowvar=False), atol=1e-2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weighted-sims",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
